[{"content":"\nCA와 CA에서 Digital Certificate을 발급받아서 적용하는 방법에 대해 설명한다.\nIndex\nCA CA에서 인증서 생성하기 인증서 적용하기 Chain of Trust (plus) 인증서가 사용되는 프로세스 모식도 CA CA는 신뢰할 수 있는 기관에 의해 운영되는데, 주요 업무는 공개키 등록 시 본인 인증과 X.509와 같은 디지털 인증서 생성 및 발생 등이 있다.\nCA의 필요성 공개키와 비밀키만을 이용해 암호화는 수행하면 보안에 매우 취약해진다. [Picture 1] 에서 해커가 사용자 B에게 해커의 공개키를 사용하여 데이터를 보낼 경우 사용자 B는 사용자 A가 보낸 데이터라착각할 수 있다. 이와 같은 해커의 공격 방식을 MTM(Man in the Middle Attack)이라고 한다. 이와 같은 취약점을 해결하기 위해 CA(Certificate Authority) 라는 인증 노드를 사용하게 된다.\n[Picture 1] Man in Middle Attack PKI에서 CA가 사용되는 과정 사용자 A가 자신의 공개키를 CA에 등록한다. (본인 인증을 거친다) 사용자 B가 사용자 A의 공개키가 필요할 경우 CA에 가서 사용자 A의 공개키를 요청한다. CA는 사용자 A의 공개키를 암화화하여 사용자 B에게 전달한다. 사용자 B는 해당 공개키만을 사용자 A의 공개키라고 믿게 된다. [Picture 2] PKI에서 CA가 사용되는 과정 CA 업체 유명한 CA들은 [Picture 3] 같다. CA의 인증서를 대리 구매해주는 서비스들이 외국에도, 한국에도 있지만 직접 CA에 가서 사는 것이 더 좋다. CA에 직접 가서 구매했을 때 QnA나 대응도 더 빠르고, 자세하며 나중에 인증서를 관리할 때에도 더 편했다.\n[Picture 3] 유명한 CA\" CA에서 인증서 생성하기 CA에서 인증서를 생성하기 위해서는 SAN과 CSR을 제출해야 한다. CA 업체에 따라서 추가 서류를 제출해야 할 수 있으니 발급 전에 확인해야 한다.\nSAN Subject Alternative Name 의 약자이다. Optional 이고, 선택한 인증서의 가격에 따라서 보장하는 SAN의 개수에도 차이가 있다. 하나의 인증서로 여러개의 FQDN (Fully Qualified Domain Name)를 보장할 수 있다. 하나의 IP로 향햐는 도메인이 여러개가 있을 때 자주 사용이 된다. SAN 예시\nwww.digicert.com knowledge.digicert.com www.thawte.com CSR Certificate signing request 의 약자이다. CA에서 certificate 발급받기 위해 보내는 메세지이다. 가장 많이 사용되는 포맷은 PKCS #10이다. CSR 을 만들기 전에 PrivateKey를 만드는데, 이 PrivateKey가 바로 PKI에서 사용되는 그 PrivateKey로 아무한테도 노출하면 안된다. [gogetssl] 처럼 인증서 중간 유통 업체들이 때때로 CSR과 PrivateKey를 사용자에게 주기도 한다. OpenSSL로 CSR 생성하기 openssl req -new -newkey rsa:2048 -sha256 -nodes -keyout server.key -out server.csr Private key을 생성한다. 생성한 Private key를 사용하여 CSR을 생성한다. CSR을 생성할 때 [Picture 4] 에 대한 정보를 입력해야 한다. [Picture 4] CSR 입력 사항 Common Name SAN이 나오고 나서부터 Common Name은 실제적인 효력 보다는 과거의 레거시 형태로 남아있다. 이번에도 CSR을 작성할 때 OV를 만들어야 하기 때문에 Common Name에 도메인을 쓰지 않고 회사 이름을 넣었다. Comman Name에 도메인을 넣는다면 인증서가 설치되는 서버의 이름과 정확하게 매치해야 한다. 만약 서브 도메인을 위해 인증서를 발급했을 경우에는 full 서브도메인을 정확하게 명시해줘야한다.\nCommon Name 예시\n도메인: mydomain.com 서브 도메인: www common name: www.domain.com Common Name vs SAN common name은 단 하나의 엔트리만 입력을 할 수 있다. wildcard 또는 non-wildcard 이라도 단 하나만 입력을 할수 있다는 것은 변함이 없다 SAN은 common name의 이런 제약을 없애기 위해 생겨났다. SAN이 있기 때문에 multi-name SSL 인증서를 만들 수 있게 되었다. SAN에는 여러가지 값을 넣을 수 있고, Common Name과 중복도 가능하다. SAN이 서버 네임이 일치하는 가 확인하기 위한 유일한 필수 레퍼런스가 되었다. 인증서 관련 커맨드 웹사이트에서 사용하고 있는 인증서 가져오기 echo | openssl s_client -servername google.com -connect google.com:443 |\\\\n sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' \u0026gt; certificate.crt 인증서 열어보기 openssl x509 -in certificate.crt -noout -text [파싱 결과 예시] Certificate: Data: Version: 3 (0x2) Serial Number: 59:43:1c:7c:0e:b1:5c:49:0a:01:4e:60:34:b8:2c:b2 Signature Algorithm: sha256WithRSAEncryption Issuer: C=US, O=Google Trust Services LLC, CN=GTS CA 1C3 Validity Not Before: Apr 25 08:31:18 2022 GMT Not After : Jul 18 08:31:17 2022 GMT Subject: CN=*.google.com Subject Public Key Info: Public Key Algorithm: id-ecPublicKey Public-Key: (256 bit) pub: 04:81:63🆎d3:29:a2:15:6c:ee:b7:43:66:55:c5: 88:6e:70:9b:4d:43:40:66:ea:a4:fc:c0:06:8b:4c: fd:60:23:5f:f7:a0:e4:3c:5a:af:7f:e5:36:63:88: 55:dd:e2:60:41:6c:a1:27:3d:48:fb:2e:6a:21:6d: 01:aa:2e:25:7e ASN1 OID: prime256v1 NIST CURVE: P-256 X509v3 extensions: X509v3 Key Usage: critical Digital Signature X509v3 Extended Key Usage: TLS Web Server Authentication X509v3 Basic Constraints: critical CA:FALSE X509v3 Subject Key Identifier: 8B:09:31:88:DD:30:A6:59:D3:86:E5:3D:EA:06:6D:F3:C0:25:96:D5 X509v3 Authority Key Identifier: keyid:8A:74:7F:AF:85:CD:EE:95:CD:3D:9C:D0:E2:46:14:F3:71:35:1D:27 Authority Information Access: OCSP - URI:http://ocsp.pki.goog/gts1c3 CA Issuers - URI:http://pki.goog/repo/certs/gts1c3.der X509v3 Subject Alternative Name: DNS:*.google.com, DNS:*.appengine.google.com, DNS:*.bdn.dev, DNS:*.cloud.google.com, DNS:*.crowdsource.google.com, DNS:*.datacompute.google.com, DNS:*.google.ca, DNS:*.google.cl, DNS:*.google.co.in, DNS:*.google.co.jp, DNS:*.google.co.uk, DNS:*.google.com.ar, DNS:*.google.com.au, DNS:*.google.com.br, DNS:*.google.com.co, DNS:*.google.com.mx, DNS:*.google.com.tr, DNS:*.google.com.vn, DNS:*.google.de, DNS:*.google.es, DNS:*.google.fr, DNS:*.google.hu, DNS:*.google.it, DNS:*.google.nl, DNS:*.google.pl, DNS:*.google.pt, DNS:*.googleadapis.com, DNS:*.googleapis.cn, DNS:*.googlevideo.com, DNS:*.gstatic.cn, DNS:*.gstatic-cn.com, DNS:googlecnapps.cn, DNS:*.googlecnapps.cn, DNS:googleapps-cn.com, DNS:*.googleapps-cn.com, DNS:gkecnapps.cn, DNS:*.gkecnapps.cn, DNS:googledownloads.cn, DNS:*.googledownloads.cn, DNS:recaptcha.net.cn, DNS:*.recaptcha.net.cn, DNS:recaptcha-cn.net, DNS:*.recaptcha-cn.net, DNS:widevine.cn, DNS:*.widevine.cn, DNS:ampproject.org.cn, DNS:*.ampproject.org.cn, DNS:ampproject.net.cn, DNS:*.ampproject.net.cn, DNS:google-analytics-cn.com, DNS:*.google-analytics-cn.com, DNS:googleadservices-cn.com, DNS:*.googleadservices-cn.com, DNS:googlevads-cn.com, DNS:*.googlevads-cn.com, DNS:googleapis-cn.com, DNS:*.googleapis-cn.com, DNS:googleoptimize-cn.com, DNS:*.googleoptimize-cn.com, DNS:doubleclick-cn.net, DNS:*.doubleclick-cn.net, DNS:*.fls.doubleclick-cn.net, DNS:*.g.doubleclick-cn.net, DNS:doubleclick.cn, DNS:*.doubleclick.cn, DNS:*.fls.doubleclick.cn, DNS:*.g.doubleclick.cn, DNS:dartsearch-cn.net, DNS:*.dartsearch-cn.net, DNS:googletraveladservices-cn.com, DNS:*.googletraveladservices-cn.com, DNS:googletagservices-cn.com, DNS:*.googletagservices-cn.com, DNS:googletagmanager-cn.com, DNS:*.googletagmanager-cn.com, DNS:googlesyndication-cn.com, DNS:*.googlesyndication-cn.com, DNS:*.safeframe.googlesyndication-cn.com, DNS:app-measurement-cn.com, DNS:*.app-measurement-cn.com, DNS:gvt1-cn.com, DNS:*.gvt1-cn.com, DNS:gvt2-cn.com, DNS:*.gvt2-cn.com, DNS:2mdn-cn.net, DNS:*.2mdn-cn.net, DNS:googleflights-cn.net, DNS:*.googleflights-cn.net, DNS:admob-cn.com, DNS:*.admob-cn.com, DNS:*.gstatic.com, DNS:*.metric.gstatic.com, DNS:*.gvt1.com, DNS:*.gcpcdn.gvt1.com, DNS:*.gvt2.com, DNS:*.gcp.gvt2.com, DNS:*.url.google.com, DNS:*.youtube-nocookie.com, DNS:*.ytimg.com, DNS:android.com, DNS:*.android.com, DNS:*.flash.android.com, DNS:g.cn, DNS:*.g.cn, DNS:g.co, DNS:*.g.co, DNS:goo.gl, DNS:www.goo.gl, DNS:google-analytics.com, DNS:*.google-analytics.com, DNS:google.com, DNS:googlecommerce.com, DNS:*.googlecommerce.com, DNS:ggpht.cn, DNS:*.ggpht.cn, DNS:urchin.com, DNS:*.urchin.com, DNS:youtu.be, DNS:youtube.com, DNS:*.youtube.com, DNS:youtubeeducation.com, DNS:*.youtubeeducation.com, DNS:youtubekids.com, DNS:*.youtubekids.com, DNS:yt.be, DNS:*.yt.be, DNS:android.clients.google.com, DNS:developer.android.google.cn, DNS:developers.android.google.cn, DNS:source.android.google.cn X509v3 Certificate Policies: Policy: 2.23.140.1.2.1 Policy: 1.3.6.1.4.1.11129.2.5.3 X509v3 CRL Distribution Points: Full Name: URI:http://crls.pki.goog/gts1c3/QOvJ0N1sT2A.crl 1.3.6.1.4.1.11129.2.4.2: ......v.)y...99!.Vs.c.w..W}.` ..M]\u0026amp;\\%]......`........G0E.!......Y.Z...Z.s#...Al...\u0026amp;......Wi. m.-a..._^,...#....D.tZ.j..W.g....w...^.h.O.l..._N\u0026gt;Z.....j^.;.. D\\*s....`..!.....H0F.!..6:.?...f..m.}%.r..........E.....!..U....G...%.$D.mG.B.. Signature Algorithm: sha256WithRSAEncryption 5c:2b:62:ec:f6:ee:92:0c:28:98:92:af:35:f0:78:5b:75:f2: a2:c5:e9:56:04:da:31:ed:0c:92:16:3a:14:47:f9:60:7d:e4: 36:33:82:13:68:54:37:47:81:f8:b6:0e:66:a7:87:c4:f5:82: ca:58:62:a2:15:63:16:28:5b:8e:bc:e7:18:af:97:a2:f4:92: 92:e3:2f:69:df:ba:7a:80:92:20:14:22:4f:3d:26:69:c6:f8: 90:d1:2c:36:57:0a:5c:20:00:86:d2:bd:52:db:19:39:46:12: b0:65:1d:16:3d:f1:58:4b:d6:19:c0:4b:0d:eb:ad:0b:b9:1c: 03:ad:cb:d1:04:33:a2:2c:b8:33:f6:01:7c:71:7f:e8:8a:32: c1:74:9a:11:f7🆎b9:ff:f8:89:99:f3:f9:50:7b:31:c7:fa: fc:71:d1:c6:f2:b4:d2:82:93:84:ae:d8:eb:55:41:d4:de:9d: 7f:47:44:05:4a:fb:a7:09:b2:89:99:a7:7f:64:13:52:be:73: ee:00:b9:1c:ad:e1:44:48:41:a4:77:55:8d:0a:c8:b0:bb:69: fe:9a:84:a5:cd:2d:a9:61:3b:60:92:e4:43:d6:2b:79:d6:5a: 0d:db:f7:7f:7a:fc:7d:c3:59:e3:7d:d7:47:78:c2:b2:7d:6d: f2:7a:75:49 간단하게 인증서 조회를 하기 위해서는 online parser를 사용할 수 있다. [Online Parser] 파싱의 결과로 나온 field에 대한 설명은 [LeeLee- Digital Certificate] 포스트 에서 더 자세하게 확인할 수 있다.\n인증서 적용하기 CA bundle CA에서 인증서를 발급받으면 end-entity (내 application)의 인증서만 오지 않는다. Root CA의 인증서와 Intermediate 인증서가 함께 오는데 이를 CA bundle이라고 한다. CA bundle은 웹 브라우저 등의 클라이언트와 인증서의 호완성을 높히기 위해서 사용이 된다.\nCA bundle 예시\nRoot CA 인증서 + Intermediate 인증서 + end-entity 인증서 = Certificate Chain [Picture 5] CA bundle CA bundle은 *.ca-bundle 확장자의 zip 파일이나 root, intermediate 개별로 주어진다. CA bundle은 클라이언트에게 순차적으로 제공이 되어야 하기 때문에 대게는 CA에서 이미 하나의 bundle을 만들어서 제공해준다. 만약 개별로 주어졌을 경우에는 하나의 ca-bundle로 합쳐야 한다\nCA bundle 합치기 예시\nAddTrustExternalCARoot.crt –\u0026gt; Root CA Certificate COMODORSAAddTrustCA.crt –\u0026gt; Intermediate CA Certificate 1 COMODORSADomainValidationSecureServerCA.crt –\u0026gt; Intermediate CA Certificate 2 yourDomain.crt –\u0026gt; Your SSL Certificate cat ComodoRSADomainValidationSecureServerCA.crt ComodoRSAAddTrustCA.crt AddTrustExternalCARoot.crt \u0026gt; yourDomain.ca-bundle Google CA Bundle 예시\n브라우저의 브라우징 창에 🔒 모양을 누르면 해당 도메인에 대한 인증서의 종류, 상태 등을 볼 수 있는데 google.com의 인증서를 열어봤을 때에도 아래처럼 인증서가 계층을 이루고 있다.\nend-entity 인증서: *.google.com Intermediate 인증서: GTS CA 1C3 Root 인증서: GTS Root R1 [Picture 6] End Entity Certificate [Picture 7] Intermediate Certificate [Picture 8] Root Certificate Apply CA Bundle CA bundle을 서버 호스트 어디엔가 다운로드해두었다면 해당 도메인이 등록되어있는 Nginx 등의 웹서버에 CA Bundle가 있는 path를 적어주면 된다. 그럼 알아서 클라이언트에게 CA Bundle을 한꺼번에 넘겨준다. 클라이언트는 이 Bundle을 통해서 end-entity가 믿을 수 있는지 없는지 여부를 결정한다. 이와 같은 과정을 Chain of Trust라고 한다.\n인증서를 서버에 다운로드 하는 것을 ‘installed’ 한다고 표현하는데 실제로는 인증서를 서버에 설치한다기 보다는 특정 디렉터리에 넣어둔다.\nLet’s encrypt 를 인증서로 사용했을 때 Nginx의 configuration 예시\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 server { root /var/www/html; index index.html index.htm index.nginx-debian.html; server_name www.test.me test.me; location / { try_files $uri $uri/ =404; } listen [::]:443 ssl http2; # managed by Certbot listen 443 ssl http2; # managed by Certbot ssl_certificate /etc/letsencrypt/live/test.me/fullchain.pem; # managed by Certbot ssl_certificate_key /etc/letsencrypt/live/test.me/privkey.pem; # managed by Certbot include /etc/letsencrypt/options-ssl-nginx.conf; # managed by Certbot ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; # managed by Certbot } 14번 줄의 의 fullchain.pem이 하나로 압축이 된 CA bundle이다. Let’s encrypt에서는 처음부터 하나로 압축된 ca-bundle을 pem 형식으로 제공해준다.\nChain of Trust CA가 서명한 인증서의 내부 구조 CA가 서명을 end-entity의 인증서에 서명을 하게 되면 end-entity의 인증서에는 아래와 같은 정보가 추가되어 발급된다.\n누가 서명을 했는지 (Issuer name) Issuer의 Digital Signature (Issuer의 Private key로 서명) end-entity (Subject)의 public key [Picture 9] End Entity Certificate 내부 구조 Chain of Trust 과정 Chain of Trust는 브라우저가 신뢰할 수 있는 CA가 Issuer로 있는 인증서를 찾기 까지 모든 layer의 인증서를 탐색하는 과정이라고 생각하면 된다. 다른 말로 신뢰할 수 있는 CA가 발급한 인증서를 증명하는 과정이다. 참고로 브라우저에 Root CA의 인증서를 보내도 대부분은 그 인증서를 쓰지 않고, 브라우저에 이미 자체적으로 저장이 되어있는 Root CA 인증서를 사용한다.\nChain 시나리오 End-entity의 인증서 제출 End-entity는 브라우저가 신뢰할 수 있는 CA가 아니다. End-entity의 Issuer를 확인한다. Intermediate CA가 Issuer로 되어있다. Intermediate CA는 브라우저가 신뢰할 수 있는 CA가 아니다. intermediate CA의 Issuer를 확인한다. Root CA가 Issuer로 되어있다. Root CA는 브라우저가 신뢰할 수 있는 CA다. Root CA의 digital signature를 브라우저가 이미 가지고 있는 Root CA의 public key로 verify한다. Root CA digital signature verify 완료. Root CA → Intermediate CA → End-entity 모두 chain of trust를 통해서 믿을 수 있는 인증서가 된다. [Picture 10] Chain of Trust 과정 Chain 계층 [Picture 11] Chain 계층 Root Certificate AKA Trust Anchor 최상단인 Root는 보증해 줄 곳이 없기 때문에 Root CA는 self-signed을 한다. OS, 서드파티 웹 브라우저, 클라이언트 어플리케이션은 Root CA의 인증서를 사전에 설치해둔다. clinet는 chain of trust를 통해서 end-entity가 Root 에 인증이 되었는지 식별할 수 있다. Root가 end-entity에 직접 서명하는 일은 거의 없고, 중간에 하나 또는 여러 개의 Intermediate CA가 서명을 한다. Root의 Private key가 변경이 되었다면 해당 Private key로 발급받은 모든 인증서를 재발급 해야 한다. Intermediate Certificate 거의 모든 SSL 인증서 chain에는 최소 하나의 Intermediate CA가 들어가 있다. Intermediate CA는 Root CA의 신뢰성(trustworthy)의 확장에 필수적인 연결점이고, Root CA와 end-entity에 추가적인 보안 레벨이 된다. End-entity Certificate End-entity의 인증서는 보안, 확장성, CA compliance 등을 제공하지만, 인증서를 발급받은 대상 (subject)의 신뢰성을 보장하지는 못한다. End-entity는 크리티컬한 정보를 CSR에 담아 인증서를 발급해주는 CA(issuer)에게 제출하고, CA가 판단하기에 제출된 정보가 믿을 수 있다면 CA의 Private key로 서명을 한 인증서를 발급해준다. 이 인증서가 verified 또는 signed 되어있지 않다면 SSL connection은 실패한다. (plus)인증서가 사용되는 프로세스 모식도 브라우저와 서버의 request - response 과정 모식도\n[Picture 11] request - response 과정 모식도 브라우저가 youtube.com 을 요청한다. youtube가 인증서 번들을 브라우저에 제출한다. 브라우저는 자신의 CA public key 리스트로 서버가 제출한 인증서가 정말로 신뢰할 수 있는 CA에 의해 서명되었는지 확인한다. Intermediate CA는 지정된 Root CA가 아니기 때문에 브라우저의 입장에서는 믿을 수 없는 CA다. 제출된 인증서 번들을 타고 올라가서 Root CA까지 간다. 신뢰하기로 한 인증서에 명시되어있는 domain name이나 ip가 맞다면, 서버의 public key를 사용하여 대칭키를 생성하여 서버와 공유한다. 이 대칭키로 connection의 모든 트래픽을 암호화한다. 대칭키를 사용하는 것이 비대칭키를 사용하여 모든 커뮤니케이션을 암호화하는 것보다 효율적이기 때문에 대칭키르르 사용한다. 복호화는 private key를 가지고 있는 서버만 가능하다. CA-signed 인증서 발급 과정 모식도\n[Picture 12] CA-signed 인증서 발급 과정 모식도 Self-signed 인증서 발급 과정 모식도\n[Picture 13] Self-signed 인증서 발급 과정 모식도","date":"2022-08-27","permalink":"https://leeleelee3264.github.io/post/2022-08-27-digital-certificate-part-final/","tags":["Infra"],"title":"Digital certificate 적용하기"},{"content":"\noauth2를 사용하고 있는 Myinfo API 를 사용하는 connector client를 Python/Django로 구현한다.\n[github]\n[api document]\n[quick start]\nIndex\nMyinfo란? 프로젝트 목표 프로젝트 구현 프로젝트 회고 Myinfo란? 싱가폴 Mydata 서비스 정부가 주도한 Mydata 서비스가 몇 개의 나라에 있다고 하는데, 싱가폴 정부의 Singpass는 그 중에서도 모범사례로 뽑힌다고 한다. Singpass는 싱가포르의 15세 이상의 인구 중 97% 가 쓰고 있는 아주 활발한 서비스이다.\nSingpass에 있는 여러가지 서비스 중 Myinfo는 Person Data를 제공하는 서비스로, 한국의 카카오나 네이버 아이디 처럼 ouath2 로그인과 회원가입을 할 수 있다. [Picture 1] 에서 singpass에 대해서 조금 더 자세히 살펴볼 수 있다.\n[Picture 1] Introduce Singpass Myinfo oauth2 Myinfo는 [Picture 2] 와 같은 oauth2 구조를 가진다.\n[Picture 2] Myinfo oauth2 구조 Resource Owner Myinfo 사용자 Application 내가 구현하는 connector로, Myinfo 사용자의 데이터를 사용하는 주체 Identify Providers / Service Authorization Platform 인증서버 사용자의 인증정보와 권한 정보를 소유한 서버 Singpass 로그인 페이지 제공 Resource Server 사용자 데이터를 소유한 서버 인증 서버에 로그인 성공 후 접근 Myinfo Resource API 권한 인증 요청 [authorise api] GET /v3/authorise Singpass 로그인 페이지를 불러온다. 로그인 후 Singpass에서 사용자의 데이터를 불러오는 것에 대한 동의를 진행한다. 사용자가 동의했을 경우 authcode를 return 한다. Token 요청 [token api] POST /v3/token authocode를 사용하여 token을 요청한다. PKI를 사용하여 인증을 진행한다. 인증이 완료되면 token을 return 한다. 사용자 정보 요청 [person api] GET /v3/person/{sub} token 속의 access token을 사용하여 사용자 정보를 요청한다. 사용자 정보를 return 한다. 프로젝트 목표 Singpass가 제공하는 [Java]와 [node.js] 버전의 client connector 처럼 이번에 프로젝트로 python 버전의 connector를 만들었다. 아예 하나의 REST API 형태로 제공을 하기 위해 프레임워크로 Django를 선택했다.\n프로젝트 목표 프로젝트를 진행하면서 이루고자 한 목표는 아래와 같다. 대부분의 토이 프로젝트가 제대로 정리가 되어있지 않거나 코드가 엉망으로 짜여질 때가 많아서 이번에는 처음부터 확실하게 목표를 설정했다.\nCode Quality\nDDD 아키텍처로 서버를 구현한다. 최소 2회의 리팩토링을 진행한다. python lint(flake8, pylint, mypy)를 사용하여 최대한 python style을 고수한다. Pipenv를 사용해서 python 패키지를 관리한다. Documentation\nGithub README를 작성한다. API document를 작성한다. Project에 대한 블로그 포스팅을 작성한다. Dev Stack stack info Backend Language Python Backend Framework Django Code Architecture Domain Driven Desgin Python Package Managment Pipenv API Security PKI Version Control Github API Document GitBook 프로젝트 구현 Make Request Myinfo oauth2 구조를 반영하여, connector 서버의 호출 flow를 [Picture 3] 같이 설계했다.\n[Picture 3] connector 서버 호출 flow Step 1: Get myinfo login url Request GET /users/me/external/myinfo-redirect-login\ncurl -i -H 'Accept: application/json' \u0026lt;http://localhost:3001/user/me/external/myinfo-redirect-login\u0026gt; Response { \u0026quot;message\u0026quot;: \u0026quot;OK\u0026quot;, \u0026quot;data\u0026quot;: { \u0026quot;url\u0026quot;: \u0026quot;https://test.api.myinfo.gov.sg/com/v3/authorise?client_id=STG2-MYINFO-SELF-TEST\u0026amp;attributes=name,dob,birthcountry,nationality,uinfin,sex,regadd\u0026amp;state=eb03c000-00a3-4708-ab30-926306bfc4a8\u0026amp;redirect_uri=http://localhost:3001/callback\u0026amp;purpose=python-myinfo-connector\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;eb03c000-00a3-4708-ab30-926306bfc4a8\u0026quot; } } Step 2: Browse myinfo login url Request curl \u0026lt;https://test.api.myinfo.gov.sg/com/v3/authorise?client_id=STG2-MYINFO-SELF-TEST\u0026amp;attributes=name,dob,birthcountry,nationality,uinfin,sex,regadd\u0026amp;state=eb03c000-00a3-4708-ab30-926306bfc4a8\u0026amp;redirect_uri=http://localhost:3001/callback\u0026amp;purpose=python-myinfo-connector\u0026gt; Response [Picture 4] Myinfo Login Page Step 3: Login and check agree terms Login Check Login page in [Picture 5]\nAgree Terms [Picture 5] Myinfo Terms Agreement Page Step 4: Callback API get called by Myinfo Myinfo에서 Request를 하는 Step이다.\n로그인을 하고 terms에 동의를 하면 Myinfo에서 connector client의 callback API를 호출해 authcode를 넘겨준다.\nRequest GET /callback?{code}\ncurl \u0026lt;http://localhost:3001/callback?code=8932a98da8720a10e356bc76475d76c4c628aa7f\u0026amp;state=e2ad339a-337f-45ec-98fa-1672160cf463\u0026gt; Response [Picture 6] Callback Response Page\nFinal Step: Get Person data 자동화된 Step이다.\nCallback API의 응답인 callback 페이지는 자동으로 connector client의 person data API를 호출하도록 했다. 해당 API가 Myinfo에서 사용자 정보를 가져오는 마지막 단계이다.\nRequest GET /users/me/external/myinfo\ncurl -i -H 'Accept: application/json' \u0026lt;http://localhost:3001/user/me/external/myinfo\u0026gt; Response { \u0026quot;message\u0026quot;: \u0026quot;OK\u0026quot;, \u0026quot;sodata\u0026quot;: { \u0026quot;regadd\u0026quot;: { \u0026quot;country\u0026quot;: { \u0026quot;code\u0026quot;: \u0026quot;SG\u0026quot;, \u0026quot;desc\u0026quot;: \u0026quot;SINGAPORE\u0026quot; }, \u0026quot;unit\u0026quot;: { \u0026quot;value\u0026quot;: \u0026quot;10\u0026quot; }, \u0026quot;street\u0026quot;: { \u0026quot;value\u0026quot;: \u0026quot;ANCHORVALE DRIVE\u0026quot; }, \u0026quot;lastupdated\u0026quot;: \u0026quot;2022-07-14\u0026quot;, \u0026quot;block\u0026quot;: { \u0026quot;value\u0026quot;: \u0026quot;319\u0026quot; }, \u0026quot;source\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;postal\u0026quot;: { \u0026quot;value\u0026quot;: \u0026quot;542319\u0026quot; }, \u0026quot;classification\u0026quot;: \u0026quot;C\u0026quot;, \u0026quot;floor\u0026quot;: { \u0026quot;value\u0026quot;: \u0026quot;38\u0026quot; }, \u0026quot;type\u0026quot;: \u0026quot;SG\u0026quot;, \u0026quot;building\u0026quot;: { \u0026quot;value\u0026quot;: \u0026quot;\u0026quot; } }, \u0026quot;dob\u0026quot;: \u0026quot;1988-10-06\u0026quot;, \u0026quot;sex\u0026quot;: \u0026quot;M\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;ANDY LAU\u0026quot;, \u0026quot;birthcountry\u0026quot;: \u0026quot;SG\u0026quot;, \u0026quot;nationality\u0026quot;: \u0026quot;SG\u0026quot;, \u0026quot;uinfin\u0026quot;: \u0026quot;S6005048A\u0026quot; } } PKI Digital Signature Myinfo는 PKI Digital Signature를 필요로 한다. 해당 문서에서는 python에서 구현을 할 때 PKI를 사용하는 방법만을 다루기 때문에 PKI에 대한 더 자세한 설명은 링크로 첨부하겠다. [LeeLee- Digital Certificate]\npython 패키지로는 *jwcrypto*와 Crypto를 사용했다.\nPKI 시나리오 connector client private key\n[myinfo token api]를 호출할 때 [myinfo person api]를 호출 할 때 [myinfo person api] 응답 decrypt 할 때 myinfo에서 connector client의 public key로 응답을 암호화 했기 때문 myinfo public key\nmyinfo token api 응답 verify 할 때 myinfo에서 myinfo의 private key로 응답을 암호화 했기 때문 myinfo person api 응답 verify 할 때 myinfo에서 myinfo의 private key로 응답을 암호화 했기 때문 public, privateKey 불러오기 1 2 3 4 5 6 7 8 9 10 11 from jwcrypto import jwk PrivateKey = jwk.JWK PublicKey = jwk.JWK def _get_key(self, key: str) -\u0026gt; Union[PrivateKey, PublicKey]: encode_key = key.encode(\u0026#39;utf-8\u0026#39;) key_dict = jwk.JWK.from_pem(encode_key) return key_dict connector client private key로 서명하기 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import base64 from Crypto.Hash import SHA256 from Crypto.PublicKey import RSA from Crypto.Signature import PKCS1_v1_5 Signature = str def _sign_on_raw_header( self, base_string: str, private_key: str, ) -\u0026gt; Signature: digest = SHA256.new() digest.update( base_string.encode(\u0026#39;utf-8\u0026#39;), ) pk = RSA.importKey(private_key) signer = PKCS1_v1_5.new(pk) signature = str(base64.b64encode(signer.sign(digest)), \u0026#39;utf-8\u0026#39;) return signature connector client private key로 응답 decrypt 하기 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import json from jwcrypto import ( jwe, jwk, ) PrivateKey = jwk.JWK def _decrypt( self, encrypted_payload: str, key: PrivateKey, ) -\u0026gt; DecryptedPersonData: params = self._get_decrypt_params(encrypted_payload) encryption = jwe.JWE() encryption.deserialize(params, key) data = encryption.plaintext data_str = json.loads(data) return data_str myinfo public key로 응답 verify 하기 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import json from jwcrypto import jwk PublicKey = jwk.JWK DecodedPersonData = dict def _decode( self, encoded_payload: str, key: PublicKey, ) -\u0026gt; DecodedPersonData: token = jwt.JWT() token.deserialize(jwt=encoded_payload, key=key) data = token.claims data_dict = json.loads(data) person = DecodedPersonData(**data_dict) return person 프로젝트 회고 문서화 정해둔 목표를 잘 이행한 기분이 들어서 뿌듯했다. 또한 항상 미흡했던 문서화를 꼼꼼하게 해 둔 거 같아 만족스럽다. 하지만 문서화를 하는 과정에서 어떻게 내가 말하고자 하는 바를 더 깔끔하게 글로 옮길 수 있을까 고민을 많이 했고, 아직도 부족한 부분이 많이 보인다. 리팩토링 코드 또한 2번 리팩토링을 진행했지만 포스팅을 위해서 다시 코드를 보니 또 리팩토링 해야겠다는 생각이 든다. 프로젝트를 할 때 1,2번의 리팩토링을 하고 프로젝트가 끝나고 2~3달 지나서 리팩토링을 1 번 진행하면 좋을 거 같다. Boilerplate 어떤 프로젝트를 하더라도 프레임워크 setting을 하는데 초기 시간을 많이 소요하는데, 앞으로 Django로 계속 프로젝트를 진행할 예정이라면 pre-setting이 어느 정도 되어있는 Django Boilerplate 를 만들어야겠다.\n","date":"2022-07-23","permalink":"https://leeleelee3264.github.io/post/2022-07-23-project-myinfo-connector-python/","tags":["Project"],"title":"Python으로 Myinfo oauth2 client connector 구현하기"},{"content":"\n간단한 프로파일링을 할 수 있는 profiler를 Python decorator로 구현한다.\nIndex\nprofiler 구현 계기 profiler 구현 개선해야 하는 부분 profiler 구현 계기 Django 환경에서 unittest를 하며 간단하게 퍼포먼스를 측정하고 싶었다. 아주 간단하게 프로파일링을 하면 되기 때문에 Middleware 같은 것은 만들지 않았다. 대신에 Python의 decorator로 만들었다. 앞으로 더 심화되고 유익한 정보를 포함하고 있는 Profiler를 decorator 형식으로 만들면 유용할 것 같다.\n구현한 profiler\n메모리 사용량을 보기 위한 ram_profiler 함수에서 호출된 쿼리와 쿼리 실행시간을 보기 위한 query_profiler 함수 실행시간 을 보기 위한 elapsed_timer profiler 구현 ram_profiler 이 프로파일러를 구현하다가 알게 된 사실인데 Python에서 메모리나 CPU 사용량을 보려면 psutill builtin 패키지를 사용하는 경우가 많았다.\ndecorator는 함수 호출 전과 후의 메모리 사용량을 가져와, 함수가 호출이 되면서 얼추 어느 정도의 메모리를 사용했는지 비교할 수 있도록 한다.\nram_profiler 구현 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def ram_profiler(fn): \u0026#34;\u0026#34;\u0026#34; 퍼포먼스 체크를 위한 메모리 프로파일러 메모리 사용량을 측정할 함수 위에 @ram_profiler 를 추가해주시면 됩니다. 체크하는 메모리는 아래와 같습니다. 1) 그냥 현재 memory usage 정보를 그대로 가져오는 경우 2) 현재 process id를 통해 해당 프로세스의 memory usage를 정확하게 비교하는 경우 ex) @ram_profiler def pre_allot_offering(offering: Offering): \u0026#34;\u0026#34;\u0026#34; def inner(*args, **kwargs): print(\u0026#39;\\n\u0026#39;) print(\u0026#34;===== memory usage check =====\u0026#34;) memory_usage_dict = dict(psutil.virtual_memory()._asdict()) memory_usage_percent = memory_usage_dict[\u0026#39;percent\u0026#39;] print(f\u0026#34;BEFORE CODE :: memory_usage_percent: {memory_usage_percent}%\u0026#34;) pid = os.getpid() current_process = psutil.Process(pid) current_process_memory_usage_as_kb = current_process.memory_info()[0] / 2. ** 20 print(f\u0026#34;BEFORE CODE :: Current memory KB : {current_process_memory_usage_as_kb: 9.3f} KB\u0026#34;) target_func = fn(*args, **kwargs) print(\u0026#34;==\u0026#34; * 20) print(f\u0026#34;== {fn.__name__} memory usage check ==\u0026#34;) memory_usage_dict = dict(psutil.virtual_memory()._asdict()) memory_usage_percent = memory_usage_dict[\u0026#39;percent\u0026#39;] print(f\u0026#34;AFTER CODE :: memory_usage_percent: {memory_usage_percent}%\u0026#34;) pid = os.getpid() current_process = psutil.Process(pid) current_process_memory_usage_as_kb = current_process.memory_info()[0] / 2. ** 20 print(f\u0026#34;AFTER CODE :: Current memory KB : {current_process_memory_usage_as_kb: 9.3f} KB\u0026#34;) print(\u0026#39;\\n\u0026#39;) return target_func return inner ram_profiler 결과 ===== memory usage check ===== BEFORE CODE :: memory_usage_percent: 79.7% BEFORE CODE :: Current memory KB : 197.250 KB ======================================== == pre_allot_offering memory usage check == AFTER CODE :: memory_usage_percent: 79.7% AFTER CODE :: Current memory KB : 197.312 KB query_profiler decorator는 쿼리를 관리하는 context를 임포트 해와서 타겟 함수를 실행하면서 호출되었던 쿼리들을 캡처해두었다가 함수가 끝이 나면 보여준다. ORM에서 실제로 어떤 쿼리가 실행되는지 봐야 할 때, 조건문에 따라 쿼리가 달라질 때, N + 1 쿼리를 잡아 낼 때 사용하기 좋다.\nquery_profiler 구현 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def query_profiler(fn): \u0026#34;\u0026#34;\u0026#34; 호출이 된 query를 확인하기 위한 프로파일러 호출 된 query를 확인할 함수 위헤 @query_profiler 를 추가해주시면 됩니다. ex) @query_profiler def pre_allot_offering(offering: Offering): \u0026#34;\u0026#34;\u0026#34; def inner(*args, **kwargs): print(\u0026#39;\\n\u0026#39;) print(f\u0026#34;===== {fn.__name__} called query check =====\u0026#34;) with CaptureQueriesContext(connection) as context: target_func = fn(*args, **kwargs) for index, query in enumerate(context.captured_queries): sql = query.get(\u0026#39;sql\u0026#39;) time = query.get(\u0026#39;time\u0026#39;) print(f\u0026#39;CALLED QUERY :: [{index}]\u0026#39;) print(f\u0026#39;CALLED QUERY :: query: {sql}\u0026#39;) print(f\u0026#39;CALLED QUERY :: executed time: {time}\u0026#39;) print(\u0026#34;=====\u0026#34;) return target_func return inner query_profiler 결과 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 ===== pre_allot_offering called query check ===== CALLED QUERY :: [0] CALLED QUERY :: query: SAVEPOINT `s4377609600_x5` CALLED QUERY :: executed time: 0.001 ===== CALLED QUERY :: [1] CALLED QUERY :: query: SELECT `offering_subscription`.`id` # 실제 쿼리는 블라인드 CALLED QUERY :: executed time: 0.00 ===== CALLED QUERY :: [2] CALLED QUERY :: query: UPDATE `offering_subscription` SET # 실제 쿼리는 블라인드 CALLED QUERY :: executed time: 0.003 ===== CALLED QUERY :: [3] CALLED QUERY :: query: SELECT `offering_subscription`.`id`, # 실제 쿼리는 블라인드 ===== CALLED QUERY :: [4] CALLED QUERY :: query: SELECT `offering_subscription`.`id`, # 실제 쿼리는 블라인드 CALLED QUERY :: executed time: 0.002 ===== CALLED QUERY :: [5] CALLED QUERY :: query: UPDATE `offering_subscription` SET # 실제 쿼리는 블라인드 CALLED QUERY :: executed time: 0.002 ===== CALLED QUERY :: [6] CALLED QUERY :: query: RELEASE SAVEPOINT `s4377609600_x5` CALLED QUERY :: executed time: 0.001 ===== elapsed_timer decorator는 함수의 시작시간과 끝나는 시간을 측정하여 함수 실행시간이 어느정도인지를 계산한다.\nelapsed_timer 구현 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def elapsed_timer(fn): \u0026#34;\u0026#34;\u0026#34; 퍼포먼스 체크를 위한 실행 시간 측정 타이머 실행 시간을 측정할 함수 위에 @elapsed_timer 를 추가해주시면 됩니다. 퍼포먼스 테스트가 끝나면 지워주시길 바랍니다. ex) @elapsed_timer def pre_allot_offering(offering: Offering): \u0026#34;\u0026#34;\u0026#34; def inner(*args, **kwargs): print(\u0026#39;\\n\u0026#39;) print(f\u0026#34;===== {fn.__name__} elapsed time check =====\u0026#34;) start = perf_counter() target_func = fn(*args, **kwargs) end = perf_counter() print(f\u0026#39;ELAPSED :: total: start: {start} sec - end: {end} sec\u0026#39;) duration = end - start print(f\u0026#39;ELAPSED :: duration: {duration} sec\u0026#39;) print(f\u0026#39;ELAPSED :: duration in minutes : {str(timedelta(seconds=duration))} mins\u0026#39;) print(\u0026#39;\\n\u0026#39;) return target_func return inner elapsed_timer 결과 1 2 3 4 ===== pre_allot_offering elapsd time check ===== ELAPSED :: total: start: 8.705878 sec - end: 8.72449375 sec ELAPSED :: duration: 0.018615750000000375 sec ELAPSED :: duration in minutes : 0:00:00.018616 mins (builtin) @profiler 메모리를 프로파일링하는 것에 한정한다면 Python에서 builtin으로 제공하는 memory_profiler 라는 것이 있다. 패키지 안의 @profiler 데코레이터를 사용하면 함수 안에서 코드가 실행이 될 때 한 줄 한 줄 얼마의 메모리를 사용했는지를 볼 수 있다.\nORM을 포함하고 있는 코드에도 사용을 해보려 했는데 recursive 하게 동작을 하다가 스택이 터져서 사용을 할 수 없었다. 함수가 한 줄 씩 호출되면서 메모리 사용량을 보기에는 정말 좋은 데코레이터 처럼 보이는데 꼭 다음에 써보도록 해야겠다.\n@profiler 사용 예시 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from memory_profiler import profile @profile def main_func(): import random arr1 = [random.randint(1,10) for i in range(100000)] arr2 = [random.randint(1,10) for i in range(100000)] arr3 = [arr1[i]+arr2[i] for i in range(100000)] del arr1 del arr2 tot = sum(arr3) del arr3 print(tot) if __name__ == \u0026#34;__main__\u0026#34;: main_func() @profiler 결과 예시 1 2 3 4 5 6 7 8 9 10 11 12 13 Line # Mem usage Increment Line Contents ================================================ 3 37.0 MiB 37.0 MiB @profile 4 def main_func(): 5 37.0 MiB 0.0 MiB import random 6 37.6 MiB 0.3 MiB arr1 = [random.randint(1,10) for i in range(100000)] 7 38.4 MiB 0.3 MiB arr2 = [random.randint(1,10) for i in range(100000)] 8 39.9 MiB 0.5 MiB arr3 = [arr1[i]+arr2[i] for i in range(100000)] 9 39.9 MiB 0.0 MiB del arr1 10 38.0 MiB 0.0 MiB del arr2 11 38.0 MiB 0.0 MiB tot = sum(arr3) 12 37.2 MiB 0.0 MiB del arr3 13 37.2 MiB 0.0 MiB print(tot) 개선해야 하는 부분 Python의 Decorator는 함수의 시작 - 끝 부분에 추가 action을 할 수 있게 해주는 Wrapper이다. 함수 중간중간에 프로파일링을 해야 한다면 다른 방법으로 접근을 해야 할 것 같다.\n","date":"2022-06-29","permalink":"https://leeleelee3264.github.io/post/2022-06-30-python-profiler-decorator/","tags":["Backend"],"title":"Python decorator로 간단한 profiler 구현하기"},{"content":"\nPKI의 certificate에 대해서 다룬다.\nIndex\n인증서에 대해 잘못 알고 있던 점들 인증서의 종류 인증서와 관련된 개념 인증서에 대해 잘못 알고 있던 점들 인증서의 종류 적절하지 않은 인증서 분류 web에서 HTTPS 통신을 하기 위해 사용하는 web secure용 SSL 인증서 digital signature(디지털 서명) 를 하기 위한 code signing 인증서 디지털 서명을 해야 했기 때문에 code signing 인증서를 발급받으려 했다. 하지만 SSL 인증서도 디지털 서명을 할 수 있었고, 목표인 server to server 커뮤니케이션에서 사용하는 신원 보장용 디지털 서명을 하기 위해서는 SSL 인증서를 발급해야 했다.\n[Picture 1] 적절하지 않은 인증서 분류 적절한 인증서 분류 SSL 인증서는 웹사이트, 정확히 말하면 브라우저에 한정해서 사용한다고 생각했는데 이는 너무 한정적인 생각이었다. SSL 인증서는 client와 server가 (browser to server / server to server) 데이터를 주고 받을 때 암호화를 하기 위해 쓰이기 때문에 브라우저에 한정적이지 않다.\n결국 인증서는 [Picture 2] 처럼 분류를 해야 한다.\n[Picture 2] 적절한 인증서 분류 Code Signing 인증서 발급받으려 했던 code signing 인증서는 어플리케이션에 서명을 해서 해당 어플리케이션 publisher의 신원을 보장하는데 사용하기 때문에 내가 필요했던 server to server 커뮤니케이션에서 사용되는 인증서가 아니다.\ncode signing 인증서로 어플리케이션에 서명을 하게 되면 [Picture 3] 와 같이 사용자에게 어플리케이션 publisher 에 대한 정보가 추가로 제공이 된다.\n[Picture 3] code signing 인증서로 서명한 application CSR 생성하기 모든 SSL 인증서는 domain validation을 한다. 이 생각에 사로잡혀서 Root CA가 서명한 인증서를 발급받기 위해 제출해야 하는 CSR는 인증서가 사용이 될 서버에서 만들어야 한다고 생각했다.\n예를 들어 leelee.me 이라는 루트 도메인에서 사용하는 인증서라고 했을 때 leelee.me 도메인을 호스팅하는 서버에서 CSR을 생성해야 한다고 생각한 것이다. 하지만 CSR을 다른 서버에서 생성했다면, CSR을 생성할 때 함께 만든 private key를 인증서를 사용할 서버로 옮기면 된다.\n인증서의 종류 SSL Certificate VS Code Signing Certificate 공통점\nX.509 형태의 디지털 인증서다. PKI 형식을 사용한다. 두 인증서가 없다면 사용자에게 보안 경고가 띄워진다. 발급하기 전에 CA에서 발급요청자를 검증한다. end user들이 해킹등의 사이버 범죄에 노출되는 것을 방지하기 위해 사용된다. 차이점\nSSL Certificate은 두 시스템에서 오고가는 data를 암호화한다. Code Signing Certificate은 소프트웨어 자체를 hash \u0026amp; sign 한다. 모든 코드에 디지털 서명을 한 것과 마찬가지이며 만약 중간에서 누가 코드를 수정한다면 해시값이 변경이 되어 end user가 코드를 다운받기 전에 alert를 띄워 사전에 설치를 못 하도록 막는다. [Picture 4] 인증서 다이어그램 SSL Certificate SSL Certificate는 크게 2가지 기준으로 분류할 수 있다.\n검증히려는 대상 커버하는 도메인의 개수 검증하려는 대상 검증 대상에 따라서 암호화하는 방식이 달라지거나 하지 않는다. 따라서 저렴하게 발급받은 DV와 비싸게 발급받은 EV가 결국 보안의 관점에서는 큰 차이가 없다.\n하지만 검증받은 대상의 identiy가 다르기 때문에 DV는 작은 서비스에서 사용하기가 용이하고 그 외에 e-commerce나 중요한 정보를 주고 받아야 하는 서비스의 경우에는 다른 인증서를 써야 한다.\nDomain Validation 도메인 소유권을 확인한다. 손쉽게 발급받을 수 있다. (최대 1일) 가장 많이 사용되는 SSL Certificate의 형태다. Let’s encrypt에서 발급하는 인증서도 DV로, OV나 EV 처럼 다른 인증을 받을 때는 사용할 수 없다. Organization Validation 도메인 소유권 + Organization 존재를 확인한다. 회사 같은 경우, 실제로 존재하는 회사인지를 확인하기 위해 관련 서류를 제출하기도 한다. 발급하는데 몇 시간 또는 며칠 정도가 소요된다. Extended Validation 도메인 소유권 + Organization 존재 확인 + 물리적인 추가 절차를 확인한다. SSL certification industry’s governing consortium 가이드라인을 준수해야 하는 등 발급 절차가 복잡하다. 커버하는 도메인의 개수 인증서에는 도메인이라는 말이 자주 등장하게 된다. 하지만 도메인이라는 단어 자체가 뜻하는 의미가 모호할 수 있기 때문에 혼선방지 차원에서 인증서에서는 FQDN 이라는 단어를 더 선호한다.\nFQDN은 Fully-Qualified Domain Name 의 약자로 도메인 전체 이름을 표기하는 방식을 뜻한다.\nFQDN 예시\n# 아래는 서로 동일한 도메인이다. www.leelee.co.kr leelee.co.kr www.sub.leelee.co.kr # 아래는 서로 다른 FQDN 이다. www.leelee.co.kr leelee.co.kr Single Domain 단 하나의 FQDN을 커버한다. Multiple Domain 여러개의 FQDN을 커버한다. 인증서를 등록할 때 SAN (Subject Alternative Name) 항목에 다수의 도메인을 입력한다. Wildcard Domain 하나의 Root Domain과 무제한의 서브도메인을 커버한다. wildcard는 하나의 전체도메인을 입력하지 않기 때문에 FQDN이라 하지 않았다. CA 업체에서는 비용에 따라서 커버하는 서브도메인 개수에 제한을 두기도 한다. * 을 사용해서 도메인을 커버한다. 커버하는 도메인 예시\n# Singple Certificate www.leelee.co.kr # Multi-Domain Certificate www.leelee.co.kr leelee.co.kr www.sub.leelee.co.kr # Wildcard Certificate *.leelee.co.kr 인증서와 관련된 개념 PKI PKI는 Public Key Infrastructure 의 약자로 공개키 기반구조라고 하며 디지털 인증서를 생성, 관리, 배포, 사용, 저장 및 파기, 공개키 암호화의 관리에 필요한 정책 등을 뜻한다. PKI는 아래의 항목을 전체로 하고 있다.\n비대칭 키 알고리즘을 이용한다. Private key, Public key pair 를 생성한다. Private key는 개인만 소유하며 밖으로 공개해서는 안된다. Public key는 공개하는 키로, 누구에게든 공개해도 된다. 비대칭 키 알고리즘 Private key, Public key 두 개의 키를 사용한다. 이처럼 Encryption(암호화)와 Decryption(복호화)에 두 개의 다른 키를 쓰기 때문에 비대칭 키라고 한다. 대표적인 알고리즘으로는 RSA가 있다.\n적용 시나리오 Public key를 이용해 암호화 : 데이터 보안 사용자 B가 사전에 공유받은 사용자 A의 public key를 이용하여 데이터를 암호화한다. 사용자 A의 public key로 암호화된 데이터는 오직 사용자 A의 private key로 복호화 할 수 있다. 사용자 A의 private key는 사용자 A만 소유하고 있기 때문에 사용자 B는 사용자 A만 볼 수 있는 데이터를 전송하게 된 것이다. [Picture 5] Public key를 이용한 암호화 Private key를 이용해 암호화 : 신원인증 사용자 A의 private key로 암호화 된 데이터는 사용자 A의 public key를 이용해야만 복호화를 할 수 있다. 데이터가 사용자 A의 public key로 복호화가 안된다면 사용자A가 보냈다는 것을 인증할 수 없다. 사용자 A의 public key로 데이터를 복호화할 수 있다면 사용자 A가 보냈다는 것을 인증할 수 있다. [Picture 6] Private key를 이용한 암호화 해커가 암호화된 데이터를 도청할 경우 사용자 A의 private key가 없기 때문에 해커가 중간에서 데이터를 복호화ㅊ할 수 없다. [Picture 7] 해킹 시도 PKI에서 가능한 두 가지 방식의 네트워크 보안 public key를 이용해 암호화 하면 원하는 상대방에게만 데이터를 공개할 수 있다. from 사용자 B → to 사용자 A private key를 이용해 암호화 하면 신원 인증을 할 수 있다. from 사용자 A → to 사용자 B X.509 X.509는 디지털 인증서 생성 관련 국제 표준을(format)을 의미한다. X.509를 사용하는 네트워크 노드들은 전세계적으로 약속된 x.509 국제 표준을 방식으로 디지털 ID를 생성해 서로의 신원을 증명할 수 있다.\nX.509는 인터넷의 다양한 분야에서 신원 확인을 위해 광범위하게 사용되고 있는 가장 유명한 디지털 신원 증명 방식이다.\nX.509 version 3 인증서 양식 [Picture 8] X.509 인증서 양식 Key usage extension 인증서에 포함되어있는 public key의 목적을 나타낸다. Key usage extension을 설정해 public key의 사용처를 제한할 수 있다.\nKey usage extension 예시\n인증서가 서명을 하거나 signature를 검증하기 위해 사용된다면 Key usage extension으로 Digital signature와 Non-repudication 를 설정한다. 인증서가 key 관리를 위해서만 사용이 된다면 key encipherment를 설정한다. [Picture 9] Key usage extension 종류 Extended key usage Key usage extension이 기본적인 인증서의 사용 목적(purpose)를 나타내는 것이라면 Extended key usage는 인증서의 additional한 사용 목적을 나타낸다. 보통 Extended key usage는 end entity의 인증서에만 표시가 된다.\nExtended key usage 예시\ncritical이라면 인증서는 설정되어있는 용도로만 사용을 해야 한다. 해당 인증서를 다른 용도로 사용 했을 경우에는 CA 정책에 위반된다. non-critical 이라면 설정되어있는 용도 외의 다른 용도로 사용 했을 경우에도 CA의 제한에 걸리지 않는다. 또한 다수의 kye/certificate을 가지고 있는 entity라면 맞는 key/certificate를 찾는데에도 사용이 될 수 있다. [Picture 10] Extend key usage extension 종류 CSR Certificate Signing Request의 약자이다. CA에 인증서를 서명해달라고 요청할 때 사용이 된다. 인증서 발급할 때만 사용이 되고, 인증서가 발급된 후에는 별도로 사용 되지 않는다. Private Key PKI의 핵심이 되는 비밀키이다. 만료 기간이 별도로 존재하지 않는다. Public Key PKI의 또 다른 핵심이 되는 공개키이다. Public Key라고 따로 파일이 존재하기 보다는 Certificate에 포함이 되어있다. 때문에 public key와 certificate을 혼용해서 쓴다. 만료 기간이 별도로 존재하지 않는다. Certificate public key와 각종 정보를 담고 있는 인증서로, x.509 형식으로 되어있다. 만료 기간이 존재한다. 때문에 인증서가 만료되면 새로운 인증서를 발급받아야 한다. PKI File Extension PEM (Privacy Enhanced Mail) 형식 은 가장 흔하게 사용되는 X.509 인증서 형식인데 .crt, .pem, .cer, .key 확장자 모두가 PEM 형식이다. \u0026mdash;\u0026ndash;BEGIN CERTIFICATE\u0026mdash;\u0026ndash; 로 시작한다.\n.cert *.crt Certificate를 위한 확장자다. Base64 encoded X.509 certificate DER encoded X.509 certificate 해당 확장자들은 Private key를 지원하지 않는다. .key Private key 확장장다. .pem Certificate를 위한 확장자다. .crl Certificate Revoke List를 위한 확장자다. Certificate Revoke List는 폐기된 인증서의 목록이며, CA는 CRL을 통해 폐기된 인증서를 관리한다. .csr Certificate Singing Request를 위한 확장자다. .der Certificate을 위한 인증서 확장자인데, DER encoded X.509 Certificate에 한정된다. 해당 확장자는 Private key를 지원하지 않는다. 보통의 인증서들과 달리 ----BEGIN CERTIFICATE----- 로 시작하지 않는다. Java contexts에서 자주 사용이 된다. .der 인증서 예시\n3082 07fd 3082 05e5 a003 0201 0202 1068 1604 dff3 34f1 71d8 0a73 5599 c141 7230 0d06 092a 8648 86f7 0d01 010b 0500 3072 310b 3009 0603 5504 0613 0255 5331 0e30 0c06 0355 0408 0c05 5465 7861 7331 1030 0e06 0355 0407 0c07 486f 7573 746f 6e31 1130 0f06 0355 040a 0c08 5353 4c20 436f 7270 312e 302c 0603 5504 030c 2553 534c 2e63 6f6d 2045 5620 5353 4c20 496e 7465 726d 6564 6961 7465 2043 4120 5253 4120 ","date":"2022-06-15","permalink":"https://leeleelee3264.github.io/post/2022-06-15-digital-certificate-part-one/","tags":["Infra"],"title":"Digital certificate"},{"content":"\n책 [Effective Python] better way를 1장부터 5장까지 요약한다.\nIndex\n사용중인 파이썬의 버전을 알아두라 PEP8 스타일 가이드를 따르라 bytes와 str의 차이를 알아두라 C 스타일 형식 문자열을 str.format과 쓰기보다는 f-문자열을 통한 인터플레이션을 사용하라 복잡한 식을 쓰는 대신 도우미 함수를 작성하라 사용중인 파이썬의 버전을 알아두라 파이썬 버전 콘솔 명령어 예시\n1 python --version 파이썬 버전 코드 예시\n1 2 3 4 5 6 7 import sys print (sys.version_info) \u0026gt;\u0026gt;\u0026gt; sys.version_info(major=3, minor=8, micro=12, releaselevel=\u0026#39;final\u0026#39;, serial=0) print(sys.version) \u0026gt;\u0026gt;\u0026gt; 3.8.12 (default, Oct 22 2021, 17:47:41) \u0026gt;\u0026gt;\u0026gt;[Clang 13.0.0 (clang-1300.0.29.3)] 파이썬2 파이썬2는 2020년 1월 1일부로 수명이 다했다. 이제 더이상 버그수정, 보안 패치가 이뤄지지 않는다. 공식적으로 지원을 하지 않은 언어이기 때문에 사용에 대한 책임은 개발자에게 따른다.\n파이썬2로 작성된 코드를 사용해야 한다면 2to3, six와 같은 라이브러리의 도움을 받아야 한다.\nsix 사용 예시\n1 2 3 4 return ( six.text_type(user.pk) + six.text_type(timestamp) + six.text_type(user.username) + six.text_type(user.uuid) + six.text_type(user.type) ) PEP8 스타일 가이드를 따르라 PEP8는 파이썬 코드를 어떤 형식으로 작성할지 알려주는 스타일 가이드다. 문법만 올바르다면 어떤 방식으로 원하든 파이썬 코드를 작성해도 좋지만, 일관된 스타일을 사용하면 코드에 더 친숙하게 접근하고, 코드를 더 쉽게 읽을 수 있다.\nPEP8 스타일 가이드 공백 탭 대신 스페이스를 사용해 들여쓰기를 하라. 문법적으로 중요한 들여쓰기에는 4칸 스페이스를 사용하라. 라인 길이는 79개 문자 이하여야 한다. 긴 식을 다음 줄에 이어서 쓸 경우에는 일반적인 들여쓰기보다 4스페이스를 더 들여써야 한다. 파일 안에서 각 함수와 클래스 사이에는 빈 줄 두 줄 넣어라. 클래스 안에서 메서드와 메서드 사이에는 빈 줄을 한 줄 넣어라. dict에서 키와 콜론(:) 사이에는 공백을 넣지 않고, 한 줄 안에 키와 값을 같이 넣는 경우에는 콜론 다음에 스페이스를 하나 넣는다. 변수 대입에서 = 전후에는 스페이스를 하나씩만 넣는다. 타입 표기를 덧붙이는 경우에는 변수 이름과 콜론 사이에 공백을 넣지 않도록 주의하고, 콜론과 타입 정보 사이에는 스페이스를 하나 넣어라. 공백 예시\n1 2 def add(a: int, b: int): return a + b 명명 규약 (name convention) 함수, 변수, 애트리뷰트 명명 규약 함수, 변수, 애트리뷰트는 lowercase_underscore 처럼 소문자와 밑줄을 사용해야 한다. snake case 모듈 수준의 상수는 ALL_CAPS처럼 모든 글자를 대문자로 하고, 단어와 단어 사이를 밑줄로 연결한 형태를 사용한다. private 명명 규약 보호해야(protect) 하는 인스턴스 애트리뷰트는 일반적인 애트리뷰트 이름 규칙을 따르되, 밑줄로 시작한다. private 인스턴스 애트리뷰는 밑줄 두 줄로 시작한다. 밑줄 두 줄은 maigc method를 위한 컨벤션인줄 알았는데, 클래스에서 private 메소드와 애트리뷰트를 밑줄 두줄로 만든다. 파이썬에서 private, protect 모두 정식 지원을 하지 않기 때문에, 가독성을 위한 네임 컨벤션에 더 가깝다. 클래스 명명 규약 클래스와 예외는 CapitalizedWord 처럼 여러 단어를 이어 붙이되, 각 단어의 첫 글자를 대문자로 만든다. PascalCase, CamelCase 클래스에 들어있는 인스턴스 메서드는 호출 대상 객체를 가리키는 첫번쨰 인자의 이름으로, 반드시 self를 사용해야한다. 자기자신을 가르키기 때문에 클래스 메서드는 클래스를 가르키는 첫 번째 인자의 이름으로 반드시 cls를 사용해야 한다. 클래스 자체를 가르키기 때문에 함수, 변수 명명 예시\n1 2 3 4 _count = 100 def _add(a, b): return a + b private 인스턴스 명명 예시\n1 2 3 4 __count = 100 def __add(a, b): return a + b 식과 문 읽기 쉬운 식 작성 긍정적인 식을 부정하지 말고, 부정을 내부에 넣어라. 한 줄짜리 if 문이나 한 줄짜리 for, while 루프, 한 줄 짜리 except 복합문을 사용하지 말라. 명확성을 위해 각 부분을 여러 줄에 나눠 배치하라. 식을 한 줄 안에 다 쓸 수 없는 경우, 식을 괄호로 둘러싸고 줄바꿈과 들여쓰기를 추가해서 읽기 쉽게 만들라. 여러 줄에 걸쳐 식을 쓸 때는 줄이 계속된다는 표시를 하는 \\ 문자 보다는 괄호를 사용하라. editor를 사용하다보면 이렇게 \\ 되는 형식을 많이 사용하는데, PEP8에서 권장하지 않은 형태였다니, 앞으로 ()를 써보도록 노력하겠다. 빈 컨테이너 확인 빈 컨테이너나 시퀸스([], ‘’)등을 검사할 때는 길이를 0과 비교하지 말라. 빈 컨테이너나 시퀸스 값이 암묵적으로 False 취급된다는 사실을 활용해라. 파이썬을 자바와 동실시 해서, 얼마전에 str ≠ ‘’ 로 비어있는 여부를 검사한 적이 있다. 파이썬에서 비어있는 str는 False 취급이라는 것을 꼭 기억하자. 마찬가지로 비어 있지 않은 컨테이너나 시퀸스([1], ‘hi’) 등을 검사할 때는 길이를 0과 비교하지 말라. 컨테이너가 비어있지 않은 경우 암묵적으로 True 평가가 된다는 사실을 활용해라. 내부 부정 예시\n1 2 3 4 5 6 7 8 # 긍정적인 식을 부정 if not a is b: # 부정을 내부에 넣음 if a is not b: # 비교 대상이 없이 자기 자신만 있을 때는 이렇게 해도 된다. if not a: 빈 컨테이너 확인 예시\n1 2 3 4 5 6 7 8 9 10 11 12 d = {} st = \u0026#39;\u0026#39; if not d: if not st: # Don\u0026#39;t do 1 if str == \u0026#39;\u0026#39;: # Dont\u0026#39;do 2 if len(d) == 0: 임포트 임포트 문은 평소에 정말 신경을 쓰지 않았는데, 앞으로는 editor에서 어떻게 임포트를 하는지 보고, 신경 써서 작성하도록 하자.\nimport 문을 항상 파일 맨 앞에 위치시켜라. 모듈을 임포트할 때는 절대적인 이름을 사용하고, 현 모듈의 경로에 상대적인 이름은 사용하지 말라. 반드시 상대적인 경로로 임포트를 해야 하는 경우에는 명시적인 구문을 사용하라. 임포트 순서 임포트를 적을 때는 아래와 같은 순서로 섹션을 나누고, 각 세션은 알파벳 순서로 모듈을 임포트하라. 표준 라이브러리 모듈 서드 파티 모듈 여러분이 만든 모듈 임포트 예시\n1 2 3 4 5 6 # bar 패키지에서 foo 모듈을 임포트 하려고 할 때 # Do from bar import foo # Don\u0026#39;t do import foo 상대 경로 임포트 예시\n1 from . import foo 결론 개인 프로젝트를 할 때에도 pylint, flake8 등의 파이썬 lint를 이용해서 스타일을 준수하도록 노력하자.\nbytes와 str의 차이를 알아두라 파이썬에는 문자열 데이터의 시퀸스를 표현하는 두 가지 타입 bytes와 str이 있다.\nbytes 부호가 없는 8바이트 데이터가 그대로 들어가거나, 아스키 인코딩을 사용해 내부 문자를 표시한다.\nbytes 예시\n1 2 a = b\u0026#39;h\\x6511o\u0026#39; c = b\u0026#39;eojwpkmcdlklksm\u0026#39; str 사람이 사용하는 언어의 문자를 표현하는 유니코드 코드 포인트가 들어간다.\n인코딩 str에는 직접 대응하는 이진 인코딩이 없고, bytes에는 직접 대응하는 텍스트 인코딩이 없다.\n때문에 함수를 호출해야 한다.호출 할 때 여러분이 원하는 인코딩 방식을 명시적으로 지정할 수 도 있고 시스템 디폴트 인코딩을 사용할 수 있는데 일반적으로 utf-8이 시스템 디폴트다.\n인코딩 시나리오 유니코드 데이터를 이진 데이터로 변환해야 할 때: str의 encode 메서드 호출 이진 데이터를 유니코드 데이터로 변환해야 할 때: bytes의 decode 메서드 호출 유니코드 샌드위치 유니코드 데이터를 인코딩하거나 디코딩하는 부분을 인터페이스의 가장 먼 경계 지점에 위치시켜라. 이런 방식을 유니코드 샌드위치라고 부른다.\n프로그램의 핵심 부분은 유니코드 데이터가 들어 있는 str를 사용해야 하고, 문자 인코딩에 대해 어떤 가정도 해서는 안된다.\n핵심 함수에는 이미 인코딩이 된 str이 인자로 전달되어야 한다. 이런 접근을 하면 다양한 텍스트 인코딩으로 입력 데이터를 받아들일 수 있고, 출력 텍스트 인코딩은 한 가지로 엄격하게 제한할 수 있다. [Picture 1] 유니코드 샌드위치 블랙박스 str 반환 유니코드 샌드위치 예시\n1 2 3 4 5 6 7 def to_str(bytes_or_str): if isinstance(bytes_or_str, bytes): value = bytes_or_str.decode(\u0026#39;utf-8) else: value = bytes_or_str return value bytes 반환 유니코드 샌드위치 예시\n1 2 3 4 5 6 7 def to_bytes(bytes_or_str): if isinstanceof(bytes_or_str, str): value = bytes_or_str.endode(\u0026#39;utf-8\u0026#39;) else value = bytes_or_str return value 파이썬에서 bytes와 str을 다룰 때 기억해야 하는 점 연산 bytes와 str이 똑같이 작동하는 것처럼 보이지만 각각의 인스턴스는 서로 호환되지 않기 때문에 전달중인 문자 시퀸스가 어떤 타입인지 알아야 한다.\n연산 불가항목 예시\n1 2 3 bytes + str bytes \u0026gt; str b\u0026#39;foo\u0026#39; == \u0026#39;foo\u0026#39; \u0026gt;\u0026gt;\u0026gt; False 파일 (내장함수 open을 호출해 얻은) 파일 핸들과 관련한 연산들이 디폴트로 유니코드 (str) 문자열을 요구하고, 이진 바이트 문자열을 요구하지 않는다.\n이진 쓰기 모드 (’wb’)가 아닌 텍스트 쓰기 모드 (’w’) 로 열면 오류가 난다. bytes로 파일 읽기, 쓰기를 하고 싶다면 이진 읽기모드 (’rb’) 또는 이진 쓰기모드 (’wb’)를 써야 한다.\n파일 불가항목 예시\n1 2 3 4 5 with open(\u0026#39;data.bin\u0026#39;, \u0026#39;w\u0026#39;) as f: f.write(b\u0026#39;\\xf1\\xf2\\xf3\u0026#39;) \u0026gt;\u0026gt;\u0026gt; Traceback ... TypeError: write() argument must be str, not bytes. 파일 시스템의 디폴트 텍스트 인코딩을 bytes.encode(쓰기), str.decode(읽기) 에서 사용하는데 utf-8이기 때문에 이진데이터의 경우 utf-8로 읽지 못하는 경우가 생겨 에러가 발생할 수 있다.\n이런 현상을 막고자, utf-8로 인코딩을 못하는 경우에는 읽어올 때 인코딩을 아예 명시해주는 경우도 있다.\n인코딩 명시 예제\n1 2 with open(\u0026#39;data.bin\u0026#39;, \u0026#39;r\u0026#39;, encoding=\u0026#39;cpl252\u0026#39;) as f: data = f.read() 결론 시스템 디폴트 인코딩을 항상 검사하도록 하자.\n1 python3 -c \u0026#39;import locale; print(locale.getpreferredencoding())\u0026#39; C 스타일 형식 문자열을 str.format과 쓰기보다는 f-문자열을 통한 인터플레이션을 사용하라. 형식화는 미리 정의된 문자열에 데이터 값을 끼워 넣어 사람이 보기 좋은 문자열로 저장하는 과정이다. 파이썬에서는 f-문자열을 통한 인터플레이션 빼고는 각자의 단점이 있다.\n형식 문자열 % 형식화 연산자를 사용한다. 왼쪽에 들어가는 미리 정의된 텍스트 템플릿을 형식 문자열이라고 부른다. C 함수에서 비롯된 방법이다.\n형식 문자열 예시\n1 print(\u0026#39;이진수: %d, 십육진수: %d\u0026#39;, %(a, b)) 형식 문자열 문제점 형식화 식에서 오른쪽에 있는 값들의 순서를 바꾸거나, 타입을 바꾸면 포멧팅이 안되어 오류가 발생한다. 형식화를 조금이라도 변경하면 식이 매우 복잡해 읽기가 힘들다. 같은 값을 여러번 사용하고 싶다면 오른쪽 값을 여러번 복사해야 한다. 내장 함수 format과 str.format 파이썬 3부터는 %를 사용하는 오래된 C스타일 형식화 문자열보다 더 표현력이 좋은 고급 문자열 형식화 기능이 도입됐다. 이 기능은 format 내장 함수를 통해 모든 파이썬 값에 사용할 수 있다.\nformat 예시\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 key = \u0026#39;my_var\u0026#39; value = 1.234 formatted = \u0026#39;{} = {}\u0026#39;.format(key, value) print(formatted) \u0026gt;\u0026gt;\u0026gt; my_var = 1.234 # format 메서드에 전달된 인자의 순서를 표현하는 위치 인덱스를 전달할 수도 있다. formatted = \u0026#39;{1} = {0}\u0026#39;.format(key, value) print(formatted) \u0026gt;\u0026gt;\u0026gt; 1.234 = my_var # 형식화 문자열 안에서 같은 위치 인덱스를 여러 번 사용할 수도 있다. formatted = \u0026#39;{}는 음식을 좋아해. {0}가 요리하는 모습을 봐요\u0026#39;.format(name) print(formatted) \u0026gt;\u0026gt;\u0026gt; 철수는 음식을 좋아해. 철수가 요리하는 모습을 봐요. format 문제점 C 스타일의 형식화와 마찬가지로, 값을 조금 변경하는 경우에는 코드 읽기가 어려워진다. 가독성 면에서 거의 차이가 없으며, 둘 다 읽기에 좋지 않다. 인터폴레이션을 통한 형식 문자열 (f-문자열) 위의 문제들을 완전히 해결하기 위해 파이썬 3.6 부터는 인터폴레이션을 통한 형식 문자열 (짧게 f-문자열)이 도입되었다. 이 새로운 언어 문법에서는 형식 문자열 앞에 f 문자를 붙여야 한다. 바이트 문자열 앞에 b 문자를 붙이고, raw 문자열 앞에 r문자를 붙이는 것과 비슷하다.\nf-문자열은 형식화 식 안에서 현재 파이썬 영역에서 사용할 수 있는 모든 이름을 자유롭게 참조할 수 있도록 허용함으로써 이런 간결함을 제공한다.\nf-문자열 예시\n1 2 3 4 5 6 7 8 key = \u0026#39;my_var\u0026#39; value = 1.234 formatted = f\u0026#39;{key} = {value}\u0026#39; print(formatted) \u0026gt;\u0026gt;\u0026gt; my_var = 1.234 파이썬에서 제공하는 형식화 문법의 차이 예시\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # f-문자열 f_string = f\u0026#39;{key:\u0026lt;10} = {value: .2f}\u0026#39; # c 스타일 c_tuple = \u0026#39;%-10s = %.2f\u0026#39; % (key, value) # c 스타일 + 딕셔너리 c_dict = \u0026#39;%{key}-10s = %{value}.2f\u0026#39; % {\u0026#39;key\u0026#39;: key, \u0026#39;value\u0026#39;: value} # str.format str_args = \u0026#39;{:\u0026lt;10} = {value: .2f}\u0026#39;.format(key, value) # str.format + 키워드 인자 str_kw = \u0026#39;{key:\u0026lt;10} = {value:.2f}\u0026#39;.format(key=key, value=value) 결론 f-문자열은 간결하지만, 위치 지정자 안에 임의의 파이썬 식을 직접 포함시킬 수 있으므로 매우 강력하다. 값을 문자열로 형식화해야 하는 상황을 만나게 되면 다른 대안 대신 f-문자열을 택하라.\n복잡한 식을 쓰는 대신 도우미 함수를 작성하라. 파이썬은 문법이 간결하므로 상당한 로직이 들어가는 식도 한 줄로 매우 쉽게 작성할 수 있다. 예를 들어 URL의 query string를 파싱하고 싶다고 하자.\nquery string 파싱 예시\n1 2 3 4 5 6 7 8 from urllib.parse import parse_qs my_values = parse_qs(\u0026#39;빨강=5\u0026amp;파랑=0\u0026amp;초록=\u0026#39;, keep_blank_values=True) print(repr(my_values)) \u0026gt;\u0026gt;\u0026gt; {\u0026#39;빨강\u0026#39;: [\u0026#39;5\u0026#39;], \u0026#39;파랑\u0026#39;: [\u0026#39;0\u0026#39;], \u0026#39;초록\u0026#39;: [\u0026#39;\u0026#39;] } 이런 때에 파라미터가 없거나 비어 있을 경우 0이 디폴트 값으로 대입되면 좋을 것이다. 여러 줄로 작성해야 하는 if 문(if statement)를 쓰거나 도우미 함수를 쓰지 않고, if 식(if expression)으로 한줄로 처리 할 수 있다.\n뿐만 아니라, 모든 파라메터 값을 정수로 변환해서 즉시 수식에서 활용하기를 바란다고 해보겠다. 그럼 식은 아래와 같아진다.\n한 줄 대입, 변환 예시\n1 2 3 red = int(my_values.get(\u0026#39;빨강\u0026#39;, [\u0026#39;\u0026#39;])[0] or 0) green = int(my_values.get(\u0026#39;초록\u0026#39;, [\u0026#39;\u0026#39;])[0] or 0) opacity = int(my_values.get(\u0026#39;투명도\u0026#39;, [\u0026#39;\u0026#39;])[0] or 0) 현재 이 코드는 너무 읽기 어렵고 시각적 잡음도 많다. 즉, 코드를 이해하기 쉽지 않으므로 코드를 새로 읽는 사람이 이 식이 실제로 어떤 일을 하는지 이해하기 위해 너무 많은 시간을 투자해야 한다.\n코드를 짧게 유지하면 멋지기는 하지만, 모든 내용을 한줄에 우겨 넣기 위해 노력할 만큼의 가치는 없다.\n명확하게 바꾼 코드 예시\n1 2 3 4 5 6 7 def get_first_int(values, key, default=0): found = values.get(key, [\u0026#39;\u0026#39;]) if found[0]: return int(found[0]) return default 식이 복잡해지기 시작하면 바로 식을 더 작은 조간으로 나눠서 로직을 도우미 함수로 옮길지 고려해야 한다. 특히 같은 로직을 반복해 사용할 때는 도우미 함수를 꼭 사용하라. 아무리 짧게 줄여 쓰는 것을 좋아한다 해도, 코드를 줄여 쓰는 것보다 가독성을 좋게 하는 것이 더 가치 있다.\n결론 파이썬의 함축적인 문법이 지저분한 코드를 만들어내지 않도록 하라. 반복하지 말라는 뜻의 DRY 원칙을 따르라.\n","date":"2022-05-06","permalink":"https://leeleelee3264.github.io/post/2022-05-06-effective-python-betteryway-1-to5/","tags":["Book"],"title":"[Effective Python] Chapter 1: 1장부터 5장까지의 요약"},{"content":"\n책 [Introducing Python] 절반을 요약한다.\nIndex\n파이 맛보기 파이 재료: 숫자, 문자열 변수 파이 채우기: 리스트, 튜플, 딕셔너리, 셋 파이 크러스트: 코드 구조 파이 포장하기: 모듈, 패키지, 프로그램 파이 맛보기 파이썬은 인터프리터 언어다. 파이썬이 개발을 빠르게 할 수 있다는 이유도 거대한 크기의 기계어를 만들어내지 않고 실행이 가능하기 때문으로 추정된다.\n인터프리터 언어 대표적인 인터프리터 언어는 파이썬, 자바 스크립트가 있다. 인터프리터는 소스코드를 바로 실행한다. 인터프리터는 바로바로 실행을 하다보니 처음에 속도가 비교적 빠르다. 컴파일 언어 대표적인 컴파일 언어는 자바다. 컴파일 언어는 처음 프로그램을 시작할 때 모든 코드를 기계어로 바꾼다 (컴파일 한다). 컴파일을 하기 때문에 처음에 프로그램을 실행할 때 시간이 오래 걸린다. 하지만 한 번 컴파일을 하면 실행시 기계어를 불러와 더 빨리 실행을 할 수 있어 매번 실행시마다 번역을 거치는 인터프리터 언어보다 속도가 더 빨라진다. 파이 재료: 숫자, 문자열 변수 파이썬은 문자를 변형 할 수 없다. st[0] = \u0026rsquo;e\u0026rsquo; 이런식으로 변경을 할 수 없다. 즉, 불변객체이다.\n문자열도 중간에 수정을 할 수 없고 아예 객체를 새로 만드는 replace() 메서드를 이용해서 문자열을 변경해야 한다. 문자열을 불변으로 만드는 이유는 Java와 마찬가지로 프로그래밍을 편리하게 하기 위해서다.\n슬라이싱을 이용한 문자 reverse 예시\n1 2 st = \u0026#34;eererewrvfgrhfos\u0026#34; re_st = st[::-1] # sofhrgfvrwereree 파이 채우기: 리스트, 튜플, 딕셔너리, 셋 파이썬에서 기본으로 제공하는 자료구조에 대해 알아본다.\ndel 자료구조에서 항목을 삭제하는 커맨드는 del 인데, 이 del은 자료구조의 함수가 아닌 파이썬 구문이다. del은 객체로부터 이름을 분리하고 객체의 메모리를 비워준다.\ndel 예시\n1 2 3 del full[2] # full.del(2) 처럼 쓰지 못한다. 리스트 리스트는 변경 가능하다. 항목을 할당하고, 자유롭게 수정 삭제를 할 수 있다.\n리스트 예시\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # 생성 empty = [] empyt2 = list() full = [1, 2, 3] # 리스트의 끝에 항목 추가하기 append() full.appned(4) # 리스트 병합하기 extend() added = [5, 6, 7, 8] full.extend(added) # [1, 2, 3, 4, 5, 6, 7, 8] full += added # [1, 2, 3, 4, 5, 6, 7, 8] # append 를 쓰면 리스트 자체가 추가된다 full.appned(added) # [1, 2, 3, 4, [5, 6, 7, 8]] # 리스트 정렬 # sort()는 리스트 자체를 내부적으로 정렬한다. # sorted() 는 리스트의 정렬된 복사본을 반환한다. full.sort() new_sort = full.sorted() 튜플 튜플은 불변한다. 튜플에 항목을 할당하고 나서는 바꿀 수 없다. 때문에 튜플을 상수 리스트라 볼 수 있다.\n튜플 예시\n1 2 3 4 5 6 7 8 9 10 # 생성 empty_tuple = () # 콤마로 값을 나열해도 튜플을 만들 수 있다. empty_tuple = 1, 2, 3 empty_tuple = (1, 2, 3) # 튜플의 나열하는 특성을 이용해서 객체 생성없이 swap하기 password = \u0026#39;12\u0026#39; icecream = \u0026#39;sweet\u0026#39; password, icecream = icecream, password 리스트가 아닌 불변객체라 함수 지원이 더 적은 튜플을 사용하는 이유 더 적은 공간을 사용한다. 실수로 값을 바꿀 위험이 없다. 튜플을 딕셔너리 키로 사용이 가능하다. 네임드 튜플 은 객체의 대안이 될 수 있다. 함수의 인자들은 튜플로 전달된다. 딕셔너리 딕셔너리 예시\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # 생성 empty_dict = {} # dict() 를 이용해 두 값 쌍으로 이뤄진 자료구조를 딕셔너리로 변환할 수 있다. lol = [[\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;], [\u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;]] lol2 = lol #{\u0026#39;a\u0026#39;: \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;: \u0026#39;d\u0026#39;} lol3 = (\u0026#39;ab\u0026#39;, \u0026#39;cb\u0026#39;) lol4 = lol #{\u0026#39;a\u0026#39;: \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;: \u0026#39;d\u0026#39;} # 딕셔너리 결합하기 update() em = {\u0026#39;a\u0026#39;: \u0026#39;b\u0026#39;} em2 = {\u0026#39;c\u0026#39;: \u0026#39;d\u0026#39;} em.update(em2) # {\u0026#39;a\u0026#39;: \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;: \u0026#39;d\u0026#39;} # 딕셔너리 비우기 em.clear() # 딕셔너리에 특정 키가 들어있나 확인 \u0026#39;c\u0026#39; in em # 모든 키 가져오기 em.keys() # 모든 값 가져오기 em.values() # 모든 키, 값 가져오기 # (\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;), (\u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;) 처럼 튜플로 반환한다 em.items() 셋 어떤 것이 존재하는지 여부만 판단하기 위해서 셋을 사용한다. 중복을 허용하지 않는다. 셋은 수학시간에 배웠던 집합과 아주 유사하다.\n셋 예시\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 # 생성 # 그냥 {} 는 딕셔너리 생성자에 선점되었다. empty_set = set() empty_set2 = {1, 2, 3, 4} # 각종 집합 # 교집합 \u0026amp;, intersection() if contents \u0026amp; {\u0026#39;ice\u0026#39;, \u0026#39;cream\u0026#39;} # ice 와 cream 모두 들어있어야 참 a = {1, 2} b = {3, 4} a \u0026amp; b = 2 # 합집합 |, union() a | b = {1, 2, 3} # 차집합 -, difference() a - b = {1} # 대칭 차집합 ^, symmetric_difference # 각 항목에 별개로 가지고 있는 값을 구한다. a ^ b = {1, 3} # 부분집합 \u0026lt;=, issubset() a.issubset(b) # False a.issubset(a) # True a.issubset((1, 2, 3)) # 슈퍼셋 \u0026gt;=, issuperset() a.issuperset((1)) # True ((1, 2, 3)).issuperset(a) # True a.issuperset(a) # True 파이 크러스트: 코드 구조 컴프리헨션 내가 가끔 검색해보고는 하는 한 줄 로 for 문 돌리기와 유사하다.\n하나 이상의 이터레이터로부터 파이썬 자료구조를 만드는 방법이다. 더 파이써닉한 용법이라니는데 간단한 할당문 말고는 컴프리헨션을 사용하면 더 헷갈릴 것 같다.\n한 줄 for 문 예시\n1 num = [i for i in range(1, 6)] 인자 다른 언어들과 마찬가지로, 값을 순서대로 상응하는 매개변수에 복사하는 것이 위치인자이다. 키워드인자는 위치인자의 혼동을 피하기 위해 상응하는 이름을 인자 안에 지정한 것이다.\n인자 예시\n1 2 3 4 5 6 7 8 9 10 11 # 위치 인자 def menu(wine, entree, dessert): pass # 키워드 인자 def menu(wine=wine, entree=entreee, dessert=dessert): pass # 인자의 기본 값 지정 def menu(wine, entree, dessert=\u0026#39;pie\u0026#39;): pass 인자 모으기 예시\n1 2 3 4 5 6 7 8 9 10 11 12 # 위치 인자 모으기 * def print_args(one, two, three, *args): pass # 실제로 호출 시 three까지 위치에 따라 값이 들어가고 나머지는 *args가 인자를 취하게 해준다. print_args(1, 2, 3, 4, 5, 6, 7, 8) # 키워드 인자 모으기 ** def print_keyword(**kwargs) # 실제 호출 시 위치인자와 마찬가지로, 함수에 따로 정의가 안 된 위치인자를 취한다. print_keyword(one=1, two=2, three=3, four=4, five=5) 여러가지 종류의 인자들을 섞어서 사용하려면 함수를 정의할 때 위치인자, 키워드 인자, *args, **kwargs 순으로 정의를 해줘야한다.\ndocstring 파이썬 문서화에 관련된 부분. 일반 주석은 #을 사용하지만, 모듈과 클래스와 메소드에 사용하는 주석의 형태는 따로 있다. 이것을 doctstring 이라고 한다.\ndocstring 예시\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 \u0026#34;\u0026#34;\u0026#34; 모듈 (파이썬 파일) 최상단에 이런 형식으로 주석을 달아주세요. Usage: python my_test.py \u0026lt;param\u0026gt; \u0026#34;\u0026#34;\u0026#34; class TestClass: \u0026#34;\u0026#34;\u0026#34; 클래스 아래에 이런 형식으로 주석을 달아주세요. \u0026#34;\u0026#34;\u0026#34; def test_method(): \u0026#34;\u0026#34;\u0026#34; 함수 아래에 이런 형식으로 주석을 달아주세요.\t\u0026#34;\u0026#34;\u0026#34; pass docstring을 이용해서 주석을 달아두면 코드에서 help 함수를 사용해 접근을 할 수 있다.\n내가 지금 사용하는 클래스가 뭘 하는 애인지 해당 클래스 파일을 읽지 않아도 콘솔에 입력만 하면 볼 수 있다는 장점이 있다.\nhelp 함수로 docstring 접근 예시\n1 2 help(TestClass) TestClass.__doc__ 라인 유지하기 PEP 에 따르면 파이썬은 한 줄에 80글자를 넘으면 안된다. 가독성이 제일 중요한 언어에서 가독성이 떨어지기 때문이다. 그래서 긴 문장을 사용해야 할 때에는 백슬래시(\\)로 라인을 끊어준다.\n백슬래시 예시\n1 2 3 4 5 6 7 8 test = \u0026#34;this\u0026#34; + \\ \u0026#34;is very very\u0026#34; + \\ \u0026#34;long long line\u0026#34; # 추천하지 않는 라인 끊는 방법 test = \u0026#34;\u0026#34; test += \u0026#34;is very very\u0026#34; test += \u0026#34;long long line\u0026#34; 일등 시민 : 함수 함수는 뷸변하기 때문에 딕셔너리의 키로 사용할 수 있다.\n함수를 변수에 할당할 수 있고, 다른 함수에서 이를 인자로 쓸 수 있으며, 함수에서 함수를 반환할 수 있다. 파이썬에서 괄호 ()는 함수를 호출 한다는 의미로 사용되고, 괄호가 없으면 함수를 객체 처럼 간주한다.\n함수에서 함수를 반환하는 예시\n1 2 3 4 5 6 7 8 def run_something_with_args(func, arg1, arg2): func(arg1, arg2) def add_args(arg1, arg2): return arg1 + agr2 \u0026gt;\u0026gt; run_something(add_agrs, 5, 8 ) 14 예제에서 run_something_with_args로 전달된 add_args 는 괄호 없이 객체처럼 취급되어 func 매개변수로 할당된다. 뒤에 괄호 () 가 붙은 func 는 전달 받은 arg1, arg2를 매개변수로 해 함수를 호출한다. 내부 함수 먼저 읽으면 좋을 자료 [Real Python: adding behavior with inner functions decorators] 함수 안에 또 다른 함수를 정의한다. 함수를 global scope 으로부터 완전히 숨겨 encapsulation을 하거나, 복잡한 작업을 하기 위해 Helper 함수를 만들어야 할 때 내부함수를 쓴다.\n내부 함수 예시\n1 2 3 4 5 6 7 def increment(number): def inner_increment(): return number + 1 return inner_increment() \u0026gt;\u0026gt; increment(10) 11 위처럼 작성을 하면 inner_increment 함수를 어디에서도 호출을 할 수 없다.\n내부 함수를 이용해 Helper를 만든 예시\n1 2 3 4 5 6 7 8 9 10 11 12 def factorial(number): if not isinstance(number, int): raise TypeError(\u0026#34;Sorry. \u0026#39;number\u0026#39; must be an integer.\u0026#34;) if number \u0026lt; 0: raise ValueError(\u0026#34;Sorry. \u0026#39;number\u0026#39; must be zero or positive.\u0026#34;) def inner_factorial(number): if number \u0026lt;= 1: return 1 return number * inner_factorial(number - 1) return inner_factorial(number) 그런데 내부함수로 Helper 함수를 만들기보다는 private 으로 Helper 함수를 만드는 것을 권장한다. private helper 함수가 훨씬 더 코드 읽기가 편하고 같은 모듈이나 클래스에서만 재사용이 가능하기 때문이다.\nprivate을 이용해 Helper를 만든 예시\n1 2 3 4 5 6 7 8 9 10 11 12 def factorial(number): if not isinstance(number, int): raise TypeError(\u0026#34;Sorry. \u0026#39;number\u0026#39; must be an integer.\u0026#34;) if number \u0026lt; 0: raise ValueError(\u0026#34;Sorry. \u0026#39;number\u0026#39; must be zero or positive.\u0026#34;) return _factorial(number) def _factorial(number): if number \u0026lt;= 1: return 1 return number * inner_factorial(number - 1) 클로저 클로저는 바깥 함수로부터 전달된 변수값을 저장하고, 변경을 할 수 있는 함수이다. 파이썬에서 함수를 변수에 할당할 수 있는 이유도 클로저 기능을 지원하기 때문이다.\n클로저 예시 generate_power\n1 2 3 4 5 6 7 8 9 10 11 12 # closure factory function def generate_power(exponent): def power(base): return base ** exponent return power raise_two = generate_power(2) \u0026gt;\u0026gt; raise_two(4) 16 \u0026gt;\u0026gt; raise_two(5) 25 클로저 호출 과정 상세 보기 클로저의 개념이 처음이다보니 제대로 이해가 가지 않아 print로 디버깅을 해가며 이해를 진행했다.\n클로저 예시 generate_power_with_debug\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 def generate_power_with_debug(exponent): print(f\u0026#39;closure generated, passed exponent {exponent}\u0026#39;) def power(base): print(f\u0026#39;inner function in closure. passed base {base}\u0026#39;) return base ** exponent return power # closure 생성 raise_two = generate_power_with_debug(2) # closure 호출 print(f\u0026#39;result of closure : {raise_two(4)}\u0026#39;) # 콘솔에 출력된 결과 closure generated, passed exponent 2 inner function in closure. passed base 4 result of closure : 16 generate_power 호출 과정 (1)\n1 raise_two = generate_power_with_debug(2)) generate_power_with_debug 로 변수 exponent에 2를 넣어 클로저 생성한다. 클로저는 매번 호출될 때마다 새로운 클로저를 생성한다. 내부 함수 power은 호출이 되지 않고, 새로운 power 인스턴스를 생성해 리턴이 된다. 리턴값이 함수라는 얘기다. power를 리턴할 때 power의 surrounding state 를 스냅셧으로 남긴다. 여기에는 exponent 변수가 포함되어있다. generate_power 호출 과정 (2)\n1 print(f\u0026#39;result of closure : {raise_twon(4)}\u0026#39;) generate_power_with_debug 클로저를 호출한다. 클로저를 호출함에 따라 변수 base에 4를 넣어 내부함수 power가 호출한다. power는 클로저가 리턴되었을 때 함께 넘어왔던 surrounding state의 스냅샷에 저장이 된 exponent를 이용한다. power 결과를 리턴한다. 클로저 호출 시나리오 총정리\nQ: 어떻게 내부함수를 호출할 때 외부함수의 값에 접근을 할까? A: 클로저를 생성할 때 내부함수를 리턴하는데, 이때 외부함수의 상태 스냅샷을 함께 리턴해주기 때문이다. 클로저를 구분할 수 있는 부분은 내부함수를 괄호() 로 호출하지 않다는 것이다. 예제에서 power를 리턴하기만 하는데, 이렇게 리턴을 하면 exponent 값을 저장한 power 함수의 복사본을 주게 된다. 복사본을 할당 받은 변수 raise_two를 실제로 매개변수를 넣고 호출한다. 매개변수는 내부함수인 power 의 base 와 맵핑이 된다. 클로저로 권한 확인 함수 구현 클로저로 구현한 권한 확인 함수\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def has_permission(page): def permission(username): if username.lower() == \u0026#34;admin\u0026#34;: return f\u0026#34;\u0026#39;{username}\u0026#39; has access to {page}.\u0026#34; else: return f\u0026#34;\u0026#39;{username}\u0026#39; doesn\u0026#39;t have access to {page}.\u0026#34; return permission # 선언 check_admin_page_permision = has_permission(\u0026#34;Admin Page\u0026#34;) \u0026gt;\u0026gt;\u0026gt; check_admin_page_permision(\u0026#34;admin\u0026#34;) \u0026#34;\u0026#39;admin\u0026#39; has access to Admin Page.\u0026#34; \u0026gt;\u0026gt;\u0026gt; check_admin_page_permision(\u0026#34;john\u0026#34;) \u0026#34;\u0026#39;john\u0026#39; doesn\u0026#39;t have access to Admin Page.\u0026#34; 데코레이터 데코레이터는 callable(함수, 메소드, 클래스)를 인자로 받고, 다른 callable을 리턴한다(내부함수).\n생김새와 위치는 자바의 어노테이션과 동일하다. 데코레이션을 사용하면 이미 존재하고 있던 함수에 별도의 수정사항없이 액션을 추가 할 수 있다.\n데코레이터 사용 시나리오\n함수를 인자로 받는 callable을 선언한다. 인자로 받은 함수를 호출한다. 추가 액션이 있는 다른 함수를 리턴한다. 데코레이터 예시\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def example_decorator(func): def _add_messages(): print(\u0026#39;This is my first decorator\u0026#39;) func() print(\u0026#39;bye\u0026#39;) # 데코레이터도 클로저처럼 내부함수를 괄호()로 호출하지 않는다. return _add_messages # greet = example_decorator(greet) 과 동일하다. 클로저 생성 형태와 동일하다. @example_decorator def greet(): print(\u0026#39;Hello World\u0026#39;) \u0026gt;\u0026gt;\u0026gt; greet() This is my first decorator Hello World bye 이렇게 추가로 액션을 행할 수 있게 해주는 데코레이터는 디버깅, 캐싱, 로깅, 시간측정(timing)에 많이 쓰인다.\n데코레이터 디버깅 예시\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def debug(func): def _debug(*args, **kwargs): result = func(*args, **kwargs) print( f\u0026#34;{func.__name__}(args: {args}, kwargs: {kwargs}) -\u0026gt; {result}\u0026#34; ) return result return _debug @debug def add(a, b): return a + b \u0026gt;\u0026gt;\u0026gt; add(5, 6) add(args: (5, 6), kwargs: {}) -\u0026gt; 11 11 데코레이터로 구현한 generate_power\n아까 위에서는 클로저로 generate_power를 구현했는데 이번에는 데코레이터로 구현을 했다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def generate_power(exponent): def power(func): def inner_power(*args): base = func(*args) return base ** exponent return inner_power return power @generate_power(2) def raise_two(n): return n \u0026gt;\u0026gt;\u0026gt; raise_two(7) 49 @generate_power(3) def raise_three(n): return n \u0026gt;\u0026gt;\u0026gt; raise_three(5) 125 데코레이터 호출 과정 상세 보기 데코레이터로 구현한 generate_power_with_debug\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def generate_power_with_debug(exponent): print(f\u0026#39;closure is generated, passed exponent : {exponent}\u0026#39;) def power(func): print(f\u0026#39;inner function in closure. passed func : {func}\u0026#39;) def inner_power(*args): print(f\u0026#39;inner function in power. passed args : {args}\u0026#39;) base = func(*args) return base ** exponent return inner_power return power # closure 생성 # raise_two = generate_power_with_debug(2) 와 동일 하다. @generate_power_with_debug(2) # power()를 리턴 def raise_two(n): # power()를 호출, inner_power()를 리턴 return n print(f\u0026#39;result of closure : {raise_two(7)}\u0026#39;) # 콘솔에 출력된 결과 closure is generated, passed exponent : 2 inner function in closure. passed func : \u0026lt;function raise_two at 0x100e2dee0\u0026gt; inner function in power. passed args : (7,) result of closure : 49 generate_power 호출 과정 (1)\n1 2 3 @generate_power_with_debug(2) def raise_two(n): return n raise_two = generate_power_with_debug(2) 와 동일하다. @generate_power_with_debug 데코레이터는 exponent 값을 포함한 내부함수 power를 리턴한다. raise_two가 선언되면서 power도 호출이 된다. power는 func를 포함한 내부험수 inner_function을 리턴한다. 여기에서도 inner_function은 호출되지 않고, 새로운 인스턴스를 생성해 리턴이 된다. generate_power 호출 과정 (2)\n1 print(f\u0026#39;result of closure : {raise_two(7)}\u0026#39;) raise_two를 호출하면서 클로저를 호출한다. 클로저 호출함에 따라 변수 *args에는 raise_two 함수에 전달된 인자 7이 전달된다. 이또한 스냅샷으로 외부 state를 저장했기 때문이다. 2-1. *args는 함수에 전달되는 모든 인자들을 뜻하고, **kwargs는 위치 지정된 모든 인자들을 뜻한다. inner_power 결과를 리턴한다. 클로저 VS 데코레이터 데코레이터는 클로저를 반환한다. 클로저는 데코레이터에 의해 반환된다. 이름에 _와 __사용 먼저 읽으면 좋을 자료 What’s the Meaning of Single and Double Underscores In Python? _ 와 __ 사용 예시\n1 2 3 4 5 6 7 _foo # single leading underscore foo_ # single trailing underscore _ # single underscore __foo__ # double leading and trailing underscore __foo # double leading underscore 상세 사용 예시 name e.g. usage single leading underscore _foo - private(internally) 하게 사용이 됨을 나타낸다. - 여전히 외부에서 접근이 가능하기 때문에 문맥적 힌트에 가깝다. single trailing underscore foo_ - 파이썬에서 이미 선점한 키워드를 사용할 때 혼선을 피하기 위한 방법이다.\ne.g. type_, from_ single underscore _ - 사용하지 않은 변수들을 담아두는 용도로 쓴다. e.g. _ = return_something(), - 숫자가 길어질 때 혼선을 방지하기 위해 쓴다. e.g. 1000 → 1_000 double leading and trailing underscore __foo__ - dunder method 라고 한다. - 파이썬에서 이미 선점한 특수 목적 전역 클레스 메소드다. double leading underscore __foo - 부모-자식 필드 이름을 구분하기 위해 사용되는 것으로 파악했다. - 실 사용이 거의 없을 거 같다. 파이 포장하기: 모듈, 패키지, 프로그램 모듈 파이썬을 사용하다보면 모듈이라는 단어가 자주 나오는데 여기서 모듈이란 단순히 파이썬 코드가 들어가있는 파일을 뜻한다.\n패키지 파이썬을 좀 더 확장 가능한 어플리케이션으로 만들기 위해서는 모듈을 패키지라는 파일 계층구조로 구성해야 한다. __init__.py 는 파일 내용을 비워놔도 되지만, 파이썬은 이 파일을 포함하는 디렉터리를 패키지로 간주 하기 때문에 패키지로 사용하고 싶다면 꼭 만들어둬야 한다.\n파이썬에서 batteries included 철학은 유용한 작업을 처리하는 많은 표준 라이브러리 모듈들이 내장이 되어있다는 뜻이다.\nDeque = Stack + Queue 파이썬 list는 left end의 pop()과 append()가 빠르지가 않기 때문에 left-end와 right-end 모두 빠르고 메모리를 효과적으로 사용하기 위해 데크를 제공한다.\nlist의 right-end 연산 속도는 O(1)이지만, left-end 연산 속도는 O(n)이다.\nDeque 구현체 Deque 는 Stack과 Queue의 기능을 가졌다. 출입구가 양 끝에 있는 Queue다.(double-ended queue의 구현체이다) Deque는 양 끝으로부터 항목을 추가하거나 삭제할 때 유용하게 쓰인다. popleft()는 left-end를 제거해서 반환하고, pop()은 right-end를 제거해서 반환한다. Deque 예시\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from collections import deque numbers = deque([1,2,3,4]) \u0026gt;\u0026gt;\u0026gt; numbers.popleft() 1 \u0026gt;\u0026gt;\u0026gt; numbers.popleft() 2 \u0026gt;\u0026gt;\u0026gt; numbers deque([3,4]) \u0026gt;\u0026gt;\u0026gt; numbers.appendleft(2) \u0026gt;\u0026gt;\u0026gt; numbers.appendleft(1) \u0026gt;\u0026gt;\u0026gt; numbers deque([1,2,3,4]) Deque의 흥미로운 점 최대 길이 (maximum lenght)를 지정할 수 있다. 한 쪽에서 데이터를 넣어 큐가 꽉 차게 되면 자동으로 다른 쪽에 있는 아이템을 버린다. 이러한 기능으로 인해 이전 0회의 기록을 남기기 와 같은 요구사항이 있을 때 활용하기가 용이하다. [더 많은 Deque 사용법] 에서 더 많은 용도를 확인할 수 있다. 히스토리 남기기 예시\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 from collections import deque sites = ( \u0026#34;google.com\u0026#34;, \u0026#34;yahoo.com\u0026#34;, \u0026#34;bing.com\u0026#34; ) pages = deque(maxlen=3) pages.maxlen for site in sites: pages.appendleft(site) \u0026gt;\u0026gt;\u0026gt; pages deque([\u0026#39;bing.com\u0026#39;, \u0026#39;yahoo.com\u0026#39;, \u0026#39;google.com\u0026#39;], maxlen=3) pages.appendleft(\u0026#34;facebook.com\u0026#34;) \u0026gt;\u0026gt;\u0026gt; pages deque([\u0026#39;facebook.com\u0026#39;, \u0026#39;bing.com\u0026#39;, \u0026#39;yahoo.com\u0026#39;], maxlen=3) pages.appendleft(\u0026#34;twitter.com\u0026#34;) \u0026gt;\u0026gt;\u0026gt; pages deque([\u0026#39;twitter.com\u0026#39;, \u0026#39;facebook.com\u0026#39;, \u0026#39;bing.com\u0026#39;], maxlen=3) Linux의 tail 모방 예시\n1 2 3 4 5 6 7 8 from collections import deque def tail(filename, lines=10): try: with open(filename) as file: return deque(file, lines) except OSError as error: print(f\u0026#39;Opening file \u0026#34;{filename}\u0026#34; failed with error: {error}\u0026#39;) thread-safe CPython에서 deque의 append(), appendleft(), pop(), popleft(), len()은 thread-safe 하게 만들어졌기 때문에 멀티쓰레드 환경에서 deque를 사용하기 좋다. CPyton은 C로 구현한 파이썬으로, 가장 많이 사용되고 있는 파이썬 구현체다. 오픈소스로 관리가 되고 있다. [깃허브] ","date":"2022-03-08","permalink":"https://leeleelee3264.github.io/post/2022-03-08-introducing-python-part-one/","tags":["Book"],"title":"[Introducing Python] part one (1/2)"},{"content":"\n터미널에서 여러 개의 깃허브 계정을 사용하는 방법을 다룹니다.\nIndex\n깃허브 계정 여러 개 세팅하기 매번 해줘야 하는 작업들 레퍼런스 깃허브 게정 여러 개 세팅하기 디렉터리 세팅하기 복수의 깃허브 계정을 사용 할 때, 각 계정들의 root source directory를 나누어두면 관리적 측면에서도, git config 설정을 할 때에도 더 편리하다.\n디렉터리 예시\n. └── home ├── office └── personal ssh 키 발급 Step 1 사용할 계정들의 키 발급 1 2 ssh-keygen -t rsa -b 4096 -C \u0026#34;leelee@office.com\u0026#34; ssh-keygen -t rsa -b 4096 -C \u0026#34;leelee@personal.com\u0026#34; Step 2 로컬의 ssh-agent 에 발급받은 키 연결: 1 2 ssh-add -K ~/.ssh/id_rsa_personal ssh-add -K ~/.ssh/id_rsa_office 위에서 연결한 ssh 키 정보들은 ssh-add -l 로 확인이 가능하다.\nStep 3 ssh config 작성 발급받은 키들과 깃허브 계정 정보를 로컬 단에서 연결을 하기 위해 .ssh/config 에 관련 정보들을 작성을 한다.\n.ssh/config\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Personal GitHub account Host github.com-personal HostName github.com User git AddKeysToAgent yes UseKeychain yes IdentityFile ~/.ssh/id_rsa # Office Github account Host github.com-office HostName github.com User git AddKeysToAgent yes UseKeychain yes IdentityFile ~/.ssh/id_rsa_office ssh 키 깃허브에 등록 ssh-keygen 결과물\nid_rsa.pub 외부로 공개되는 공개키이다. github에 등록되는 키다. 파일 끝을 보면 이메일을 확인할 수 있다. id_rsa 외부로 공개되면 안되는 비밀키이다. id_rsa.pub 에시\n1 2 3 4 5 6 7 8 9 10 11 ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQCruMY405yFL/6fvDvVFUTlxgXVO XRhdXlWDGsX5Kcn7yvEiGwBhVngvL8WWfA+hlelodoIAvlgnN9sJmVDDHF8XkK6r/ INdvFBAQ28+2GlOM8l038HDiCOTg/GzhEQK0hVzE0Cgsfrw2YMSxDJ9Gr9eSDSN0ia0LM LHMXdn5I5aeePGlt0boMIJgohzLVb4HT5KipBxbHETe05a8oOvmc9nS8r47ibSWpecuqDMJ7 7YrBa82X0d+5nAdZ1QiJg63k7ifJdPC4/CJHVvmglsHBzTkWEdn89R6q4OwyrUBRnrcITrF8 aCQMax2A5f7SaLcJ9xXQx47LT0ApfJ5UhHmLdK2vKzWEeEXhMfT3d0wIlppsEs5FuZRLqSAyZ QCn1IxZvV+KBAe5O3B9sidhULMnTzGqLCe3lLv3K0uI2MrP694LHqjW0duppbRbZZSNGdc0AM PtOqprI+lvBAugi6mN20sWRBMsHz1m1HdUI4yM85VAhYLNLgqs5n13ZfwUYEEh9EyqtGESToy 8DCSRqPqHNINB0skGBh9DF3ChjhdKvyn40AmpHdAzFlWgKGXbvx1DKzVhkubGNkISicwT7U+9 /18UuHJL2OsoFd9YcQ0qJqcrXWY2RxVkqAxzndxaPNeT5uXhZt0yNukm3UXd9khEd/Qn8F1n IqgHGiVCntP9wmQ== leelee@personal.com github setting 페이지로 이동 [Picture 1] Github Setting 메뉴바 SSH and GPG Keys 항목에서 New SSH key 등록 office를 위한 id_rsa_personal.pub 와 id_rsa_office.pud 를 각각 등록해준다.\n[Picture 2] New SSH key 등록 계정 정보 명시 Github에서 사용하는 name과 email을 별도로 설정해주지 않으면 개인 리포지토리에 올릴 커밋의 작성자가 회사 계정으로 되어있거나, 권한이 없다며 push를 할 수 없다. 떄문에 각각 계정별로 정보를 명시해줘야 한다.\n계정 정보 명시 시나리오 default: personal\nhome과 office 디렉터리에 각각 .gitconfig 파일을 하나씩 만들어준다. home의 .gitconfig에 personal 계정의 name과 email을 입력해준다. office 의 .gitconfig를 불러오는 설정을 추가해준다. office의 .gitconfig에 회사 계정의 name과 email을 입력해준다. home .gitconfig\n1 2 3 4 5 [user] name = leelee-personal email = leelee@personal.com [includeIf \u0026#34;gitdir:~/office/\u0026#34;] path = ~/office/.gitconfig office .gitconfig\n1 2 3 [user] name = leelee-office email = leelee@office.com 완성된 디렉터리 에시\n. └── home ├── .gitconfig ├── office │ └── .gitconfig └── personal 매번 해줘야 하는 작업들 레포지토리 주소 수정 .ssh/config 에서 연결할 Host를 계정별로 분기해서 각각 github.com-personal 과 github.com-office 로 구분을 했다. 때문에 매번 레포지토리를 만들거나, 클론할 때 구분하는 작업을 해줘야한다. git remote set-url 로 레포지토리를 연결할 떄도 똑같은 형식으로 해줘야 한다.\n기존 url\n1 git clone git@github.com:(Repo path).git 수정 url\n1 git clone git@github.com-office:(Repo path).git set-url 예시\n1 git remote set-url origin git@github.com-office:(Repo path).git Reference [CodeWords: A Mobile Application Blog by Heady]\n","date":"2022-01-12","permalink":"https://leeleelee3264.github.io/post/2022-01-12-git-multi-account/","tags":["General"],"title":"터미널에서 여러 개의 깃허브 계정 사용하기"},{"content":"\n2021 백엔드 엔지니어 인터뷰의 면접질문에 대해서 다룹니다.\nIndex\nPython DevOps 개발 전반 상식 Python 파이썬 쓰레드 파이썬 쓰레드에 대해 아는 점은 파이썬 자체에서 쓰레드를 지원하는 것은 아니고, 운영체제에서 제공하는 쓰레드를 사용한다는 것과, 쓰레드가 있다고 해도 한 타임에 한 쓰레드만 돌아간 다는 사실이었다. 이러한 특성의 원인은 파이썬의 디폴트 구현채인 CPython에 있었다.\nCpython 쓰레드 Cpython은 OS 쓰레드를 사용한다. 파이썬 쓰레드란, OS 쓰레드를 파이썬이 런타임에 관리를 하는 것 뿐이다. Cpython 인터프리터는 전역 인터프리터 록 (global interpreter lock) 메커니즘을 사용하여 한 번에 오직 하나의 스레드가 파이썬 바이트 코드를 실행하도록 보장한다.\n이런 인터프리터 전체를 잠구는 특성은 인터프리터를 다중스레드화 하기 쉽게 만든다, 하지만 한 번에 단 하나의 쓰레드를 실행시켜 쓰레드의 특징인 병렬성 잃어버리게 한다. (이는 multiprocessing 으로 보완)\n파이썬의 동시성과 병렬성 파이썬 쓰레드에 대해 너무 간략하게 알고 있었기 때문에 추가로 더 찾아보게 되었는데, 그때 프로그래밍에서 흔히 사용되는 개념인 동시성과 병렬성에 대해서, 또 파이썬은 이 둘을 어떻게 지원하는지 학습했다.\n원래는 동시성과 병렬성이 같은 뜻인줄 알았다. 알고보니 동시성은 짧은 시간에 한 가지 일을 처리 하고 금방 바꿔서 또 다른 일을 처리하는 걸 말한다. 결국 한 순간에는 한가지 일만을 처리하고 있다. 그런데 병렬성은 한 순간에 여러가지 일을 처리하고 있다.\n[Picture 1] 병렬성과 동시성 그런데 병렬성이 빛을 보기 위해서는 정말로 CPU 여러 개가 있어야 한다. 여러 가지 CPU들에 일을 하나씩 할당해 마치도록 하기 떄문이다. 여러가지 일을 한 번에 처리하는 병렬성이 좋아보이지만, 어떤 일을 처리하냐에 따라서 동시성과 병렬성을 선택해야 한다. 대기하는 일이 대부분인 입출력 I/O 작업이라면 병렬을 해서 CPU를 놀게 하는 것보다는 동시성을 사용하는 게 더 효과적이다. 이제 실제 파이썬 구현 부분을 봐보도록 하자.\n동시성 장점 편리하고, 잘 이해되는 방법으로 다른 리소스들을 기다리는 테스크를 실행한다. 여기서의 리소스들은 네트워크, 입출력, 하드웨어 디바이스의 신호 등이 될 수 있다.\n단점 General한 쓰레드의 단점들처럼, 객체 엑세스를 잘 관리해줘야 해서 CPU에 민감한 작업은 할 수 없다. 또한 실행되고 있던 A 쓰레드가 B 쓰레드로 스위치 되면서 A 쓰레드는 멈춰버리기 때문에 쓰레드를 사용하는 퍼포먼스적 이점이 없다.\n동시성의 대표 예시: 코루틴 Python 코루틴 and async 예시\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import aiohttp import asyncio urls = [ \u0026#34;https://imdb.com\u0026#34;, \u0026#34;https://python.org\u0026#34;, \u0026#34;https://docs.python.org\u0026#34;, \u0026#34;https://wikipedia.org\u0026#34;, ] async def get_from(session, url): async with session.get(url) as r: return await r.text() async def main(): async with aiohttp.ClientSession() as session: datas = await asyncio.gather(*[get_from(session, u) for u in urls]) print ([_[:200] for _ in datas]) if __name__ == \u0026#34;__main__\u0026#34;: loop = asyncio.get_event_loop() loop.run_until_complete(main()) 코루틴 실행 시나리오\nget_from 함수가 코루틴이다. asyncio.gather 는 여러개의 코루틴 (다른 url들을 가진 다수의 get_from 함수 인스턴스) 를 생성해낸다. asyncio.gather 는 모든 코루틴이 실행 완료되기를 기다렸다가, 결과를 취합해서 리턴한다. 코루틴 장점 어떤 게 코루틴인지 문맥적으로 확실하게 구분을 할 수 있다. 쓰레드를 사용하면 어떤 함수도 쓰레드로 돌아갈 수 있어 혼동이 오는 데 코루틴은 그 점을 방지한다. 꼭 쓰레드를 필요로 하지 않는다. 코루틴은 파이썬 런타임이 직접 관리를 할 수 있다. 때문에 스위칭이 될 때에도 쓰레드보다 오버헤드도 작고, 메모리도 더 적게 필요로 한다. 코루틴 단점 async await 문법을 잘 지켜야 한다. 또한 쓰레드가 그러하듯, CPU에 민감한 작업은 할 수 없다.\n병렬성 말 그대로 프로세스를 여러 개 만드는 것이다. 각각의 CPU에서 돌아가기 때문에 멀티코어여야 한다. 프로세스를 여러개 만든 다는 말은 파이썬 인터프리터 인스턴스를 여러개 만든 다는 뜻이다!\n장점 쓰레드와 코루틴과는 다르게 객체의 다중 엑세스에 대한 관리가 조금 더 쉽다. 그리고 쓰레드와 코루틴은 다중 엑세스 때문에 모든 오퍼레이션이 순차적으로 돌아가게 하지만 이는 그럴 필요가 없다.\n단점 프로세스를 여러개 만드는 것 자체에 오버해드가 든다. 그리고 서브 프로세스들은 메인 프로세스에서 온 데이터 복사본이 있어야 하는데 이렇게 프로세스 사이에서 데이터가 왔다갔다 하기 위해서는 직렬화를 해야 한다. 이때 pickle 파이썬 라이브러리를 쓰는데, 보통의 객체들은 지원하지만 특이한 객체들은 지원을 하지 않는다.\n[Python concurrency and parallelism explained] 을 보면서 파이썬이 지원하는 동시성, 병렬성 구현을 살펴봤다.\n파이썬을 백엔드로 쓰면서 한꺼번에 몰린 요청을 처리하는 방법 이건 면접에서 나온 질문은 아니었고, 내가 궁금해서 물어본 부분이었다.\n파이썬은 인터프리터 언어다보니 백엔드로 사용을 하다보면 한계가 온다고 한다. 그래서 결국 컴파일 언어인 자바로 다시 만드는 경우가 왕왕있다고 하는데 한꺼번에 요청이 몰리면 어떻게 파이썬으로 처리를 하냐고 물어봤다.\n처리 방법 백엔드 코드를 쿠버네티스에 올려서 운용을 한다고 말씀해주셨는데, 그래서 요청이 몰릴때에도 이 쿠버네티스에 올린 파드를 증설한다고 한다. 즉, 컨테이너를 몇 개 더 만들어 요청을 분산처리 한다. 역시 코드 자체로 성능을 개선하기 보다는 돈을 써서 서버를 증설하는 방법이 회사에서 제일 많이 사용되는 방법이 아닐까.\nDjango 모듈 구조와 흐름 장고의 흐름은 [Picture 2] 같다. 굉장히 간단한 질문이었는데 장고를 거의 사용하고 있지 않아서 아주 뜨문뜨문 대답을 했다.. 한창 스프링을 쓰고 있었기 때문에 스프링에 빗대어서 대답을 했다.\n[Picture 2] Django 모듈 구조 View Controller Model 데이터가 들어있는 엔티티 장고의 모델의 필드들은 db 테이블의 컬럼들과 매핑이 된다. db와 바로 연결이 되어있기 때문에 모델을 변경하면 자동으로 db에 migrate 될 수 있다. Template 화면이다. DRF라면 거의 쓸 일이 없다. Django wsgi 항상 프로젝트 안의 wsgi가 뭔가 했는데 웹 서버 및 어플리케이션을 위한 파이썬 표준이라고 한다.\n[Picture 3] wsgi 흐름 클라이언트가 정적인 웹페이지를 요청했을 경우 웹서버에서 쳐 낼 수 있지만, 동적인 페이지를 요청했을 때는 웹서버에서 처리를 할 수 없고, 장고 서버에 리퀘스트를 넘겨줘야한다. 그런데 웹서버는 파이썬을 모른다. 그렇기 때문에 가운데에 wsgi가 인터페이스 역할을 해서 웹서버와 장고를 연결해준다.\n그럼 당연히 장고에서 제일 먼저 호출되는 부분은 wsgi가 될 수 밖에 없다. 얘가 리퀘스트를 물어다주니까. url 디스패처와 미들웨어 등등도 일단은 리퀘스트를 받아와야 해서 그 이후에 호출이 된다.\nDjango ORM objects.all objects.all 은 전체 자료를 불러온다. 모든 데이터가 필요하지 않은 이상 제일 효율이 좋지 않은 쿼리라 할 수 있다.\nselected_related VS prefetch_related Django ORM에서 쿼리 성능 향상으로 많이 쓰이는 기능은 select_related 와 prefetch_related인데 둘 다 즉시 로딩 (Eager-loading)과 한 번 DB에서 로딩을 한 이후부터는 캐싱이 되어서 쓰이는 등의 특징은 동일하지만 데이터를 가져오는 방식과 사용하는 상황이 조금 다르다.\n대게는 selected_related 사용을 추천하고 있다.\nselected_related one-to-one 같은 single-valued relationship 에서만 사용이 가능하다. DB에서 join을 해서 데이터를 가져온다. prefetch_related one-to-one 같은 single-valued relationship 에서 사용이 가능하다. many-to-many, many-to-one에서도 사용이 가능하다. DB에서 데이터를 가져올 때 2개의 단계를 거친다. 관계별로 개별 쿼리를 실행 (연결 테이블이 3개라면 3개의 쿼리가 따로 실행) 각각 가져온 데이터들을 파이썬에서 합쳐주기 참고하면 좋을 자료들 [select_related와 prefetch_related]\nselect_related와 prefetch_related에 대한 차이를 잘 설명해주고 있다. select_related와 prefetch_related를 함께 사용하는 방법도 있다. [당신이 몰랐던 Django Prefetch]\nDjango Prefetch 효율적 사용으로 성능개선하기 DevOps Nginx load balancing 스케쥴링 메서드 Round Robin 라운드 로빈은 운영체제 수업에서 배웠던 라운드 로빈과 동일한 알고리즘으로, 아무런 설정을 하지 않았다면 기본적으로 라운드 로빈 방식으로 스케쥴링이 된다. Least Connections 가장 적은 수의 active connection을 가지고 있는 서버에게 요청을 할당한다. IP Hash 똑같은 아이피에서 온 요청들을 똑같은 서버에서 처리할 수 있도록 보장한다. 동일 아이피의 기준은 IPv4일때는 앞의 3 옥텟이 동일해야 하고, IPv6에서는 모든 자리가 동일해야 한다. Generic Hash 유저가 정의한 키(hashed key value) 에 맞춰서 요청을 할당한다고 한다. 쿠버네티스 파드 쿠버네티스에서 생성하고 관리할 수 있는 배포 가능한 가장 작은 컴퓨팅 단위. 하나 이상의 컨테이너 그룹이다. 이 그룹은 스토리지 및 네트워크를 공유한다. 도커 개념 측면에서 파드는 공유 네임스페이스와 공유 파일시스템 볼륨이 있는 도커 컨테이너 그룹과 비슷하다.\nDocker Compose 도커를 실행하기 위해서는 도커 커맨드를 사용해야 하는데 간단한 커맨드는 상관이 없지만 볼륨을 연결하는 등의 추가 설정을 하다 보면 커맨드가 굉장히 길어진다. 이렇게 길어진 도커 커맨드를 쉘 스크립트로 짜도 되지만 이런 불편을 도커 컴포즈를 이용해 해결 할 수 있다.\n도커 컴포즈는 일종의 툴인데 도커 컨테이너 실행 환경을 YAML 파일로 관리할 수 있다.\n도커 컴포즈 예시\n1 2 3 4 5 6 7 8 9 10 11 version: \u0026#34;3.3\u0026#34; services: # db config web: build: . ports: - \u0026#34;4012:4012\u0026#34; environment: TZ: \u0026#34;Asia/Seoul\u0026#34; container_name: gov-prod-web_1 개발 전반 상식 DDD 디자인 패턴중 하나이다. Domain Driven Design의 약자이다. 보통 클린 아키텍처랑 함께 사용된다. 도메인 코드가 어플리케이션과 인프라 코드와 분리 되는 것에 집중을 한다는 점이 같기 때문이다. 도메인이란 프로젝트 안에서 개발되어야 하는 주제를 뜻하며, 비즈니스 로직이라 할 수 있다. 모든 소프트웨어 디자인 패턴의 목적처럼 시스템을 만들고 유지보수하는데 투입되는 인력 최소화에 있다. DDD 를 보다보면 유비쿼터스라는 단어가 많이 나온다. 개발자, 디자이너, 기획자가 모두 동일한 의미로 이해하는 단어라는 뜻이다. DDD 패턴은 비즈니스 로직인 도메인을 인프라와 어플리케이션(서비스) 와 분리하기 때문에 비즈니스 자체에 집중을 할 수 있다. 그래서 개발자가 다른 부서와 협업을 할 때 의사소통이 더 원활하게 이뤄질 수 있다. (db, network 등등의 로우 레벨 을 얘기하지 않기 때문)\n[Picture 4] Layer of the Clean Architecture 도메인 개념에 집중해서 아키텍처를 만들면 [Picture 4] 와 같다. 도메인 속에 엔타티가 들어있는데 이게 제일 중요한 개념이라고 생각한다. 엔티티에 최대한 많은 비즈니스 룰을 담아서 응집성을 높이고, 중복코드를 줄이는게 ddd의 목표이다. 단! 엔티티 밖의 메소드 (예를 들어 어플리케이션 레이어) 가 엔티티의 값을 변경하는 일은 절대 없어야 한다. 이는 객체지향에서 추구하는 캡슐화와 유사하다고 볼 수 있다.\n엔티티의 일은 엔티티 안에서 끝이 나야 한다! validation 조차도 UseCase에서 호출하면 안되고, 비즈니스를 제일 잘 아는 Entity에서 검증해야 한다.\n[Picture 5] Entity Anti Pattern MVC 패턴에서도 DDD에서도 서비스의 역할을 정하는 게 가장 어렵다. 조금만 잘못해도 코드가 서비스에 치중을 해 결국 서비스에서 엔티티의 값도 바꾸고, DB에 저장 요청도 하는 거대한 서비스 중심이 될 수 있기 때문이다. DDD 에서 서비스 (또는 use case) 의 역할은 여러가지 엔티티가 함께 다뤄져야 할 때이다. (여러가지 엔티티가 함께 다뤄지지 않아도 Controller 영역에서 바로 엔티티를 호출해서는 안된다)\nDAO와 DTO, ENTITY DAO DAO는 Data Access Object 로, 데이터에 엑세스하기 위한 객체다. 정말 데이터를 얻어오기 위해 접근을 하는 객체라 다른 말을 할 것이 없다. DAO 볼때마다 드는 생각이 그럼 Repository 는 뭐지? 둘의 차이는 [Picture 6] 와 같다.\nDAO VS Repository 레포지토리가 더 상위계층이고 DAO가 하위계층이다.\n레포지토리는 도메인과 데이터 매핑 레이어 가운데에 있는 존재이다. DAO 는 말 그대로 못생긴 쿼리를 한 번 숨기는 역할을 하고, 레포지토리는 여러가지 DAO를 활용해서 데이터를 상위 계층으로 전달을 할 수 있다.\n[Picture 6] DAO and Entity DTO DTO는 Data Transfer Object로, 데이터를 여러 계층 사이로 전송하기 위한 객체다. 스프링관점으로 생각을 해보자면 DAO가 DB에서 가져온 정보를 서비스 계층으로 넘길 때 DTO 에 넣어서 전송을 하고, 서비스 계층이 컨트롤러 계층으로 넘길 때 DTO를 사용한다 .\nEntity Entity 는 DDD의 관점에서 보면 비즈니스 로직이고, JPA로 보면 DB테이블 그 자체이다. 이렇게 중요한 부분 그 자체이기 때문에 변경도 엔티티 안에서 일어나야 하고, 다른 계층으로 넘길때도 그냥 넘겨서는 절대 안된다. 꼭 DTO에 담아서 전달을 해 줘야 한다.\nRequest Method GET 데이터를 조회하기 위한 요청이다. 몇번을 반복적으로 요청해도 받아보는 데이터에 변화가 없다는 게 중요한 점이다. 이런 GET의 성격 때문에 무엇인가를 바꾸려는 작업을 GET으로 만들지 말아야 한다. POST POST는 무엇인가를 바꾸기 (update/insert/delete) 위한 요청이다. 같은 요청을 여러번 날리다보면 사이드 이펙트가 발생할 수 있다. GET VS POST POST가 GET보다 보안상 안전하다. GET은 URL안에 모든 정보를 다 포함하고 있기 때문에 웹서버 등지에서 엑세스 로그를 남길 경우 GET에 있던 정보들이 그대로 남게 된다. 하지만 POST 또한 데이터를 URL에 넣지 않고 body에 넣었단 차이만 있을 뿐이고, 이 body도 까서 볼 수 있기 때문에 마냥 안전하다고 볼 수 없다. 민감한 데이터는 어지간하면 네트워크를 타고 움직이지 않는 것이 제일 좋다. 멱등성 이렇게 GET처럼 똑같은 요청(연산)을 여러번 하더라도 결과가 달라지지 않는 성질을 멱등성(Idempotent)라고 한다. Rest api에서 제일 많이 사용되는 메서드들 중에서 POST를 제외한 GET, PUT, DELETE은 멱등성을 지켜야 함을 명심하면서 서버 구현을 하도록 해야 한다.\n[Picture 7] HTTP Method POST VS PUT 처음에 HTTP 메서드들을 배웠을 때 POST와 PUT 구분이 어려웠다. 결국 POST나 PUT 둘 다 CREATE이 가능하기 때문에. 그런데 찾아보니 아래와 같은 사항으로 구분을 할 수 있었고, 다시 고려해보면 PUT은 CREATE 보다는 UPDATE에 더 적합해 보인다.\n시나리오 1: 멱등성을 유지해야 하는가?\nPOST = (a++) ⇒ 계속 1이 증가하여 결과 값이 매번 달라진다. PUT = (a=4) ⇒ 계속 a는 4로 업데이트 되기 때문에 결과값이 매번 같다. 시나리오 2: 리소스 결정권이 있는가?\n리소스 결정권은 클라이언트가 이 리소스의 위치 (리소스 id)를 정확히 아는지 모르는지의 차이다.\nPOST /articles PUT /articles/11120 PUT 클라이언트가 리소스가 어디에 저장이되는지 정확히 알 수 있다. 내가 많이 쓰던 POST의 path variable 이 들어간 URL이 사실은 PUT에 맞는 컨벤션이었다. POST POST는 해당 URL을 요청하기만 하면 원하는 리소스를 만들어주겠다며 factory만 제공해주고 리소스의 정확한 위치는 제공하지 않고 있다. PUT VS PATCH 둘 다 리소스를 업데이트 한다는 느낌이 강하지만 작은 차이가 있다.\nPUT은 리소스 전체 또는 다수를 업데이트 하는 느낌이 강하다. PATCH는 리소스의 일부 또는 단 하나를 업데이트 하는 느낌이 강하다. Cross Origin Resource Sharing CORS의 정확한 개념 줄여서 CORS라고 한다. 위키사전에서 찾아봤을 때 교차 출처 리소스 공유라고 직역을 하고 있다.\n원래 내가 알고 있던 부분은 같은 출처가 아닌, 다른 출처에서 리소스 요청을 했을 때 보안상 요청을 처리하지 못하고 error를 보낸다 였다. 그래서 이 문제를 해결하기 위한 정책이 CORS이고. 그런데 내가 심각하게 오해하고 있던 부분은, 여기서의 Origin이 서버를 의미하는 게 아니라 요청을 보낸 클라이언트를 의미하는 것 이었다.\n브라우저에 치중되어 있는 CORS RFC에 존재하는 리소스 요청 정책은 2가지 이다. 하나는 SOP (Same-Origin Policy) 이고 다른 하나는 CORS이다. 하지만 이 정책들은 브라우저에민 구현되어있는 스펙이다.\n서버와 서버가 리소스를 주고 받을 때는 CORS 문제가 발생하지 않는다. 서버에서 응답을 주더라도 브라우저 단에서 응답을 막고 error를 내려준다. 때문에 서버 로그에서는 정상적으로 응답이 내려갔다고 보이며 디버깅이 상당히 어려워진다. 때문에 서버에서도 CORS의 존재에 대해 염두해 두고 있어야 한다!\n[Picture 8] CORS [Picture 8] 의 설명이 제일 명확했다. 브라우저는 스크립트가 요청을 보낼 때 지금 스크립트가 서비스 되고 있는 url과 요청을 보내는 url을 비교한다. 여기서 동일한 url로 보여지는 조건은 프로토콜/스키마 (https, http) 와 호스트 (www.google.com) 포트번호가 모두 같을 때 동일한 url로 인정한다.\nCORS 발생 시나리오 Preflight Simple Request Credentialed Request 어떻게, 어떤 조건에서 헤더를 채워서 보내는지에 따라 브라우저가 임의로 하나의 시나리오를 보내 요청을 보낸다. 어떤 조건속에서 어떤 CORS 시나리오가 만들어지는지 잘 유념하자.\nCORS 해결 방법 클라이언트에서 발생하는 문제이지만 이걸 해결하기 위해서는 서버가 작업을 해야 한다. 해당 리소스에 대해 어떤 origin들이 요청을 할 수 있는지 응답 헤더중 하나인 Access-Control-Allow-Origin에 기입을 해주는 것이다. 여기에 와일드 카드를 넣어버릴 수 있지만 이럼 언제나 보안 문제가 일어날 수 있음을 명심하자. Access-Control-Allow-Origin에 들어있는 Origin들과 요청을 보냈던 클라이언트의 Origin을 보고 브라우저가 유효한 응답임을 판단한다.\nOptions Method Options 메소드가 언제 쓰이는지 궁금했었는데 이렇게 Access-Control-Allow-Origin 을 알기 위해 선행 요청을 보낼 때도 사용이 된다고 한다.\n[Picture 9] Options Method 참고하면 좋을 자료들 [Cross Origin Resource Sharing - CORS] [CORS는 왜 이렇게 우리를 힘들게 하는걸까?] Authentication 과 Authorization의 차이 Oauth2에 대해 대답을 하다가 나온 문제였다. 둘이 어떤 개념인지는 알지만 용어적으로 낯설어 제대로 대답을 하지 못 해 이번에 정리를 하면서 찾아봤다. Authentication과 Authorization을 쉽게 비교하기 위해 표를 작성했다.\n/ Authentication Authorization 역할 사용자 신원 확인 리소스, 기능 에 대한 엑세스 권한 확인 목적 사용자가 누구인지 확인/판별 사용자가 해당 리소스/기능에 대해 사용 권한이 있는지 확인/판별 방법 로그인, 생체인식 session key, JWT, Oauth 절차는 Authentication ➡️ Authorization 이 이루어진다고 보면 된다.\nAuthentication이 되었다고 해도 보내는 리퀘스트마다 이 사용자가 정말로 자격이 있는지 매번 확인을 하는 절차가 Authentication이라고 생각한다.\n스키마를 짤 때 하는 고민 이건 정말 광범위한 범위가 아닐까? 아직도 어떤 방식으로 스키마를 짜는 게 최적인지 잘 모르겠다. 정말 서비스마다 다른 것 같다. 내가 스키마를 짜면서 확실하게 느꼈던 부분 정도만 정리를 했다.\n높은 수준의 정규화를 했을 때는 조회를 할 때 JOIN을 많이 해줘야 하지만 데이터 수정과 삭제가 용이하다. 낮은 수준의 정규화를 했을 때는 조회할 때 JOIN을 많이 안 해줘도 된다. 그러나 중복된 데이터가 많아진다. 옛날에는 하드가 비싸서 정규화에 신경을 많이 썼는데 요즘은 하드가 비싸지 않아 정규화를 많이 할 필요가 없다고 한다. 히스토리 처럼 아래로 쌓이는 데이터의 경우 높은 수준의 정규화를 하지 않아도 된다. UPDATE가 자주 일어나는 데이터인지 INSERT가 많이 일어나는 데이터인지에 따라서 스키마가 달라질 필요가 있다. 고수준 인터페이스와 저수준 인터페이스 고수준 인터페이스와 저수준 인터페이스라는 단어보다는 고수준 모듈, 저수준 모듈 이라는 말을 더 많이 사용한다.\n고수준 모듈 추상화가 되어있는 기능을 제공한다. 1 2 3 public interface Animal { void eat(); } 저수준 모듈 고수준에서 제공하는 기능을 실제로 구현한다. 1 2 3 4 5 6 public People implements Animal { @Override void eat() { Systems.out.println(\u0026#34;People eat many things\u0026#34;); } 여태 고수준과 저수준을 반대로 생각하고 있었다. 고수준이 실제로 구현이 된 부분이고 저수준이 추상화의 부분인줄 알았으나 실제는 반대였다.\n","date":"2022-01-01","permalink":"https://leeleelee3264.github.io/post/2022-01-01-interview-python/","tags":["General"],"title":"2021 백엔드 엔지니어 인터뷰 - Python, General"},{"content":"\n2021 백엔드 엔지니어 인터뷰의 면접질문에 대해서 다룹니다.\nIndex\nIntro 기술면접 회고 Java Intro 10월 달 부터 이직을 준비하기 시작하며 Resume 와 Cover Letter 를 썼고, 11월 달에는 면접을 보러 다녔다. 대부분의 면접들이 몇 단계로 이루어져있었는데 기술 면접에서 면접관들이 물어봤던 질문들을 기록해두고 공유하면 좋을 것 같아 포스팅을 하기로 했다.\n지원분야가 Backend이기 때문에 대부분의 질문들이 Backend 와 관련이 되어있지만, 직전에 근무하고 있던 회사에서 DevOps의 경험도 있다고 이력서에 적어서 DevOps와 관련된 질문들도 약간 있었다. 깃허브에 [DevOps 커리큘럼]이 있는데 면접 때 보고 가면 도움이 될 거 같다.\n기술면접 회고 커리어를 시작하고 이렇다 할 면접들을 보러 다닌적이 없었는데 기술면접을 보고 나니 왜 기술 공부를 더 열심히 해야 하는지 깨달았다. 이론적인 측면들은 지루해서 공부를 피하기 마련이었는데 기술면접에서 다 물어보는 것들이었고, 결국은 이 이론적인 측면들을 잘 알아야지만 더 좋은 코드를 만들 수 있다.\n깊이는 없는 새로운 기술에 대한 욕심 파이썬을 그냥 써보기만 했고 깊이가 없었다. 다른 개념들도 마찬가지였다. 막 커리어를 시작했을 때는 이것저것 조금씩 공부하는 게 좋았는데 이제는 깊이가 있는 공부를 해야하는 때가 아닌가 싶다.\n퇴사를 준비하면서 이것저것 많이 여쭤봤던 팀장님께 인사를 드렸는데 기술에 대한 욕심을 조금 버리는 게 좋다고 조언해주셨다. 그도 그럴게 2년 동안 정말 신기술에 집착을 많이 했던 것 같다\u0026hellip; 그 분이 항상 해주시던 말씀이 프레임워크를 공부하기보다는 언어를 더 공부하라 였는데 결국 기본기가 제일 중요한 게 아닐까? 디자인 패턴과 정규식은 어디에서도 쓰이는 것처럼.\n빨리 사둔 디자인 패턴 책도 읽고 이팩티브 자바도 다시 읽어봐야겠다. 자바 빨리빨리 공부하고 파이썬으로 진짜 넘어가야지!\n면접 질문 추이 이력서에 주로 사용하던 언어가 자바라고 썼기 때문에 자바 질문이 들어왔고 면접을 본 회사들은 대부분 파이썬을 사용하고 있었기 때문에 파이썬 질문도 많았다. 질문의 구성은 크게 아래와 같았다.\n자바 질문 파이썬 질문 개발 전반 상식 이번 포스팅에서는 자바 질문과 답변을 다룬다.\nJava 실행중인 Spring Boot에서 변경된 properties 로드하기 맨 처음에는 Spring boot에서 properties 를 적용하는 방법에 대한 질문인 줄 알고 Spring boot externalized properties 우선순위에 대해 답변했는데 아니었다. 이미 러닝중인 서버에 수정된 propreties를 재시작없이 어떻게 반영하냐에 대한 질문이었다.\nSpring Boot Actuator Spring Boot Actuator 를 이용하면 된다. 이렇게 config 를 러닝 타임에 업데이트 하는 상황은 서버가 하나 떠있을 때 보다는 서버를 여러 개 띄워두는 Spring Cloud 환경에서 많이 사용하는 것으로 보인다.\n[Picture 1] Spring Cloud Config Spring Cloud Config는 Spring Cloud Config 서버와 클라이언트 어플리케이션 ([Picture 1] 에서 Microservice #1 #2 #3 로 표기된 서버들) 로 구성이 되어있는데 config 서버 설정에 변경이 생겼을 때 클라이언트 어플리케이션도 변경을 반영해줘야 한다. 이때 다시 시작하지 않고 actuator 를 이용해서 refresh 하면 된다.\n여기서 actuator 는 실행중인 스프링 어플리케이션 내부 정보를 REST 엔드포인트와 JMX MBeans(Java Management Extension. 모니터링용 객체) 로 노출시키는 스프링 부트의 확장모듈이다. 실행중인 스프링 어플리케이션을 뜯어 볼 수 있다는 게 중요하다!\nSpring Boot Actuator 사용법 클라이언트 어플리케이션에 actuator 라이브러리를 implement 한다. 클라이언트 어플리케이션 application.properties또는 application.yml 에 actuator 사용을 위한 설정을 추가 한다. Config 서버에서 가져온 설정을 사용하는 코드 부분에 @RefreshScope 추가 한다. http://클라이언트서버/actuator/refresh POST 호출로 변경사항 적용한다. 참고하면 좋을 자료들 [Spring cloud config 리프래시 하기 (Use RefreshScope)] [Spring Cloud Config 2] JDBC Java Database Connectivity의 약자이다.\n데이터베이스 연결을 관리하는 자바 API로, 쿼리와 커맨드를 발행하고 데이터베이스에서 건내주는 결과 셋을 처리한다. JDBC는 자바 어플리케이션이 데이터베이스 또는 RDBMS와 소통하기 위한 프로그래밍 레벨 인터페이스를 제공한다.\n[Picture 2] JDBC 상세 JDBC는 결과적으로 자바 코드로 데이터베이스를 관리할 수 있게 만들어준다.\nJDBC API는 자바 어플리케이션과 JDBC Manager 사이의 커뮤니케이션을 지원한다. JDBC Driver는 데이터베이스와 JDBC Manager 사이의 커뮤니케이션을 지원한다. 직렬화 직렬화는 객체를 바이트 스트림으로 바꾸는 것이다. 이와 반대로 바이트 스트림을 객체로 바꾸는 것은 역직렬화라고 한다.\n객체는 플랫폼에서 독립적이지 못하다. 그래서 [Picture 3] 처럼 파일, 메모리, 데이터베이스 처럼 다른 시스템으로 보내려고 할 때 플랫폼에서 독립적인 바이트 스트림으로 변환을 한다.\n[Picture 3] 직렬화란? 자바 직렬화 자바 직렬화도 마찬가지로 JVM 메모리에 올라가있는 객체를 byte 스트림 (byte 형태의 데이터) 바꾸는 것이다.\n[Picture 4] 자바 직렬화 상세 그런데 요즘의 API들을 생각해보면 데이터를 다 JSON으로 직렬화 해서 내보내고 있다. JSON 처럼 문자열로 변환하는 형태가 아니라 이진 표현으로 변환해서 내보낼때는 Protocol Buffer를 사용한다고 한다. 그럼 손쉬운 JSON과 Protocol Buffer가 아니라 번거로운 자바 직렬화를 사용할 때의 장점은 무엇일까?\n자바 직렬화 장점 자바 직렬화의 장점은 자바에 최적화 되었다는 점이다. 자바 시스템에서 또 다른 자바 시스템으로 데이터를 보낼 때 손쉽게 직렬화-역직렬화를 할 수 있다. 서블릿 세션이 대표적인 사용처라고 하는데 홈페이지를 만들때도 유저의 로그인 세션을 직렬화해서 관리했던 게 생각이 난다.\n자바 직렬화 문제점 외부에 나가서 장기 보관될 정보는 자바 직렬화를 사용하지 않는다. 추후에 변경이 있으면 오류가 나기 때문이다. 자바 직렬화에 사용하는 시리얼 ID도 개발시에 따로 관리를 해줘야 한다. 직렬화-역직렬화를 할 때 타입과 필드의 변경에 엄격해 오류가 잘 발생할 수 있다. 때문에 자주 변경되는 클래스는 자바 직렬화를 사용하지 않는 것이 좋다. 자바 직렬화된 데이터는 JSON 보다 훨씬 크기가 크다. 떄문에 직렬화된 데이터를 캐시등의 이유로 존재하는 Redis 와 같은 메모리 서버에 저장을 하면 트래픽에 따라 비용이 급증할 수 있다. 위와 같은 이유들로 최대한 JSON 포맷으로 변경을 고려해야 한다. 참고하면 좋을 자료들 [자바 직렬화, 그것이 알고싶다. 훑어보기편] 저번에 FCM으로 보낼 푸시 메세지들을 직렬화해서 Redis 메모리에 넣어두어 어플리케이션 서버에서 큐 구조로 순차적으로 꺼내갈 수 있게 해뒀는데 이번에 직렬화를 찾아보니 JSON으로 바꾸는 방향으로 해야겠다..\nInterface와 Abstract class의 차이 추상 클래스와 인터페이스 모두 선언만 가능하고, new 를 사용해서 인스턴스를 만드는 게 불가능하다는 공통점이 있지만, 차이점이 더 많다.\nInterface 인터페이스는 A is able to B 를 만족시킨다. 인터페이스는 자바8, 9에 들어오면서 원래의 인터페이스에서 많은 변화가 생겼다.\nInterface의 변화 추상메서드만 만들 수 있었으나 JAVA 8에 들어오면서 default를 사용해 메서드 구현이 가능해졌다. 필드를 가질 수 없었으나 JAVA 8에 들어오면서 final 과 static 필드를 가질 수 있게 되었다. 접근 지정자가 default로 private 이었으나 JAVA 9에 들어오면서 private 사용도 가능해졌다. Abstract class 추상클래스는 A is B를 만족시킨다. 추상메서드를 만들 수 있고, 구현이 된 메서드를 만들 수 도 있다. 일반 클래스와 마찬가지로 필드도 가질 수 있다.\nAbstract class 사용시 주의 점 실무에서 추상클래스를 사용했는데 상속관계를 제대로 고려하지 않았었기 때문에 모두 리팩토링을 해야 했다. 추상클래스를 사용을 할 때에는 super 클래스와 하위클래스의 관계를 잘 생각해서 구현을 해야 하고, 추상클래스 보다는 인터페이스 사용을 권장한다.\nDI, IOC Dependency Injection과 Inversion Of Control. Spring에서 처음 접한 개념인데, Spring 뿐만 아니라 다른 언어와 프레임워크에서도 널리 사용되는 개념이다.\nIoC는 설계 원칙이고 DI 는 IoC 원칙을 지키기 위한 디자인 패턴이다. 실제로 DI 말고도 IoC를 위한 다양한 패턴이 존재한다.\n[Picture 4] IoC Pattern 구현 방법론 IoC IoC는 객체 사이의 결합도를 줄이기 위해 제어를 역전시킨다. 가장 흔한 제어의 역전의 예시로는 객체 생성이 있다. 제어 역전이 일어나면 객체를 직접 생성하지 않고, 이미 생성되어 있는 코드를 사용하기만 한다.\nSpring으로 예를 들어보면 Bean으로 선언된 객체들은 Spring에서 관리를 한다. Bean 객체들은 IoC Container 안에 생성이 되고, 관리가 된다. 우리는 그 객체들이 어떻게 관리가 되고 있는지 알 필요 없이 해당 객체를 사용해야 할 때 의존성을 주입받아 (DI) 사용을 하면 된다.\nDI DI는 의존성 있는 객체의 생성을 class 외부에서 수행한 후, 다양한 방법으로 해당 객체를 class 에게 제공한다.\nDI 예시\n1 2 3 4 5 6 7 8 9 10 @Controller public class TestController { private final TestService testService; public TestController(TestService testService) { this.testService = testService; } } DI을 이용해 객체 결합도를 느슨하게 하기 위해서는 Client, Service, Injector 클래스가 필요하다. Injector 클래스가 Service 객체를 만들어 Client 클래스에 제공하는 형태인데 코딩을 하면서 무수히 많은 객체들의 의존성을 매번 이렇게 만들어 줄 수는 없다.\n때문에 IoC Container와 같은 기능을 제공하는 프레임워크를 사용해 위와 같은 일을 위임한다. 프레임워크를 사용하면 객체의 의존도를 고려하면서 객체의 생성. 소멸을 신경쓰지 않아도 되고 비즈니스 코드에 더 집중을 할 수 있고 결과적으로 변경에 유연한 코드를 만들 수 있다.\n참고하면 좋을 자료들 [Dependency Injection, IoC, DIP, IoC container정리] Static static 키워드를 사용한 메서드와 변수는 해당 클래스의 객체를 생성하지 않아도 해당 메서드와 변수를 사용할 수 있다. 매번 객체를 생성하지 않아도 되기 때문에 손쉽게 사용을 할 수 있어 내 경우는 Utill 성 메서드들을 static으로 만들었다.\nstatic 의 특징 static은 메모리에 딱 한 번 할당이 된다. 일반적인 객체들이 Heap 영역에 할당이 되는 것과는 다르게 stack 영역에 만들어진다. 때문에 Heap 처럼 Garbage Collection을 걱정하지 않아도 된다. stack 은 모든 객체가 공유하는 메모리 공간이기 때문에 객체 생성이 없이도 static 메서드와 변수를 사용할 수 있다. Java8에서의 static Java 8이전에 static 변수와 메서드는 JVM 메모리에서 PermGen에 저장이 되었으나 Java 8에서는 PermGen 가 사라지고 MetaSpace가 그 역할을 대신한다. 변경된 Java 8 JVM 구조는 [Picture 5] 와 같다.\n[Picture 5] IoC Pattern 구현 방법론 static 핵심 구현 사항 인스턴스 변수 (non-static) 를 사용하는 메서드는 인스턴스 메서드를 사용하고 클래스 변수 (static) 을 사용하는 메서드는 static 메서드를 사용한다.\n클래스를 설계할 때 인스턴스에 공통적으로 사용해야 하는 맴버변수에 static을 사용한다. static 메서드에서는 static이 아닌 맴버변수는 사용할 수 없다. static 이 아닌 메서드에서는 static인 맴버변수를 사용할 수 있다. 메서드 안에서 인스턴스 변수를 사용하지 않는다면 static을 사용을 고려한다. 1과 같은 특성으로 한 인스턴스에서 static 변수의 값을 바꿨을 때 모든 인스턴스에 변경이 적용된다. 이와 같은 모두 변경 불상사를 피하기 위해서 static 변수를 사용할 때 final 키위드를 함께 사용해 변경을 불가하게 만든다.\n2, 3과 같은 일이 일어나는 이유는 인스턴스 변수가 static 변수 또는 메서드를 사용하는 시점에서는 static 변수와 메서드가 이미 생성이 되어있지만 반대의 경우에는 사용 시점에 인스턴스가 만들어졌는지 알 수 없기 때문이다.\n","date":"2021-12-02","permalink":"https://leeleelee3264.github.io/post/2021-12-02-interview-java/","tags":["General"],"title":"2021 백엔드 엔지니어 인터뷰 - Java"},{"content":"\nShell Script 로 Mysql db 전체백업하기에 대해 다룬다.\nIndex\n백업 진행 순서 백업에 사용된 스크립트 \u0026amp; 세부 사항 개선해야 할 점과 참고자료 백업 진행 순서 Intro: 백업의 종류 SQL 백업에는 전체백업, 증분백업, 차등백업이 있다. 백업 종류에 대한 더 자세한 정보는 이전의 포스팅에서 확인이 가능하다. [SQL 백업 종류]\n백업 환경 상용에서 사용하는 디비의 데이터를 일주일에 한 번 씩 로컬 서버로 백업을 해두기로 했다. 빈번하게 진행되는 백업이 아니라서 시간이 오래 걸리지만 간단한 전체백업 을 하기로 했다. 대신 서비스에 장애가 가지 않도록 사용량이 적은 월요일 오전 3시에 백업이 되도록 crontab으로 스케쥴링을 걸어두었다.\n우분투 환경의 쉘 스크립트를 작성해 Mysql db 백업을 진행했다.\n백업 과정 현재 상태 상용에서 사용하는 디비 이름은 mydb 이다. 과거의 상용 데이터를 가진 로컬 디비 이름은 prev_mydb이다. 백업을 진행하는 일시는 2021년 09월 06일이다. 로컬 디비에는 저번주인 08월 30일의 snapshot 인 mydb와 2주전인 08월 23일의 snapshot인 prev_mydb가 있다. 백업 목표 로컬디비의 mydb를 prev_mydb 라는 이름으로 변경한다. 상용디비의 mydb를 통째로 로컬디비로 옮겨온다. (전체백업) 로컬 디비에 남는 디비는 prev_mydb와 mydb가 된다. 상세 작업 순서 mysql은 데이터베이스 자체를 RENAME 할 수 없어서 prev_mydb를 지우고 다시 만들어야 했다. 백업과정을 [Picture 1] 에서 모식도로 나타냈다.\n[Picture 1] DB 백업 상세 작업 상용 서버에서의 action 로컬 서버가 상용 db로 접속 mysqldump를 실행해서 상용 디비의 mydb의 전체 데이터를 로컬 서버로 복사 (prod mydb data 생성) 로컬 서버에서의 action 서버에는 지금 막 덤프한 prod mydb data와 mydb, prev_mydb 3개가 존재한다. prev_mydb 를 삭제하고, mydb를 복사한다. (local mydb data 생성) local mydb data 로 prev_mydb 를 생성한다. mydb를 삭제한다. prod mydb data 로 mydb 를 생성한다. 작업이 완료되면 로컬 디비에는 새로운 mydb와 prev_mydb 가 남는다. prod mydb data 는 압축을 해 gz 파일 형식으로 로컬 서버에 남겨둔다. 백업에 사용된 스크립트 \u0026amp; 세부사항 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 #!/bin/bash # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ # Title: bk_prod.sh # Author: Seungmin Lee # Date: 2021-08-16 # Schedule: Every Sunday AM 03:00 # # This script is for backup rds (product) database. Backup db will be used in statistic part cd ~/bk_prod DATE=`date +\u0026#39;%Y%m%d\u0026#39;` # dest db info HOST=rds.examplehost.com READ_ID=rds_id # src db info LOCAL_ID=local_id # pre setting file. # default pre setting file is ~/.my.cnf LOCAL_CNF_PATH=~/bk_prod/.my.local.cnf LOCAL_BASIC=\u0026#34;--defaults-file=${LOCAL_CNF_PATH} --user=${LOCAL_ID}\u0026#34; DB=mydb PRE_DB=prev_mydb # view will be ignored from db dump VIEW_ONE=${DB}.v_channel_lang VIEW_TWO=${DB}.v_voice_usage # --------------------------------------------------------------------------------------------------------------------------------------------- # # --------------------------------------------------------------------------------------------------------------------------------------------- # # dump rds db first mysqldump --single-transaction --host=${HOST} --user=${READ_ID} --ignore-table=${VIEW_ONE} --ignore-table=${VIEW_TWO} ${DB} \u0026gt; ${DB}${DATE}.sql # local db backup for db rename mysqldump ${LOCAL_BASIC} ${DB} \u0026gt; local_${DB}${DATE}.sql # create and dump previous db in local db # ---------------------------------------------------------------------------------------------------------------------------------------------- # # ---------------------------------------------------------------------------------------------------------------------------------------------- # # check prev_mydb is in local db RESULT=`mysql ${LOCAL_BASIC} --skip-column-names -e \u0026#34;show databases like \u0026#39;${PRE_DB}\u0026#39;\u0026#34;` if [ ${RESULT} == ${PRE_DB} ] then echo \u0026#34;DELETE FORMER ${PRE_DB}\u0026#34; mysql ${LOCAL_BASIC} -e \u0026#34;DROP DATABASE ${PRE_DB}\u0026#34; else : fi mysql ${LOCAL_BASIC} -e \u0026#34;CREATE DATABASE ${PRE_DB}\u0026#34; mysql ${LOCAL_BASIC} pre_${DB} \u0026lt; local_${DB}${DATE}.sql # now delete xouchcare db in local db mysql ${LOCAL_BASIC} -e \u0026#34;DROP DATABASE ${DB}\u0026#34; # make new xouchcare db in local db mysql ${LOCAL_BASIC} -e \u0026#34;CREATE DATABASE ${DB}\u0026#34; mysql ${LOCAL_BASIC} ${DB} \u0026lt; ${DB}${DATE}.sql echo \u0026#34;DB back up fine\u0026#34; # done gzip *.sql mysql command options —defaults-file (line 25) 쉘스크립트에서 DB 접속 문제\nmysqldump와 같은 mysql에서 사용하는 커맨드는 실행을 위해서는 db 접속 호스트 정보와 id, pw를 함께 입력해야 한다. 터미널에 커맨드를 실행할 때는 상관이 없지만 쉘스크립트로 실행을 할 때는 따로 터미널로 계정 정보를 기입해 줄 수 없다. 이를 해결하기 위해서는 계정 정보를 넣어둔 my.cnf 를 미리 만들어주면 된다.\n실행하려는 쉘스크립트에서 접속을 해야 하는 디비가 상용과 local 두가지라서 각각의 계정 정보를 넣어둔 my.cnf 와 my.local.cnf 파일을 만들었다. my.local.cnf 에서 보는 것처럼 실행을 해야하는 커맨드별로 계정정보를 넣어두면 쉘에서 해당 mysql 커맨드를 실행할 때 id, pw 를 입력하지 않아도 된다.\ncnf 파일 경로 설정\n기본 my.cnf 위치는 ~/my.cnf 로 홈디렉토리에 생성이 되거나, 홈디렉토리에 직접 만들어주면 된다. 쉘에서도 따로 경로를 표기하지 않아도 알아서 my.cnf의 위치로 가서 파일을 읽어온다. my.local.cnf 처럼 추가로 만들때면 경로를 따로 표기해줘야 한다. 실제 커맨드 라인에서 사용을 할 때는 —defaults-file=path_of_cnf_file 을 써주면 된다.\n상용 디비 계정 정보를 넣어둔 my.cnf 파일\n1 2 3 [mysqldump] user=rds_id password=rds_pw Local 디비 계정 정보를 넣어둔 my.local.cnf 파일\n1 2 3 4 5 6 7 [mysqldump] user=local_id password=local_pw [mysql] user=local_id password=local_pw —ignore-table (in line 37) mysqldump 를 실행할 때 덤프를 하지 말아야 하는 테이블을 명시해주면 덤프에서 제외가 된다. 제외를 할 테이블들이 있으면 —ignore-table=table_name 을 하나씩 적어주면 된다. 아쉽게도 하나의 옵션에 여러개의 테이블을 쓸 수는 없었다.\n내 경우는 로컬 디비에 접속을 하려는 계정이 view에 대한 권한이 없었기 때문에 아예 상용 디비를 덤프할 때 제외를 시켜놨다.\n—single-transaction (in line 37) innodb는 트랜잭션을 지원하기 때문에 해당 옵션을 사용할 수 있다. 덤프를 할 때 하나의 새로운 트랜잭션을 열어서 진행을 한다. 그럼 덤프를 실행하는 딱 그 순간의 디비가 스냅샷처럼 덤프가 떠진다. 덤프를 하고 있는 중에 db에 실행된 delete, insert, update과 같은 DML 쿼리의 결과는 반영하지 않는다.\n아예 트랜잭션을 걸기 때문에 덤프를 하면서 작업중인 테이블에 락을 하나하나 걸지 않아도 된다. innodb가 아니면 lock-tables 옵션을 걸어줘야 한다.\nsingle-transaction의 한계\n단, 데이터를 바꾸는 DML은 트랜잭션이 막을 수 있지만 스키마를 바꾸는 DDL은 트랜잭션이 막을 수 없다고 한다. 그래서 성공적으로 덤프를 하기 위해서는 덤프를 하는 동안 DDL 쿼리를 해서는 안된다.\n—skip-column-names (in line 47) 말 그대로 결과에서 컬럼의 이름을 빼고 보여주는 옵션이다. 쉘스크립트에서 mysql의 결과 값을 인자로 받아 실행을 할 때 컬럼 이름이 들어가 있으면 결과를 식별하기가 어려워 —skip-column-names 를 사용한다.\n사용 예시\n1 2 3 4 5 6 # with column +----------------------+ | Database (mydb) | +----------------------+ | mydb | +----------------------+ 1 2 3 4 # without column +----------------------+ | mydb | +----------------------+ -e (in line 47) —execute 과 동일한 기능을 하는 옵션. 해당 옵션을 사용하면 mysql은 따옴표로 감싸준 쿼리(SQL statement)를 실행하고 값을 뱉어낸 다음에 종료한다.\nmysql에 접속해서 콘솔을 사용하면 어떤 쿼리도 실행을 시킬 수 있는 것처럼 -e 를 사용하면 mysql 커맨드 라인에서 원하는 쿼리를 실행할 수 있다.\n-e 사용 예시\n1 2 3 4 5 # 예시 1 RESULT=`mysql ${LOCAL_BASIC} --skip-column-names -e \u0026#34;show databases like \u0026#39;${PRE_DB}\u0026#39;\u0026#34; # 예시 2 mysql -uroot -p1234 -e \u0026#34;select * from test.user from age=20\u0026#34; 개선할 점과 참고자료 개선할 점 전체백업의 장점은 구현이 쉽다. 그런데 디비 속의 모든 데이터를 백업하기 때문에 시간이 오래 걸린다.\n그리고 만약 전체 백업이 제대로 진행이 안된다면 많은 양의 백업 데이터를 유실할 위험도 크다. 그래서 서비스에서는 전체백업 + 차등백업 / 전체백업 + 차등백업 + 증분백업 으로 여러 주기로 백업을 진행한다.\n라이브러리를 사용해서 DB 백업 하기 쉘스크립트로 짤 수 있는 전체백업과는 다르게 차등백업과 증분백업은 구현된 라이브러리를 써서 많이 진행한다.\n검색을 했을 때 많이 보였던 게 XtraBackup 인데 지원 문제가 있어서 요즘은 Mariabackup 을 사용한다. 나중에 좀 더 세밀한 백업을 위해 구조를 바꾸면 좋을 것 같아서 XtraBackup과 Mariabackup에 관련된 문서를 남겨둔다.\n[우아한-장애와 관련된 XtraBackup 적용기] [Mariabackup을 이용한 증분 백업] [XtraBackup에서 Mariabackup로 변경해야 하는 이유] 참고자료 [Mysqldump \u0026ndash;single-transaction option] [4.2.2.1 Using Options on the Command Line] [Storage Engine (InnoDB vs MyISAM)] ","date":"2021-09-09","permalink":"https://leeleelee3264.github.io/post/2021-09-09-linux-shell-db-backup/","tags":["Infra"],"title":"Shell Script 로 MySQL db 전체백업하기"},{"content":"\nSQL 백업 종류에 대해 다룬다.\nIndex\n언제 DB 백업을 해야 하나? 백업 개념 MySQL의 백업 커맨드 Linux DB Migration 언제 DB 백업을 해야 하나? 백업의 용도 내가 실무에서 접할 수 있는 디비 백업의 용도는 2가지가 정도이다.\n(보관) 데이터 유실을 막기 위해 서비스에서 사용하는 디비 외의 다른 디비에 데이터를 저장하기 위해 (이전) 원래있던 디비를 버리고 새로운 디비를 사용하기 위해 2의 경위에는 디비가 바뀌니 이에 따라 코드나 명세등 여러가지를 바꿔줘야 하고, 데이터 양이 많으면 이전하는 작업 자체가 오래 걸리기 때문에 자주 일어나지 않는게 제일 좋지만, 서비스 초기와 서비스를 제공하면서 규모와 구조는 언제든지 바뀔 수 있기 때문에 항상 마음의 준비를 해두는 게 좋다.\n실무에서 백업 작업을 한 경험 서버와 DB가 하나의 호스트에 있을 때 문제점 앱/웹서버들과 디비서버를 하나에 몰아두는 것은 좋은 구조가 아니다. 한 곳에 몰아두면 관리는 쉬울 수 있겠지만 하나가 잘못되면 다른 부분까지 영향을 미쳐 결국 서비스를 마비시킬 수 있는 위험부담이 크다. 앱서버에 요청이 몰리거나, 대규모 디비 연산이 수행되면 양쪽이 정상작동 하지 않는 문제가 발행한다.\n대표적인 예가 지금 회사에서 사용하고 있는 개발 서버이다. 상용서버인 API는 서버와 아마존 RDS를 사용하는 디비과 완전히 분리가 되어있기 때문에 구조적 문제가 없다. 그러나 개발서버인 TPI는 디비가 TPI 서버 안에 들어가 있다.\n[Picture 1] 회사 서버 구조 문제 발생 개발서버다 보니 서버안에 여러가지 앱 서버들을 띄워두었는데 초기에 개발서버가 커질 것을 예상하지 못하고 AWS에서 CPU와 메모리 성능이 작은 인스턴스를 선택해서 서버로 띄웠다. 게다가 위에서 말한 것 처럼 개발에서 사용하는 디비또한 해당 서버 안에 띄워두었다. 앱서버의 갯수가 늘수록 서버의 자원은 부족해졌다.\n결국 어느 한 앱 서버에서 지나치게 CPU를 독점해서 연산을 진행하면서 다른 앱서버들과 디비가 멈췄고 심지어 서버 자체에 ssh로 접속하는 것도 불가능했다. 자원이 적은 서버에 이것저것 올려두는 것도 무리였지만 제일 큰 문제는 서버가 터지자 디비까지 작동을 멈췄던 점이다. 도미노처럼 다 멈춘 서버는 결국 재부팅을 해서 살렸다.\n문제 해결 이런 도미노 문제가 발생하다보니 하나에 뭉쳐있는 서버를 작게 쪼개는 수 밖에 없다. 서버를 3대를 띄워서 2대를 앱서버 컨테이너 서버로 사용하고 나머지 하나를 디비 컨테이너 서버로 사용한다. 상용 서버 처럼 RDS 디비를 사용하면 좋겠지만 RDS는 비싸기 때문에 선택하지 못했고 원래 목적인 서버와 디비의 분리는 이루어졌다!\n이렇게 나눠두면 앞의 두 컨테이너 서버에 요청이 심각한 수준까지 몰려서 서버가 다운이 된다고 해도 디비 컨테이너 서버에는 아무런 영향이 없고, 디비를 띄워둔 서버에는 다른 것들을 띄워두지 않고 정말 Mysql 만 띄워두기 때문에 서버에 과부하가 걸릴 확률이 크게 낮아진다.\n더 어려운 백업은? 디비 이전을 위한 백업은 사용하고 있던 디비 속의 데이터를 몽땅 꺼내와서 새로 만든 디비에 넘겨주기만 하면 끝이라서 비교적 쉬운 백업이다. 문제는 데이터 유실을 막기위해, 정말 백업의 용도로 이루어진 디비 백업이다. 디비를 통째로 복사해서 가지고 있을 것인지, 일정한 간격으로 백업을 진행해서 마지막 백업 이후에 이어서 백업을 할 것인지 등등 여러 컨셉의 백업이 있다.\n(plus) 문제 해결 중 느낀 점 이번 문제를 직면하면서 요즘 왜 MSA가 많이 선택을 받는지 이유를 조금이나마 알 것 같았다. MSA를 다룬 글에서는 에자일식으로 빠르게 개발이 가능한게 큰 장점이었는데 이렇게 잘게 나눠두면 서버의 오류가 멀리 퍼지는 것도 최소화 할 수 있는 것도 장점이라고 생각된다. 한군데에 몰려둔 TPI 형태의 아키텍처가 모놀리틱이고 모놀리틱보다는 분리가 되어있는 게 SOA (Service-Orient Architecture) 였다.\nSOA와 MSA의 차이는 여기서 더 자세히 볼 수 있다. [SOA vs MSA]\n[Picture 2] 여러가지 아키텍처 백업 개념 아래의 컨셉은 데이터베이스 뿐만 아니라 컴퓨터 전반에서 이루어지는 백업에서 통용되는 컨셉이다.\n전체 백업 (Full Backup) 차등 백업 (Differential Backup) 증분 백업 (Incremental Backup) 전체 백업 디비 이전을 위해 진행하는 백업은 전체 백업이다. 아무 조건 없이 모든 데이터를 덤프 뜨기 때문에 조건이 없어 백업을 진행하기 쉽지만 시간이 제일 오래 걸리고 파일 크기도 제일 크다.\n차등 백업 [Picture 3] 차등 백업 차등 백업은 전체 백업보다 조금 더 정교한 형태의 백업이다. 전체백업은 큰 규묘의 데이터를 백업하다보니 일주일에 한 번 정도 진행이 된다. 만약 저번주의 전체 백업은 잘 마쳤지만 이번주의 전체 백업이 정상적으로 진행이 안되었다면 많은 데이터를 유실하게 된다.\n이런 전체백업의 문제점을 커버하기 위한 차등 백업은 전체 백업 이후 추가된 데이터들만 백업을 하는 방식으로, 그림처럼 일요일에 전체 백업을 한다고 하면\n월: 월요일에 추가된 데이터\n화: 월요일 + 화요일에 추가된 데이터\n수: 월요일 + 화요일 + 수요일\n\u0026hellip;\n이런 식으로 다음 전체 백업이 있는 일요일 전까지 백업이 진행된다. (마지막 차등백업은 무시된다고 한다). 전체 백업보다는 정교하고, 데이터도 비교적 적기 떄문에 크기도 작고 작업 시간도 단축되었으나 다음 전체 백업 날이 다가올수록 파일 크기가 커진다. 그리고 변경된 사항들을 누적해가면서 저장하기 떄문에 하루에 한 번 이상 진행하기에는 무리가 있다.\n증분 백업 [Picture 4] 증분 백업 증분 백업은 차등백업보다 더 작은 규모의 백업을 진행한다. 차등백업이 전체백업 이후의 변경을 하루하루 누적한다면 증분백업은 딱 그날 바뀐 부분만 백업을 한다. 예를 들어 일요일날 전체백업을 진행했다면\n월: 월요일에 추가된 데이터\n화: 화요일에 추가된 데이터\n수: 수요일에 추가된 데이터\n\u0026hellip;\n이런 식으로 그날의 데이터만 저장을 한다. 때문에 크기가 가장 작고, 백업을 진행하는 속도도 가장 빠르다. 워낙에 작고 빠른 백업이 이루어지기 때문에 다른 백업들과는 다르게 한 시간에 한 번 씩 백업을 해도 무리가 없다!\n이렇게 보면 증분 백업이 제일 좋아보이지만 백업본이 날아가서 복구를 해야 하거나 할 때 시간이 오래 걸린다. 정교한 백업을 진행한 만큼 데이터를 다시 똑같이 쪼개서 백업을 해야 하기 때문에. 반면 전체백업은 똑같이 통으로 가져오기만 하면 되기 때문에 가장 빠르게 복구할 수 있다.\n표로 정리하는 백업의 장단점 [Picture 5] 백업의 장단점 MySQL의 백업 커맨드 백업 커맨드는 DB서버에 직접 들어가서 실행해야 한다. (DML/DDL/DCL 처럼 DB 콘솔창에서 실행 불가)\n백업하기\n1 2 3 4 5 6 7 8 9 \u0026lt;-- 아래에 나오는 # 는 리눅스 프롬프트 입니다. ex) root# --\u0026gt; \u0026lt;-- DB 백업 --\u0026gt; # mysqldump -u계졍 -p패스워드 복사할_DB_이름 \u0026gt; file_name.sql # mysqldump -uroot -p1234 mine \u0026gt; /etc/var/log/temp/my_backup.sql \u0026lt;-- Table 백업 --\u0026gt; # mysqldump -u계정 -p패스워드 DB_이름 Table_이름 \u0026gt; file_name.sql # mysqldump -uroot -p1234 mine mine_table \u0026gt; /etc/var/log/temp/my_table_backup.sql 복원하기\n1 2 3 4 5 6 7 \u0026lt;-- DB 복원 --\u0026gt; # mysql -u계정 -p패스워드 복원할_DB_이름 \u0026lt; file_name.sql # mysql -uroot -p1234 mine \u0026lt; /etc/var/log/temp/my_backup.sql \u0026lt;-- Table 복원 --\u0026gt; # mysql -u계정 -p패스워드 DB_이름 Table_이름 \u0026lt; file_name.sql # mysql -uroot -p1234 mine mine_table \u0026lt; /etc/var/log/temp/my_table_backup.sql Linux Mysql DB Migration 추후에 있을 DB 이전 작업을 위해 리눅스 환경에서 한 서버(old server)에서 다른 서버(new server)로 디비를 옮기는 방법을 추가로 알아봤다.\n선행 작업\nDB 백업 (위 항목의 DB 백업 Command 사용) 백업한 sql 파일을 FTP 등을 이용해 new server로 전달 new server에서 진행하는 작업\n1 2 3 4 5 6 \u0026lt;-- in mysql server --\u0026gt; mysql \u0026gt; create database db_name mysql \u0026gt; quit \u0026lt;-- in mysql server host --\u0026gt; # mysql -u계정 -p패스워드 db_name \u0026lt; file_name.sql ","date":"2021-07-21","permalink":"https://leeleelee3264.github.io/post/2021-07-21-mysql-backup/","tags":["Infra"],"title":"SQL 백업 종류"},{"content":"\nMySQL의 Join \u0026amp; Join Set에 대해서 다룬다.\nIndex\nHistory of Join Join Set Reference 리뷰 후에 알게된 부분들 비등가 조인. Oracle에서는 지원을 하는데 Mysql이 지원을 하는지 모르겠다. 일반 조인이랑 비등가 조인 쿼리를 직접 짜서 결과를 보고 비교해야 할 것 같다 [비등가 조인] 나중에 쿼리 추가하기. 테이블 조인 방식이 원래 for loop을 돌리는 건 알고있었지만 더 다양한 알고리즘이 있다. [테이블 조인 알고리즘] History of Join Join 테이블과 테이블을 묶어서 보여주는 Join 커맨드는 관계를 핵심으로 하는 RDBMS에서는 빼놓을 수 없는 기능이다. 한 테이블에 모든 정보를 넣어놨다면 Join을 쓸 필요가 없겠지만 중복을 피하기 위해 정규화를 거치고 데이터 유지보수를 위해 테이블을 쪼개다보면 Join을 꼭 써야 한다. 서브 쿼리를 이용해서 조인을 흉내낼 수 있다고 하는데, 서브쿼리 특성상 쿼리가 더 어렵고 지저분해지지 않을까?\nJoin? Inner Join?\nJoin과 Inner Join은 동일한 기능인데, 쿼리를 읽을 때 더 명확한 느낌을 주기 위해 Inner Join이라고 쓴다고 한다. 동일하게 Left Join과 Left Outer Join, Right Join과 Right Outer Join은 동일한 기능이다.\n지금은 이렇게 필수가 되어버린 Join이 사실은 sql이 만들어졌을 때 부터 있던 기능은 아니라고 한다.\nHistory of Join [Picture 1] history of join [Picture 1] 에서 보는 것 처럼 Join은 1992년에 만들어진 SQL-92 standard 에서 만들어졌다. 92는 Join뿐만 아니라 DDL인 ALTER과 DROP도 포함이 되어있고, 학교에서 제일 많이 가르치고 실무에서도 제일 많이 사용되고 있다.\nJoin이 없었을 때의 Join 방법 이처럼 Join이 없을때도 Join을 흉내내는 방법은 있었는데 From에 여러 테이블을 써주면 된다.\n예를 들어 A와 B 테이블을 합치려고 할 때 From 에 A와 B를 써주면 두 테이블을 합칠 때 나올 수 있는 모든 경우의 수들이 다 나오고, ON에서 A.id = B.a_id 하는 것처럼 WHERE 조건에서 A.id = B.a_id를 하면 Inner Join을 한 것과 동일한 결과가 나온다.\nfrom과 join 예시\n1 2 3 4 5 6 7 8 9 10 11 12 13 # Multiple Table In From SELECT * FROM orders O, users U WHERE user_id = U.id # Join SELECT * FROM orders O INNER JOIN users U ON O.user_id = U.id # 테이블 별칭을 사용했을 때 맨날 O.user_id, U.id 이런 식으로 모든 Column에 테이블 명시를 해줬는데 # 알고보니 한 쪽 테이블에만 존재하는 Column이면 따로 명시를 안 해줘도 된다! # 그런데 하나하나 명시를 해주는 쪽이 쿼리를 한 눈에 파악하기 더 용이해보인다. Where 조건을 걸지 않아서 orders와 users를 합쳤을 때 나오는 모든 경우의 수가 다 나왔다. 이걸 Cross Join 또는 Cartesian Join이라고 한다. Join연산은 항상 카티전 Join 과 동일하거나 작은 수의 결과를 리턴한다.\n[Picture 2] Cross Join Cross Join 결과\n더보기 id user_id cost id name number addr active 4 100 33 100 Peter 01012345678 대전광역시 Y 4 100 33 200 Lee 01087654321 경기도 N 4 100 33 300 Jamie 403926999 Calgary Alberta Canda N 5 100 11 100 Peter 01012345678 대전광역시 Y 5 100 11 200 Lee 01087654321 경기도 N 5 100 11 300 Jamie 403926999 Calgary Alberta Canda N 6 200 2 100 Peter 01012345678 대전광역시 Y 6 200 2 200 Lee 01087654321 경기도 N 6 200 2 300 Jamie 403926999 Calgary Alberta Canda N 7 200 100 100 Peter 01012345678 대전광역시 Y 7 200 100 200 Lee 01087654321 경기도 N 7 200 100 300 Jamie 403926999 Calgary Alberta Canda N 8 100 20 100 Peter 01012345678 대전광역시 Y 8 100 20 200 Lee 01087654321 경기도 N 8 100 20 300 Jamie 403926999 Calgary Alberta Canda N 9 400 200 100 Peter 01012345678 대전광역시 Y 9 400 200 200 Lee 01087654321 경기도 N 9 400 200 300 Jamie 403926999 Calgary Alberta Canda N Where 조건을 건 Join과 동일한 결과\nid user_id cost id name number addr active 4 100 33 100 Peter 01012345678 대전광역시 Y 5 100 11 100 Peter 01012345678 대전광역시 Y 6 200 2 200 Lee 01087654321 경기도 N 7 200 100 200 Lee 01087654321 경기도 N 8 100 20 100 Peter 01012345678 대전광역시 Y 그럼 Join을 안 써도 되나? 사실 Join과 Multiple Table In From의 퍼포먼스는 동일하다. 하지만 요즘은 모두가 Join을 쓰는 추세이고, Join을 사용해서 기본적으로 피해지는 오류들이 있으니 가능하면 Join을 사용하도록 하자.\nJoin의 장점 1 Join은 테이블을 합치기 위해서 사용하는 조건과 값을 뽑아내기 위한 조건을 분리할 수 있다.\nJoin의 테이블 결합 조건은 ON에 들어가기 때문에 WHERE에는 정말 값을 뽑아내기 위한 조건만 써주면 되기 때문에 복잡한 WHERE 조건이 들어간다고 해도 헷갈리지 않는다.\n여러 값을 뽑는 예시\n1 2 3 4 5 6 7 8 9 10 11 12 # 2021-07-13의 데이터를 뽑아낸다고 가정 #Join SELECT * FROM orders O INNER JOIN users U ON O.user_id = U.id WHERE O.sold_date = \u0026#39;2021-07-13\u0026#39; # Multiple Table In From SELECT * FROM orders O, users U WHERE O.user_id = U.id AND O.sold_date = \u0026#39;2021-07-13\u0026#39; Join의 장점 2 여러개의 테이블을 합칠 때 Join이 훨씬 용이하고, Multiple Table In From은 재앙이 된다.\n여러 개의 테이블 합치는 예시\n1 2 3 4 5 6 7 8 9 10 11 # Join SELECT * FROM orders O INNER JOIN users U ON O.user_id = U.id INNER JOIN point P ON O.id = P.order_id # Multiple Table In From # 테이블이 합쳐질때마다 무거워지는 WHERE 조건 SELECT * FROM orders O, users U , point P WHERE O.user_id = U.id AND O.id = P.order_id Join의 장점 3, 4 Left Join, Right Join, Full Outer Join의 지원으로 내가 원하는 형태로 테이블을 합치기가 더 좋다. 기본 Join과 마찬가지로 Multiple Table In From 를 사용해서는 원하는 결과를 뽑아내기 힘들거나, 더 복잡한 쿼리를 짜야 한다. Join을 사용하면 자연스럽게 Cross Join 문제를 피할 수 있다. Join Set 위에서 말한 것 처럼 Join은 여러가지 결합 형태를 지원하기 때문에 상황에 맞춰 테이블들을 조합해서 집합을 만들어 낼 수 있다. 검색을 해보니 정말 다양한 집합을 만들 수 있었는데 실무를 하면서 평소에 쓸 일이 제일 많았던 Join 형태 4개를 골라서 정리했다.\n예시로 사용할 데이터\nusers (사용자) id name number addr active 100 Peter 01012345678 대전광역시 Y 200 Lee 01087654321 경기도 N 300 Jamie 403926999 Calgary Alberta Canda N orders (주문) id user_id cost 4 100 33 5 100 11 6 200 2 7 200 100 8 100 20 9 400 200 위의 예시 데이터로 [Picture 3] 의 Join set을 다뤄보도록 하겠다.\n[Picture 3] Join Set Inner Join Join중에서 제일 많이 쓰는 기본형 JOIN.\n두 테이블에서 공통적으로 가지고 있는 데이터를 꺼내올 때 사용한다. table1 을 기준으로 table2에 있는 값이라고 해도 table1에 없으면 값이 누락된다. 이런때는 공통분모만 뽑아오는 Inner join이 아니라 커버 해주는 범위가 넓은 다른 JOIN 연산들을 써야 한다.\nLeft Join 왼쪽 테이블을 기준으로 데이터를 합치는 JOIN.\nJoin 연산에서 기준이 되는 테이블이 table1이다 보니까 쿼리를 짤 때도 데이터를 뽑아오는 중심이 되는 테이블을 table1 자리에 두는 것 같다. 그러다보니 table2의 데이터 존재 여부와 상관없이 일단 table1에 있는 데이터는 다 뽑아오는 Left Join을 Right Join 보다 많이 사용한다.\nLeft Join 예시\n1 2 3 SELECT * FROM users u LEFT JOIN orders o ON u.id = o.user_id; Left Join 결과\nid name number addr active id user_id cost 100 Peter 01012345678 대전광역시 Y 4 100 33 100 Peter 01012345678 대전광역시 Y 5 100 11 200 Lee 01087654321 경기도 N 6 200 2 200 Lee 01087654321 경기도 N 7 200 100 100 Peter 01012345678 대전광역시 Y 8 100 20 300 Jamie 403926999 Calgary Alberta Canda N \\0 \\0 \\0 Order에 300번 손님이 주문한 데이터가 없다고 해도 table1로 users 을 설정했기 때문에 결과에서도 300번을 보여준다.\nRight Join 오른쪽 테이블을 기준으로 데이터를 합치는 JOIN.\nLeft Join이 있는데 Right Join이 왜 필요할까? Left Join 하나만 있고 테이블 위치만 그때그때 바꿔주면 안되나 하는 의심이 있었다. 검색을 해보니 나랑 똑같은 생각을 한 사람이 있었는데 거기에 달린 답변을 보니 테이블 2개를 사용할 경우에는 테이블 순서를 바꿔가면서 Join연산을 하면 되는데 여러개의 테이블을 사용하면 그럴 수 없었다.\nRight Join 예시\n1 2 3 SELECT * FROM users u RIGHT JOIN orders o ON u.id = o.user_id; Right Join 결과\nid name number addr active id user_id cost 100 Peter 01012345678 대전광역시 Y 4 100 33 100 Peter 01012345678 대전광역시 Y 5 100 11 200 Lee 01087654321 경기도 N 6 200 2 200 Lee 01087654321 경기도 N 7 200 100 100 Peter 01012345678 대전광역시 Y 8 100 20 \\0 \\0 \\0 \\0 \\0 9 400 200 Left Join을 했을때와는 다르게, users에서만 존재하던 300번 유저에 대한 정보는 사라졌고 orders에만 존재하는 400번 유저에 대한 구매 내역이 나왔다.\nFull Outer Join Full Outer Join = 1 Time of Inner Join + Left Join + Right Join\n합집함을 위한 Join이다. 테이블 1과 테이블 2를 합칠 때 생기는 중복을 처리하고 돌려준 값이라고 생각하면 된다. 아쉽게도 Mysql 에서는 Full Outer Join을 지원하지 않는데 Left Join과 Right Join을 사용해서 똑같은 결과를 만들어낼 수 있다.\nMySQL Full Outer Join 예시\n1 2 3 4 5 6 7 8 9 SELECT * FROM users U LEFT JOIN orders O ON U.id = O.user_id UNION SELECT * FROM users U RIGHT JOIN orders O ON U.id = O.user_id MySQL Full Outer Join 결과\nid name number addr active id user_id cost 100 Peter 01012345678 대전광역시 Y 4 100 33 100 Peter 01012345678 대전광역시 Y 5 100 11 200 Lee 01087654321 경기도 N 6 200 2 200 Lee 01087654321 경기도 N 7 200 100 100 Peter 01012345678 대전광역시 Y 8 100 20 300 Jamie 403926999 Calgary Alberta Canda N \\0 \\0 \\0 \\0 \\0 \\0 \\0 \\0 9 400 200 재미있는 점은 Left Join과 Right Join을 했을 때 각각 Inner Join을 한 것 과 같은 교집합 값이 생기는데 UNION을 해서 중복을 제거했다는 점이다! 만약 UNION ALL을 사용했으면 중복이 그대로 포함된다.\nReference [Difference Between Join and Inner Join] [The History of SQL Standards] [What\u0026rsquo;s the Difference Between Having Multiple Tables in FROM and Using JOIN?] [Why do we have Left Join and Right Join in SQL, if we can use Left Join to get same result as of Right Join by just changing the position of tables?] ","date":"2021-07-14","permalink":"https://leeleelee3264.github.io/post/2021-07-14-mysql-history-of-join-and-join-set/","tags":["Infra"],"title":"MySQL Join \u0026 Join Set"},{"content":"\nMySQL String 함수와 참조 무결성 (Referential Integrity) 에 대해서 다룹니다.\nIndex\nMySQL에서 String 함수 참조 무결정 소소한 하이디 SQL \u0026amp; markdown 팁 Reference 리뷰 후에 알게된 부분들 [Markdown 각주/미주 달기] 1 SUBSTRING_INDEX(문자열, 구분자, 가져올 구문 갯수) 1 2 3 4 SELECT SUBSTRING_INDEX(addr, \u0026#39; \u0026#39;, 2) FROM users; # 경기도 성남시 판교동 대왕판교로 --\u0026gt; 경기도 성남시 까지 나옴 도메인 무결성과 고유 무결성(Unique Column Values) 도메인 무결성은 만약 Gender를 표현 한다고 했을 때 남성/여성/무성 등 미리 정의가 되어있는 도메인 (convention같은 느낌)에 속한 값이 들어가야 한다. 고유 무결성은 row에 속해있는 특정한 column 들이 유니크한 고유의 값일때만 업데이트와 인서트를 허가한다는 뜻이다. UNIQUE key constraints이 이 무결성을 지키는 역할을 한다. A unique value rule defined on a column (or set of columns) allows the insert or update of a row only if it contains a unique value in that column (or set of columns) \u0026hellip; That is, no two rows of a table have duplicate values in a specified column or set of columns.\nMySQL에서 String 함수 [Picture 1] MySQL String 함수 1 [Picture 2] MySQL String 함수 2 Example LENGTH(), CHAR_LENGTH() LENGTH 는 바이트로 글자를 카운트하고 CHAR_LENGHT는 글자 자체의 길이를 카운트한다. 한글의 경우 1글자가 2바이트라서 제이미 세글자면 6바이트가 나올 것 같은데 DB에 설정된 utf8 charset은 한글을 3바이트로 저장하기 때문에 총 9바이트가 나왔다.\nLENGTH(), CHAR_LENGTH() 예제\n1 2 3 4 5 6 7 SELECT NAME, LENGTH(NAME), CHAR_LENGTH(NAME) FROM users # where 조건에도 사용이 가능해서 검색을 할 때 응용해서 사용하면 좋을 것 같다. SELECT NAME FROM users WHERE CHAR_LENGTH(NAME) \u0026gt; 3 LENGTH(), CHAR_LENGTH() 결과\nNAME LENGTH(NAME) CHAR_LENGTH(NAME) Peter 5 5 제이미 9 3 RIGHT(), LEFT(), MID() RIGHT(), LEFT(), MID() 는 파이썬 String 연산과 동일해서 놀랐다. 어떻게 활용을 할까 했는데 where로 검색을 하기에는 이미 더 편한 like가 다 커버를 해주고 있다. where절에 걸기보다는 select 절에 걸어서 간단하게 데이터를 조작하고 비식별화 하는 것에 사용하면 좋아보인다.\n예제를 작성하다보니 이래저래 간편하게 쓰기 좋아보이는 유틸성 함수같다. SUBSTRING()과 SUBSTR()도 비슷한 맥락에서 사용이 되면 좋을 것 같은데 이건 좀 더 정교한 조작을 할 때 이용!\nRIGHT(), LEFT(), MID() 예제\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 SELECT NAME FROM users WHERE LEFT(NAME, 1) = \u0026#39;이\u0026#39;; # like로 더 쉽게 찾을 수 있다. SELECT NAME FROM users WHERE NAME LIKE (\u0026#39;이%\u0026#39;); # concat()연산과 함께 사용해서 데이터 비식별화..? SELECT concat(left(NAME,1), \u0026#39;**\u0026#39;) FROM users WHERE LEFT(NAME, 1) = \u0026#39;이\u0026#39;; # Replace() 연산과 함께 사용해도 가능! SELECT replace(name, RIGHT(NAME, 2), \u0026#39;**\u0026#39;) FROM users WHERE LEFT(NAME, 1) = \u0026#39;이\u0026#39;; # 작정하고 만들면 이렇게 data format을 씌워줄 수 있는데 여러번 써야 한다면 엑셀에서 수정을 하거나 #프로그래밍으로 정규식을 입혀주는 게 더 간편하다는 생각이 든다. SELECT concat(left(number,3), \u0026#39;-\u0026#39;, MID(NUMBER, 4, 4), \u0026#39;-\u0026#39;, RIGHT(NUMBER, 4)) FROM users WHERE LEFT(NAME, 1) = \u0026#39;이\u0026#39;; RIGHT(), LEFT(), MID() 결과\nconcat(left(number,3), \u0026lsquo;-\u0026rsquo;, MID(NUMBER, 4, 4), \u0026lsquo;-\u0026rsquo;, RIGHT(NUMBER, 4)) number 010-8765-4321 01087654321 INSERT() INSERT() 예제\n1 2 3 4 5 6 SELECT INSERT(NAME, CHAR_LENGTH(NAME), 5, \u0026#39; 고객님\u0026#39;) FROM users # concat으로 해결 SELECT CONCAT(NAME, \u0026#39; 고객님\u0026#39;) FROM users; INSERT() 결과\nINSERT(NAME, CHAR_LENGTH(NAME), CHAR_LENGTH(NAME), \u0026rsquo; 고객님\u0026rsquo;) Pete 고객님 제이 고객님 Peter 고객님과 제이미 고객님을 뽑고 싶었으나 내 의도대로 나오지 않았다. 받는 인자가 INSERT(문자열, 시작위치, 길이, 새로운 문자열 순서인데 길이가 어떤 역할을 하는지 잘 모르겠다. 스트링을 조작할 때는 INSERT()로 하려 하지 말고 CONCAT() 으로 간단하게 끝내야 할 것 같다.\nLPAD(), RPAD() LPAD는 왼쪽으로, RPAD는 오른쪽으로 해당 길이 만큼 입력된 글자들을 채워주는 기능. 단순하게 보면 글자수를 맞춰서 채워주는 기능인데 화면에서나 문서에서 글자들이 모두 같은 글자수를 가지고 있어야 할 때 쓸 일이 있어보인다. RPAD(NAME, 10, ' ') 로 자릿수만 채워줄 수 도 있다.\nLPAD(), RPAD() 예제\n1 2 SELECT RPAD(NAME, 10, \u0026#39;*\u0026#39;), LPAD(NAME, 10, \u0026#39;\u0026amp;\u0026#39;) FROM users; LPAD(), RPAD() 결과\nRPAD(NAME, 10, \u0026lsquo;*\u0026rsquo;) LPAD(NAME, 10, \u0026lsquo;\u0026amp;\u0026rsquo;) Peter***** \u0026amp;\u0026amp;\u0026amp;\u0026amp;\u0026amp;Peter 제이미******* \u0026amp;\u0026amp;\u0026amp;\u0026amp;\u0026amp;\u0026amp;\u0026amp;제이미 PAD라는 단어가 익숙해서 어디서 들어봤나 했는데 저번 회차에서 주임님이 말씀하셨던 db엔진에서 글자수 비교를 할 때 CHAR형은 지정된 글자수를 맞춰주기 위해 그보다 짧은 단어들에 공백을 채워서 비교해준다는 PAD Attribute와 같은 기능이었다.\n[Picture 3] PAD의 뜻 참조 무결정 관계를 맺은 테이블들의 데이터 사이에서 일관성을 유지하기 위해 foreign key를 만들 때 참조무결성 제약조건이 만들어진다. 강의를 듣는 처음에는 참조무결성이 왜 필요할까? 왜 foreign key를 사용하지? 라는 의구심이 들었는데 데이터를 일관적으로 관리 한다는 성격 때문에 참조무결성을 사용하는 것 같다.\nforeign key의 특징 본테이블에 없는 데이터를 sub 테이블 (참조하는 테이블)에 넣을 수 없다. 이 특성이라면 프로그래밍적 오류를 db단에서 막을 수 도 있다. 본테이블에 없는 데이터로 sub의 foreign key를 수정할 수 없다. update와 delete에 cascade 옵션을 걸어놨을 경우 본테이블에 변경이 있으면 아무것도 안해도 sub 테이블에 알아서 반영을 해준다. 예를 들어 admin_user가 본테이블이고 admin_favorite이 sub 테이블이면 직원 중 하나가 퇴사를 하면 본테이블에 delete을 했을때 sub테이블에서도 자동으로 delete이 되어서 따로 관리를 할 필요가 없다. stateful 하지 않은 history 데이터도 foreign key 의미가 있을까? 외래키를 설정하는게 이렇게 1:1로 데이터가 stateful할 떄 (ex: 사용자-디바이스 정보) 유용하지 그냥 밑으로 확장하는 history성 테이블에서도 의미가 있을까? 또 의심을 했는데 일단 history성이여도 본테이블에 없는 데이터를 넣지 못하는 일관성이 보장이 되기 때문에 필요 자체가 없는 기능은 아닌 것 같다.\nforeign key action Restrict : 본테이블에 있는 데이터를 바꾸거나 삭제하려는데 다른테이블이 참조를 하고 있을 경우 action을 무시한다. No Action : mysql에서는 restrict 와 동일하다. Cascade : 본테이블에서 수정/삭제가 일어났을 때 참조하고 있는 테이블도 동일하게 수정/삭제가 적용된다. Set Null : 본테이블에서 수정/삭제가 일어났을 때 참조하고 있는 테이블의 Foreign Key를 NULL로 만든다. 수정에서 보다 삭제에서 더 유용한 설정 생각해 볼 부분 만약 데이터를 정말 지워버리는 Hard Delete말고 flag를 설정해줘서 지운 척을 하는 Soft Delete에서는 Foreign Key Delete Action을 어떻게 설정해주는 게 좋을까?\n소소한 하이디 SQL \u0026amp; markdown 팁 하이디 sql에서 격자행 내보내기 할 때 대다수의 경우 csv 파일로 내보내기를 했는데 wiki나 블로그 작성 용으로 결과를 마크다운으로도 내보낼 수 있는 걸 오늘 발견했다.\n이렇게 하면 [Table Generator] 에 가서 셀 하나하나를 만들거나 결과창을 캡처 해 올 필요가 없어서 굉장히 편리하게 포스팅을 할 수 있다.\n[Picture 4] 하이디 SQL 격자 행 내보내기 [Picture 5] 하이디 SQL 격자 행 내보내기 결과 Reference [Mysql String Function Official Document]\n[MySQL 문자열 함수]\n[MySQL RESTRICT and NO ACTION]\n[Mysql Foreign Key Action Official Document] 1: 주석에 관한 설명을 이곳에\u0026hellip;\n","date":"2021-07-08","permalink":"https://leeleelee3264.github.io/post/2021-07-08-mysql-stringfunction-and-refrential-integrity/","tags":["Infra"],"title":"MySQL String 함수와 참조 무결성"},{"content":"\nMySQL SQL과 key, collation에 대해 다룬다.\nIndex\nStructured Query Language Key Collation Reference 리뷰 후에 알게된 부분들 Primary Key가 들어간 column 의 이름도 아닌 제약조건에 이름을 만드는 이유\n주임님이 주신 의견: 기본키를 항상 하나의 column으로 만드는 건 아니고 여러가지 키를 섞어서 만들었을 때 보기 편하게 하기 위해. Charset = Symbol (plain text) + encoding\nmysql 에서 지원하는 Collation을 쿼리할 때 나오는 pad-attribute란 무엇인가?\n가변형인 VARCHAR 말고 CHAR 형의 텍스트를 비교할 때 뒤에 공백을 넣어서 비교하는지 아닌지의 여부. 모든 DB가 그런것은 아니다. [Reference: MySQL에서 ‘a’ = ‘a ‘가 true로 평가된다?] 테이블에 걸린 제약조건을 확인하는 방법 (테이블 하나씩 검색하는 방법은 없고 sql 서버 전체에 걸려있는게 나와서 where 절로 뽑아내야 한다)\n1 2 3 4 5 select * from information_schema.table_constraints; # use where select * from information_schema.table_constraints WHERE CONSTRAINT_SCHEMA = \u0026#39;test\u0026#39; 외래키는 고유하게 식별이 가능한 데이터면 되기 때문에 꼭 primary key말고 unique key를 이용해서도 외래키를 만들 수 있다.\nForeign key should be made with primary key? Structured Query Language DML Data Manipulation Language\nSELECT, INSERT, DELETE, UPDATE 데이터를 조작하기 위한 SQL 커맨드 개발을 할 때 기본이 되는 CRUD (Create/Read/Update/Delete)이 해당된다. 응용프로그램으로 접근할 수 있는 유일한 SQL이다. DDL Data Definition Language\nCREATE, ALTER, RENAME, DROP, TRUNCATE 데이터베이스의 스키마를 정의/조작 하기 위한 SQL 커맨드 데이터베이스의 구조와 제약조건에 대한 전체적인 명세를 가지고 있는 메타성 데이터의 집합을 스키마라고 한다. DB에 가장 크리티컬하게 영향을 미치는 커맨드라 DBA나 DB 설계자가 자주 사용하는 커맨드 ALTER, RENAME 예시\n1 2 3 4 5 # ALTER 으로 테이블 이름 변경하기 ALTER TABLE old_name RENAME new_name; # RENAME 으로 테이블 이름 변경하기 RENAME TABLE old_name TO new_name; DROP과 TRUNCATE 예시\n1 2 3 4 5 6 7 8 9 10 11 12 # DROP으로 테이블을 지워버리기 DROP TABLE table_name; # TRUNCATE으로 테이블 초기화하기 TRUNCATE TABLE table_name; # DROP으로 테이블 초기화하기 DROP TABLE table_name; CREATE TABLE table_name ( field_name field_type ... ) Drop은 테이블 자체를 데이터베이스에서 지워버리는데 Truncate은 테이블의 컬럼, 제약조건들은 남겨두고 데이터만 지워준다. 조건을 이용해서 레코드 하나씩 지워가는 Delete 보다 Truncate이 실행속도는 더 빠르다고 한다.\nDDL인데도 불구하고 응용프로그램에서도 Truncate을 사용하는 경우가 있다고 하는데 응용프로그램에서 테이블의 데이터를 모두 지워버리는 행위는 너무 위험하다고 생각한다.\nDCL Date Control Language\nCOMMIT, ROLLBACK, GRANT, REVOKE DB의 권한과 무결성을 지키기 휘한 Transaction을 위한 SQL 커맨드 COMMIT과 ROLLBACK을 따로 빼서 TCL (Transaction Control Language)라고 하기도 한다. Transaction은 커맨드를 테스트 할 때 데이터의 변경 없이 테스트를 하고 싶을 때 활용할 수 있다. TRANSACTION 예시\n1 2 3 4 5 6 7 START TRANSACTION; SELECT * FROM users; DELETE FROM users WHERE NAME = \u0026#39;jamie\u0026#39;; SELECT * FROM users; ROLLBACK; 이런식으로 Transaction을 걸면 각 줄이 실행되면서 SELECT로 결과를 보여주고 Rollback 커맨드로인해 테스트를 하기 전의 상태로 데이터베이스가 돌아간다. 일반 SELECT가 아닌 조건이 복잡한 DELETE, UPDATE, INSERT 쿼리를 사용해야 할 때 쓰면 좋아보인다.\n그런데 저렇게 짠 쿼리 중에 오류가 있으면 마지막 ROLLBACK까지 오지가 않기 때문에 Transaction 처리가 안된다. 아무리 Transaction을 걸었다고 해도 조심을 해야할 필요가 있어보인다.\n[Picture 1] Transaction 실패 Key Key와 제약조건의 차이 Key 키는 테이블의 단일 필드에 걸거나 복합적인 필드에 걸 수 있다. 테이블에서 조건에 따라 데이터를 가져오기 위해 사용되며 다른 테이블과 뷰 사이의 관계 또한 생성을 할 수 있다. 내가 생각했던 것 처럼 키 자체를 제약조건이라고 보기는 힘들다 . 제약조건 제약조건은 데이블의 데이터를 위한 특정한 규칙들이다. 만약 data action(SQL command로 데이터를 변경하기 위한 action들)이 제약조건을 위배한다면 그 data action을 취소해버린다. 제약조건은 테이블을 생성할 시기나, 생성한 이후에 걸 수 있다. DBMS에서의 Key와 MySQL에서의 키 DBMS의 키와 Mysql에서 사용하는 키의 차이점은 DBMS에서 말하는 Key들은 Concept에 가깝다. Mysql은 실제로 Primary Key와 Foreign Key 기능을 사용할 수 있고 나머지 Key들은 데이터베이스를 설계할 때 고려가 되는 개념들이었다.\nDBMS Primary Key 테이블 안에서 중복되지 않은 값을 사용해서 데이터들(row)을 고유하게 식별한다. 테이블 안에서 Primary Key 는 하나밖에 있을 수 없다. NULL은 absence of value 기 때문에 값이라고 할 수 없다. 그래서 Primary Key에 NULL값을 넣을 수 없다. 물리적인 인덱스를 생성하는데 값이 비어있으면 순서를 정할 수 없어서 NULL값을 넣을 수 없다. Primary Key = Unique Constraint + NOT NULL Foreign Key가 걸려있으면 Primary Key를 변경할 때 이를 참조하고 있는 Foreign Key까지 함께 변경해줘야 한다. 이떄 ON UPDATE CASCADE 기능을 사용한다. Foreign Key 테이블들 사이에 관계를 만들고, 참조 무결성을 유지하는데 사용된다. 테이블을 연결시켜 주기 때문에 어떤 데이터인지 고유하게 식별이 가능해야 해서 Foreign Key를 만들기 위해서는 Primary Key를 사용해야 한다. NULL 값이 들어갈 수 있다. Foreign Key로 이미 관계는 만들어졌으나 참조하는 테이블에서 Primary Key를 비롯한 데이터가 아직 만들어지지 않았을 수 있기 때문이다,. Super Key 테이블에서 데이터를 식별할 수 있게 해주는 Key 또는 Key Set 식별만 가능하면 되기 때문에 여러가지 Column들을 조합해서 여러개의 Super Key 를 만들 수 있다. Primary Key도 Super Key에 들어있는 Key 중 하나이다. Candidate Key Super Key와 동일한 성격을 가지고 있는 Key 데이터를 식별해주나 최소한의 Column을 사용해야 한다. (Minimal Super Key) Primary Key도 Candidate Key 에서 뽑힌다. [Picture 2] Key 다이어그램 Super Key와 Candidate Key 예시\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # Example Of Super Key and Candidate Key # 전화번호에 중복이 없다는 전제 | Id | Number | Name | |----|---------------|----------| | 1 | 111-1111-1111 | Seungmin | | 2 | 222-2222-2222 | Jamie | | 3 | 333-3333-3333 | LeeLee | # Super Key {Id} {Number} {Id, Number} {Id, Name} {Id, Number, Name} {Number, Name} # Candidate Key {Id} {Number} Alternate Key Primary Key로 선정되지 않은 Key들 Compound Key 와 Composite Key 여러개의 Column을 묶어서 테이블에서 고유하게 식별가능하게 만든 복합키 Compound Key는 Column 중에 Foreign Key 를 가질 수 있다. Composite Key는 Column 중에 Foreign Key 를 가질 수 도 있고 가지고 있지 않을 수 도 있다. Compound Key와 Composite Key를 구별하는 게 특별이 중요하지 않아서 자주 바꿔서 부른다고 한다. Surrogate Key 각 데이터들을 고유하게 식별하기 위한 대리로 만들어진 키 데이터 자체에는 아무런 의미도 없고, 정보도 없다. Primary Key 로 자주 사용되는 Auto increment 가 걸린 Id가 여기에 해당된다. Id 자체에는 아무런 뜻이 없으나 서비스 에서는 유저들을 식별하는 정보로 사용된다. Natural Key라고 반대 개념이 있는데 이것은 정보 자체를 사용해서 데이터들을 식별한다. 예를 들어 유저를 식별하는 값으로 주민등록번호 사용. Surrogate Key VS Natural Key Surrogate Key와 Natural Key 사이에서 Primary Key 를 고르는 문제가 빈번히 대두되는데 아무리 고유한 값이라도 일말의 바뀔 여지가 있는 Natural Key 보다는 (주민등록 번호 변경 등) 비즈니스 안에서는 절대 변할 수 없는 Surrogate Key 를 Primary Key 로 등록하는게 좋다.\nMySql Key Key를 만들면 Index가 만들어진다. 결국 일반 Key는 Index와 동일하다. Primary Key Unique Key Foreign Key Collation Charset: 글자들을 어느정도 되는 크기에 집어 넣을 것인지 Collation: 텍스트들을 어떻게 정렬할 것인지 MySQL에서 지원하는 Collation utf8과 utf8mb4의 Charset으로만 이루어진 Collation도 67개나 지원을 하고 있다. Mysql에서는 총 322개의 Collation을 지원하고 있다.\nMySQL의 Collation을 확인하는 쿼리 예시\n1 2 3 4 5 SHOW COLLATION # 제일 자주 사용하는 charset인 utf를 지원하는 Collation SHOW COLLATIOn WHERE CHARSET LIKE (\u0026#39;%utf8%\u0026#39;) [Picture 3] SQL Charset UTF charset 한글을 사용하려면 utf8을 사용하면 된다. 웹에서는 자연스럽게 utf8로만 설정을 하면 되지만, mysql에서는 utf8, utf8mb4, utf16, utf32 로 다양한 utf를 지원한다.\n나는 charset 크기가 점점 커져서 utf8 → utf8mb4 → utf16 → utf32 이런 순서로 만들어진 줄 알았는데 그렇지는 않았다. 사용하는 공간이 큰 utf16과 utf32보다는 utf8과 utf8mb4가 많이 쓰인다.\nutf8mb4? utf8에서 utf8mb4가 생겨난 이유는 이모지들을 저장하기 위해서다. 원래대로라면 utf8은 8bits, 즉 1byte만 들어갈 것 같지만 사실 utf8은 1~4byte 까지 사용할 수 있는 가변크기를 가지고 있다. 그래서 plain text인 영어와 한국어는 각각 1byte, 3byte 크기를 이용해서 저장이 된다.\nMysql을 설계할때도 utf8형이 3byte를 넘어갈 일이 없다고 생각을 했어서인지 utf8이 실제로 저장할 수 있는 공간을 3byte로 제한을 해버렸다. 그런데 한자나 이모지들은 길이가 4byte인 문자열들이라서 Mysql에서 utf8을 쓰다가는 데이터가 다 들어가지를 않는다.\n그래서 원래의 utf8의 취지에 맞게 1에서 4byte 까지 가변적으로 크기를 가질 수 있는 charset인 utf8mb4를 만들었다. 그리고 대부분의 경우 utf8mb4 Charset을 사용한다.\n자주 사용하는 Collation 실제 서비스에서는 텍스트를 관리하는 Column은 대부분 utf8mb4_unicode_ci를 사용하고, 옛날에 실수로 잘못 설정을 했을 때 utf8mb4_general_ci를 사용했다. 여기서 ci란 Case Insensitive 의 약자로 대소문자를 구별하지 않겠다는 뜻이다.\nInsensitive Charset 예시\n1 2 3 4 5 6 7 8 9 10 # use case insensitive charset SELECT * FROM users WHERE name = \u0026#39;Jamie\u0026#39;; SELECT * FROM users WHERE name = \u0026#39;jamie\u0026#39;; # 대소문자를 구별하지 않기 때문에 SELECT의 결과는 동일하다. utf8mb4_bin 바이너리 값을 그대로 저장한다. bin은 binary의 약자이다. utf8mb4를 사용하지만 텍스트가 아닌 바이너리값이 중심이라 파일이나 이미지를 저장하는 Column에 사용하면 된다. utf8mb4_unicode_ci Unicode 규칙을 따라서 정렬이 되었고 언어들 사이에서 제일 많이 선택받는 Collation. 특별한 기호들을 사용할때도 unicode_ci를 사용하는 편이라고 한다. utf8mb4_general_ci 텍스트 형태로 정렬을 해준다. 한국어, 영어, 중국어, 일본어는 general_ci와 unicode_ci collation의 결과값이 동일하다. general_ci는 속도개선을 중점으로 하고 있기 때문에 정렬과 비교를 할 때 공식적인 Unicode 규칙을 따르지 않고 내부에서 따로 디자인을 했다. 그래서 때에 따라서 우리가 기대하는 (Unicode 규칙에 따른) 값이 나오지 않을 수 있다. 요즘은 CPU의 성능이 많이 좋아져서 Unicode 규칙을 따르지 않는 general_ci를 써가면서 성능향상을 할 필요가 없다! Reference [Difference Between Key and Constraint] [Difference Between Composite and Compound Key] [Primary Key - Surrogate Key VS Natural Key] [deep dive into utf8 and utf16] [Difference Between unicode_ci and general_ci] ","date":"2021-06-30","permalink":"https://leeleelee3264.github.io/post/2021-06-30-mysql-sql-and-key-collection/","tags":["Infra"],"title":"MySQL SQL과 key, collation"},{"content":"\nSummary of post\nIndex\nClustered Index / Non-Clustered Index - Concept Clustered Index / Non-Clustered Index - Structure and Index algorithm Side effects of Indexes Related SQL Command with Indexes Reference 리뷰 후에 알게된 부분들 Non-Clustered Index가 여러 개 생성이 가능하지만 무제한으로 만들 수 있는 것은 아니다. 조회는 Clustered Index가 빠르지만, 수정과 입력은 Non-Clustered Index가 빠르다. 왜냐하면 Non-Clustered Index는 물리적인 정렬을 가지고 있지 않아서 새 데이터가 들어와도 순서대로 정렬을 안 해도 된다. Clustered Index / Non-Clustered Index - Concept Clustered Index와 Non-Clustered Index 둘 다 테이블에 있는 데이터 엑세스를 빠르게 하기 위한 용도이지만 둘의 성격이 조금 다르다.\n가장 큰 차이는 Clustered Index는 테이블 당 딱 1개만 만들어지고, Non-Clustered Index는 복수개가 만들어 질 수 있다는 점과 Clustered Index를 생성하는 Column은 중복을 허용하지 않지만 Non-Clustered Index는 중복을 허용한다는 점이라고 생각한다.\nClustered Index Primary Key 로 만들어진다. 테이블에 단 하나만 만들 수 있다. 중복된 데이터를 허용하지 않는다. 테이블에서 데이터를 순서대로 저장하게 해준다. 크기가 큰 테이블에서 데이터를 빠르게 찾아오게 하기 위해서 꼭 만들어주는 게 좋다. 속도가 Non-Clustered Index 보다 빠르다. Non-Clustered Index Unique Key와 Key 로 만들어 진다. Mysql의 일반 Key는 인덱스를 생성하는 역할만 수행한다. Unique Key의 특성상 해당 키로 인덱스를 생성했다면 중복값이 있을 수 없다. 그래서 Unique Key의 Index를 Unique Non-Clustered Index라고 하기도 한다. 테이블에 여러개를 만들 수 있다. 중복된 데이터를 허용한다. 데이터 저장에 아무런 영향을 미치지 않는다. 과도한 Non-Clustered Index 생성은 퍼포먼스를 떨어뜨리기 때문에 잘 설계해서 꼭 필요한 부분만 만들어야 한다. 속도가 Clustered Index 보다 느리다. Clustered Index / Non-Clustered Index - Structure and algorithm Clustered Index Structure 트리 구조로 만들어진다. (B-tree) 각 노드에는 실제 데이터가 저장된다. 실 데이터를 가지고 있기 때문에 IO 작업이 적어 데이터 찾기가 훨씬 빠르다. (1)속도가 빠르고 (2)모든 Column을 필요로 하는 읽기 전용 어플리케이션이라면 Clustered Index를 만들면 된다. [Picture 1] Cluster structure Non-Clustered Index Structure 트리 구조로 만들어진다. (B-tree) 각 노드에는 포인터 (주소)가 저장되어 있고, 이 주소들은 Clustered Index의 노드들을 가르키고 있다. 인덱스 안에 데이터들은 논리적으로 순서를 가지고 있기 때문에 물리적으로는 다른 순서를 가지고 있을 수 있다. Index에 해당되는 Column에 따라 논리적으로 정렬이 되어있다. Non-Clustered Index를 사용해 검색을 했을 경우 결국 Clustered Index까지 거쳐서 결과가 나오게 된다. Clustered Index가 없는 테이블의 Non-Clustered Index는 Heap을 가르킨다. Heap in MySQL Clustered Index를 설정하지 않은 테이블을 Heap이라고 한다. Heap은 순서없이 저장된 데이터들을 가지고 있다. Heap은 순서를 신경쓰지 않아 크기가 크고, 순서가 중요하지 않은 데이터들의 Insert 연산이 필요한 테이블에 사용된다. [Picture 2] Non-cluster structure Index algorithm B-Tree O(logN)의 시간 복잡도를 가진다. 하나의 노드에 여러가지 데이터를 가질 수 있고, 때문에 참조 포인터가 적어 빠른 메모리 접근이 가능하다. 탐색 뿐 아니라 수정과 저장에서도 O(logN)의 시간복잡도를 가진다. B-Tree를 쓰는 이유 시간복잡도가 O(1)인 Hash 대신에 O(logN) 시간이 걸리는 B-tree를 사용하는 이유는 Hash는 Key값을 가지고 단 하나의 데이터에 엑세스 하기 때문이다. 그래서 연산에서 동일한 값을 찾아내는 = 만 사용이 가능하고, 대소비교를 하거나 Between 을 사용할 수 없게 된다. 데이터 엑세스가 빠른 배열을 사용하지 않고 포인터로 분산되어있는 B-Tree를 쓰는 이유는 배열은 탐색을 할 때만 빠르고 다른 연산에 대해서는 B-Tree보다 더 느리기 때문이다. [Picture 3] B-Tree index algorithm R-Tree 공간 정보 탐색을 위한 알고리즘 (기하학) Mysql에서 공간정보를 관리하는 타입인 Geometry와 하위 타입들에서 쓰인다고 한다. [R-Tree 알고리즘] Side effects of Indexes Clustered Index가 있으면 데이터를 Insert/update 할 때 마다 순서대로 다시 정렬을 해줘야 하기 때문에 속도가 느리다. Non-Clustered Index는 디스크에 분산이 되어 저장이 되는데 인덱스를 만들 때 Column 값이 중복으로 저장이 되기 때문에 디스크 낭비가 발생할 수 있다. Index를 너무 많이 만들어두면 오히려 탐색 성능까지 떨어지게 된다. 탐색 성능 저하 시나리오\nINDEX1는 A와 B Column을 가지고 있고, INDEX2는 A와 C Column을 가지고 있다. 이런 경우 디스크에는 B가 2 번 기록이 된다. 이런 상태에서 B와 C를 조회한다면 INDEX 1,2 를 둘 다 사용해서 포인터를 찾고, 테이블에 있는 데이터를 찾기 시작한다.\nRelated SQL Command with Indexes 인덱스와 관련된 Command는 DML이 아니라 DDL (Date Define Language) 다!\n인덱스 쿼리 예시\n1 2 3 4 5 6 7 8 9 10 11 12 13 # 생성되어있는 인덱스 확인 SHOW INDEX FROM table_name # 인덱스 추가 ALTER TABLE table_name ADD INDEX index_name (column1, column2); # 인덱스 삭제 ALTER TABLE table_name DROP INDEX index_name; # 인덱스를 타는지 안 타는지 실행계획 보기 EXPLAIN query ex) EXPLAIN SELECT * FROM table_name EXPLAIN 쿼리 EXPLAIN 쿼리를 사용하면 작성한 쿼리의 성능과 index 사용 여부 등을 자세히 알 수 있다.\n[Picture 4] EXPLAIN 쿼리 예시 Id SELECT 문의 순서 서브 쿼리가 있을 경우 순서를 부여해준다. select_type SELECT 문의 유형 SIMPLE: UNION이나 서브 쿼리를 사용하지 않은 SELECT PRIMARY: 서브 쿼리를 사용했을 때 제일 바깥에 있는 SELECT SUBQUERY: 서브쿼리 SELECT DERIVED: FROM 절에 서브쿼리가 있는 SELECT type SELECT 문을 실행하기 위해 테이블을 어떻게 조회했는지를 나타낸다. ALL: table full scan, 테이블을 처음부터 끝까지 조회했다. (제일 나쁨) INDEX: index full scan, 인덱스를 처음부터 끝까지 조회했다. RANGE: 특정 범위 안에서 인덱스를 사용해서 조회했다. INDEX_MERGE: 두 개의 인덱스를 병합해서 조회했다. REF: 조인을 할 때 Primary 나 Unique 가 아닌 Key로 매칭해서 조회했다. CONST: 매칭되는 row가 단 한 건이며, Primary 나 Unique를 사용해서 조회했다. possible_keys 해당 Column을 찾기 위해 사용된 인덱스 이 값이 NULL이라면 사용된 인덱스가 없는 것이다. (이걸 보고 인덱스를 타게 수정이 가능하다) key 최적화를 위해 Mysql 옵티마이저가 사용하기로 결정한 인덱스 ref 데이터를 추출하기 위해 키와 함께 사용된 컬럼 또는 상수 rows 쿼리를 수행하기 위해 검색해야 할 Row의 갯수 Extra Mysql 옵티마이저가 추가로 해석한 정보. using index : 인덱스를 이용해 자료 추출 using where : where 조건으로 데이터를 추출, 만약 type이 NULL인데 이 값이 나왔다면 쿼리 성능이 좋지 않다는 뜻 using temporary: 쿼리 안에서 임시 테이블을 생성/사용 using filesort: 데이터 정렬 연산이 포함됨. Reference [Clustered Vs Non Clustered Index] [데이터베이스 인덱스는 왜 B-Tree를 선택하였는가] [R-tree 알고리즘] [Mysql Explain 실행계획 사용법 및 분석] ","date":"2021-06-21","permalink":"https://leeleelee3264.github.io/post/2021-06-21-mysql-index/","tags":["Infra"],"title":"MySQL Index"},{"content":"\n리눅스 서버 설치와 초기 설정에 대해 다룬다.\nIndex\n리눅스 설치 초기 네트워크 설정 SSH 설정 사용자 설정 기타 설정 MariDB 설치/설정 리눅스 설치 리눅스 설치용 usb 리눅스를 설치하려면 설치용 usb를 먼저 만들어야 한다. [리눅스 usb 만들기] 이 블로그에 나오는 포스팅을 참고해서 Rufus 로 우분투 20 server 용 부팅 usb 를 만들었다.\n부팅용 usb가 만들어지면 다른 운영체제를 설치할때와 마찬가지로 부팅시에 boot configuration으로 들어가는 버튼을 열심히 눌러서 usb drive를 선택해서 부팅하면 리눅스 설치는 끝이 난다.\n리눅스 server VS desktop 사실 이전에 한가지 실수를 했는데 우분투 20 desktop 부팅 usb를 만들어서 서버를 설치했다. 노트북을 닫을때 운영체제에서 일어나는 동작을 컨트롤 할 수 있기 때문에 닫는 동작을 무시하면 서버가 계속 켜저있을 거라고 생각했는데 1주일 정도 노트북을 닫고 서버를 돌리니 서버가 다운되었다. 찾아보니 우분투를 서버로 돌릴 목적이라면 초반부터 꼭! 우분투 서버용을 사용해야 했다.\n[노트북 덮어도 서버 유지] 이 포스팅에 노트북 닫을 때 우분투의 작동 제어 방법이 나와있다.\n리눅스 설치 노트북 리눅스를 설치할 노트북은 dell xps 14 2012 모델로 InsydeH20 bios를 사용하고 있다. 처음에는 이 bios 설정 사항들을 잘 몰라서 시간을 많이 썼는데 이 [블로그] 를 보면서 따라할 수 있었다.\n초기 네트워크 설정 포스팅에서는 네트워크를 와이파이로 연결했는데, 네트워크 속도가 너무 느려 나중에는 결국 이더넷 연결을 했다.\n와이파이 연결 집에서 KT 와이파이를 쓰기 때문에 우분투 서버에서도 와이파이에 연결하려고 했는데 초기에는 서버를 설치하고 나서는 랜선에 우선 연결을 해야 한다.\n와이파이에 연결하려면 몇개의 패키지를 받아야 하는데 네트워크가 연결되지 않은 상태로는 다운을 받을 수 없다\u0026hellip; 우분투 데스크톱에는 기본으로 깔려있으나 우분투 서버에서는 직접 다운 받아줘야 한다. 여러가지 포스팅을 찾아보다가 [wpa로 와이파이 연결] 보고 그대로 따라하며 와이파이 연결에 성공했다.\nwpa 실행\n1 sudo wpa_supplicant -c /etc/wpa_supplicant.conf -i wlp4s0 이 커맨드에서 \u0026amp;을 사용해서 백그라운드로 돌리려고 했는데 아무리 해도 백그라운드로 실행이 안 되어서 ctrl + alt + f1 ~f6 으로 전환할 수 있는 터미널 중 하나에서 저 커맨드를 실행하고 다른 작업들은 다른 터미널에서 진행했다.\n그런데 포스트를 쓰고 있는 지금 보니까 wpa_supplicant에서 백그라운드를 지원하는 커맨드가 따로 있었다. \u0026amp;로 직접 백그라운드로 돌리는게 아니었다. 아래와 같은 커맨드를 사용하면 된다.\nwpa 백그라운드 실행\n1 sudo wpa_supplicant -B -c /etc/wpa_supplicant.conf -i wlp4s0 고정 ip 설정하기 이더넷이나 와이파이를 연결해서 사용할때면 서버가 설치된 컴퓨터를 종료했다가 다시 키면 할당된 ip주소가 바뀔때가 있다. 얼마전에 회사에서 정전이 되어서 내부에서 사용하던 서버 컴퓨터 전원이 나갔다가 다시 들어왔는데 ssh로 접속이 안되어서 컴퓨터에 화면을 연결해서 직접 보니 IP 주소가 바뀌어있었다. 이런 불상사를 피하고 싶다면 고정 ip를 할당해주면 된다.\n매번 이더넷으로만 고정 ip를 설정했는데 와이파이도 큰 차이 없이 아래와 같이 하면 된다. 아래에 작성된 파일은 netplan에서 사용하는 yaml 파일인데 수정하고 적용하려면 꼭 netplan apply 커맨드를 입력해주자. 그리고 netplan을 사용하는 우분투 버전은 18 부터다.\n고정 ip를 위한 netplan 파일\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # This is the network config written by \u0026#39;subiquity\u0026#39; network: ethernets: enp7s0: dhcp4: true version: 2 wifis: wlp8s0: dhcp4: no dhcp6: no addresses: [ 172.30.1.100/24 ] gateway4: 172.30.1.254 nameservers: addresses: [ 8.8.8.8, 1.1.1.1 ] access-points: \u0026#34;KT_GiGA_2G_FO1F\u0026#34;: password: \u0026#34;wifi_password\u0026#34; SSH 설정 사실 서버 컴퓨터는 직접 작동을 시킬 일이 거의 없다. 대부분 ssh를 이용해서 외부에서 서버 컴퓨터 속의 서버에 접속을 한다. 이제 ssh 접속을 위한 세팅과 ssh 로 접속할 유저를 위한 세팅을 해보자.\nSSH server 설치\n1 2 3 4 5 6 # 설치 sudo apt-get install openssh-server # ssh port open sudo ufw enable sudo ufw allow 22 설정파일에 ssh_config와 sshd_config 두개가 있는데 대부분의 조작은 sshd_config에서 바꿔줘야한다. 리눅스 네임컨밴션을 생각해보면 sshd_config는 ssh가 데몬프로세스로 백그라운드 실행이 될때를 컨트롤 하는 파일이 아닐까?\n비밀번호로 SSH 접속 이번에는 서버 직접 설치라서 상관이 없는데 만약 aws와 같은 클라우드에서 서버를 만들어서 구동한다면 pem 키를 발급해준다. 문제는 이 pem키를 한 번 발급해주기 때문에 여러명이 해당 서버에 접속하려면 불편하다. 한명이 pem키로 서버에 접속을 하고 ssh를 설치해서 key 뿐만 아니라 비밀번호로도 접속을 할 수 있게 만들어줘야 한다.\nsshd_config 파일\n1 2 # 비밀번호 접속허용 PasswordAuthentication yes 사용자 설정 ssh를 설정 완료 했다면 ssh를 이용해서 로그인할 사용자와 그룹을 만들어주자. 앞으로 ssh를 사용해서 로그인을 하면 이 사용자가 되어 서버를 돌아다니기 때문에 꼭 sudoer까지 설정을 해줘야 한다. 접속할때 아예 해당 유저를 넣어서 접속을 한다. ssh user@address\n그룹과 사용자 생성 그룹, 사용자 생성\n1 2 3 4 5 6 7 8 9 10 11 # make group sudo addgroup \u0026#34;group_name\u0026#34; # make user sudo adduser \u0026#34;user_name\u0026#34; # put the user in the group gpasswd -a \u0026#34;user_name\u0026#34; \u0026#34;group_name\u0026#34; # check the user is now in the group getent group \u0026#34;group_name\u0026#34; 참고로 서버 안에 있는 그룹들과 유저들을 확인하기 위해서는 /etc에 있는 group과 passwd 파일을 직접 조회하는데 아래와 같은 커맨드를 쓰면 좀 더 깔끔하게 볼 수 있다.\n그룹, 사용자 조회\n1 2 3 4 5 # group list cut -d : -f1 /etc/group # user list cut -d : -f1 /etc/passwd 사용자에게 권한 부여해주기 딱 두가지 권한이 필요하다.\n서버에서 sudo 커맨드를 쓰기 위해 sudoer 권한이 필요하고, 소스코드나 실행파일등 개발한 것들을 /opt 파일에 두고 조작을 하기 때문에 소유가 root로 되어있는 opt디렉터리를 사용자로 바꿔줘야 한다. 안 바꾸면 매번 디렉터리 안에서 뭘 조작하려면 root 권한이 필요해서 sudo를 사용해야 한다.\netc 디렉터리의 sudoers 파일 수정을 통해 사용자에게 권한을 부여해줄 수 있다.\nsudoers 파일\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 sudo vi /etc/sudoers # User privilege specification root ALL=(ALL:ALL) ALL dev ALL=(ALL:ALL) ALL # 아래처럼 하면 sudo 실행시 비밀번호를 안 넣어도 된다. dev ALL=(ALL:ALL) NOPASSWD: ALL # 디렉터리 소유권 바꾸기 (사용자) sudo chown \u0026#34;user_name\u0026#34; opt # 디렉터리 소유권 바꾸기 (그룹) sudo chown :\u0026#34;group_name\u0026#34; opt 기타 설정 프롬프트 개인화 처음 서버를 파면 프롬프트가 보기가 다소 불편하다. 이것도 입맛에 맞게 바꿀 수 있는데 사용자 설정파일인 .bashrc안에 있는 PS1 부분을 수정해주면 된다. 사용자별로 설정이 되기 때문에 해당 사용자의 홈 디렉터리에 들어가서 수정을 해주도록 하자.\n.bashrc 파일\n1 2 3 4 5 6 7 8 9 10 11 vi ~/.bashrc IP=$( ifconfig | grep \u0026#39;inet addr:\u0026#39; | grep -v \u0026#39;127.0.0.1\u0026#39; | tail -1 | cut -d: -f2 | awk \u0026#39;{ print $1}\u0026#39;) if [ \u0026#34;$color_prompt\u0026#34; = yes ]; then PS1=\u0026#39;🎄HOME `date +%T` ${debian_chroot:+($debian_chroot)}\\[\\033[01;32m\\]\\u@$IP\\h\\[\\033[00m\\]:\\[\\033[01;34m\\]\\w\\[\\033[00m\\]\\$ \u0026#39; else PS1=\u0026#39;🎄HOME `date +%T` ${debian_chroot:+($debian_chroot)}\\u@$IP\\h:\\w\\$ \u0026#39; fi # apply bashrc source ~/.bashrc 간단하게 서버 이름 + 시간 + 유저이름이 보이도록 수정을 했는데 프롬프트는 무궁무진하게 수정이 가능하다. [프롬프트 테마] 를 보면 더 다양한 형태로 수정을 할 수 있다.\n시간대 변경 서버 시간은 UTC로 하는 것이 업계 표준이다. 앞으로는 꼭 준수하도록 하자.\n서버를 처음 시작하면 시간대가 기본 UTC로 설정이 되어있다. 글로벌 서비스를 하면 세계 기준시가 UTC라서 그대로 내버려둬도 되는데 개인이 한국에서 쓰는 서버라서 KST로 변경했다. 서버의 시간대는 date 를 찍어보면 맨 뒤에 어떤 시간대인지 나온다.\n시간대 변경\n1 2 3 4 5 # 어떤 시간대를 지원하나 먼저 확인 timedatectl list-timezones # 시간대를 서울로 변경 timedatectl set-timezone Asia/Seoul 서버 설치 후 설치할 것들 build-essential build-essential 패키지는 개발에 필요한 기본 라이브러리와 헤더파일들을 가지고 있는데 C 컴파일러 등등이 포함되어있다. 우분투에 설치되는 기본 패키지 상세 정보는 [우분투 패키지]에서 확인이 가능하다.\nbuild-essential 패키지 설치\n1 2 # install build-essential sudo apt-get install build-essential Java 개발을 자바로 하다보니까 서버를 설치하면 필수적으로 자바를 설치해야 한다. 서버에서 어떤 자바 버전들을 지원하는지 보고 원하는 버전을 설치해주면 된다.\n자바 설치\n1 2 3 4 5 # 설치 가능한 jdk 확인 sudo apt search jdk # 설치 sudo apt-get install \u0026#34;java_version\u0026#34; 이정도로 하면 초기에 기본으로 설정해줘야하는 세팅들이 어느정도 정리가 된다. 밑에는 서버 사용하면서 그때그때 생각나는 편의를 위한 세팅들을 기록할 예정이다.\nvi들어가면 줄번호 나오게 변경 기본 디렉토리의 .vimrc 를 수정해주면 된다.\n.vimrc 파일\n1 2 3 4 5 # open vim setting for user vi ~/.vimrc # add this in vimrc file set number 프롬프트 ip 주소 나오게 변경 vi ~/.bashrc 로 user별 설정 파일을 열어주고 PS1 있는 부분을 아래와 같이 수정해주면 된다. 그럼 user_name@|ip_address| 이런 형태로 프롬프트가 변한다.\n.bashrc 파일\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Set the prompt to include the IP address instead of hostname function get_ip () { IFACE=$(ip -4 route | grep default | head -n1 | awk \u0026#39;{print $5}\u0026#39;) if [ ! -z $IFACE ]; then echo -n \u0026#34;|\u0026#34;; ip -4 -o addr show scope global $IFACE | awk \u0026#39;{gsub(/\\/.*/, \u0026#34;|\u0026#34;,$4); print $4}\u0026#39; | paste -s -d \u0026#34;\u0026#34; else echo -n \u0026#34;||\u0026#34; fi } if [ \u0026#34;$color_prompt\u0026#34; = yes ]; then PS1=\u0026#39;${debian_chroot:+($debian_chroot)}\\[\\033[01;32m\\]\\u\\[\\033[01;34m\\]@\\[\\033[32m\\]$(get_ip)\\[\\033[00m\\]:\\[\\033[01;34m\\]\\w\\[\\033[00m\\]\\$ \u0026#39; else PS1=\u0026#39;${debian_chroot:+($debian_chroot)}\\u@$(get_ip):\\w\\$ \u0026#39; fi unset color_prompt force_color_prompt 우분투 기본 shell 설정 변경 가끔 도커를 쓰거나 새로 서버를 만들면 쉘에 아무것도 나오지 않고 $ 표시만 나오는 떄가 있다. 우분투에서 기본으로 사용하는 쉘이 c shell로 되어있기 때문이고, 기본 쉘을 bash shell로 바꿔주면 익숙한 형태의 터미널 쉘이 된다.\n사용자에 따라 기본으로 설정된 쉘이 다른데, 이는 /etc/passwd 파일에서 확인을 할 수 있다. 위의 이미지처럼 사용하는 계정을 찾아 /bin 뒤의 부분을 바꿔주면 된다. 나는 dev라는 계정을 쓰고 있고, bash shell로 바꾸고 싶었기 때문에 /bin/bash 로 수정해주었다. 수정을 해주고 로그인을 다시 해주면 터미널이 변경된 쉘로 나온다!\n[Picture 1] Bash MariDB 설치/설정 Maria DB 설치하기 개인 프로젝트에서 DB를 사용하게 되어서 우분투에 마리아 DB를 설치했는데 MySQL과 중첩이 되는 둥 몇 번 잘못 설치를 해서 서버를 다시 밀고 설치하기를 반복했다. 결국은 [우분투에 Maria DB 설치]보면서 무사히 DB 설치를 마쳤다.\nMaria DB 설치\n1 2 3 sudo apt update sudo apt install mariadb-server sudo mysql_secure_installation sudo mysql_secure_installation 을 실행하면 Maria DB에서 기본적으로 설정되는 보안 옵션을 조금 더 보완할 수 있게 옵션을 변경할 수 있다.\nmysql_secure_installation 설정 변경 프롬프트\n1 2 3 4 5 6 7 8 # localhost 에서만 access 가능한 계정 GRANT ALL ON *.* TO \u0026#39;admin\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;password\u0026#39; WITH GRANT OPTION; # 모든 IP에서 access가 가능하게 와일드카드를 부여한 계정 GRANT ALL ON *.* TO \u0026#39;admin\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;password\u0026#39; WITH GRANT OPTION; # mysql 계정과 관련된 작업 후에는 꼭 flush를 해서 refresh 를 해주자! FLUSH PRIVILEGES; DB를 조작할 때는 되도록 ROOT를 사용하지 말아야 하기 때문에 DB 설치를 진행하면서 default로 사용할 계정도 함께 만들어줬다. 이 계정을 localhost 에서만 사용을 한다면 여기서 끝이지만 remote 환경에서 해당 계정으로 접속을 가능하게 하려면 몇가지 설정을 더 해줘야 한다.\nMaria DB 리모트 접속 허용 원격 환경에서 DB에 접속하기 위해서는 아래의 3가지 사항을 설정해야 한다.\n리모트 접속 설정\nmysql 계정을 만들 때 와일드 카드 %를 넣던가 access IP를 추가해준다. mysql conf 파일에서 bind-access가 localhost로 되어있는데 0.0.0.0으로 변경 해 모두 허용해준다. (안된다면) 우분투에서도 mysql port인 3306 방화벽을 내려준다. 만약 aws와 같은 cloud 도 이용하고 있다면 cloud 의 설정도 변경해야 한다. mysql conf를 변경하고 난 후 mysql을 재시작해주면 이제 외부에서도 방금 만든 계정으로 DB에 접속할 수 있다.\nmysql 재시작\n1 sudo /etc/init.d/mysql restart DB 리모트 접속 허용에 대한 보안 이슈 mysql conf에서 bind-access 를 0.0.0.0으로 해두면 모든 remote IP에서 접속이 가능하기 때문에 보안상으로 문제가 있지는 않을까 찾아봤다.\nbind-access는 mysql이 들을 수 있는(listen) 특정 네트워크들을 뜻한다. 이렇게 0.0.0.0 으로 열어버리면 역시나 보안상으로 좋지 않다고 한다. default 설정처럼 그냥 로컬 머신에서만 DB와 연결하는 게 제일 안전하다.\nbind-access 설정에 따른 리모트 접속 차이\nIf MySQL binds to 127.0.0.1, then only software on the same computer will be able to connect (because 127.0.0.1 is always the local computer). If MySQL binds to 192.168.0.2 (and the server computer\u0026rsquo;s IP address is 192.168.0.2 and it\u0026rsquo;s on a /24 subnet), then any computers on the same subnet (anything that starts with 192.168.0) will be able to connect. If MySQL binds to 0.0.0.0, then any computer which is able to reach the server computer over the network will be able to connect. 보안 이슈를 해결하려면? 외부에서 접속을 해야 해서 bind-address를 활짝 열어버렸는데 보안을 조금이라도 강화하려면 mysql user 설정을 할 때 와일드 카드로 모든 IP에서 access 할 수 있게 할 게 아니라 특정 IP에서만 access 할 수 있게 해야 겠다.\n","date":"2021-04-21","permalink":"https://leeleelee3264.github.io/post/2021-04-21-linux-server-init-setting/","tags":["Infra"],"title":"리눅스 서버 설치와 초기 설정하기"},{"content":"\nTwitter API 와 Github Action을 사용하는 트위터 봇을 Python으로 구현한다.\n[github]\n[트위터 계정]\nIndex\n프로젝트 목표 프로젝트 구현 다음 프로젝트 목표 프로젝트 목표 프로젝트 진행 동기 예전부터 자동으로 응답을 해주는 카톡봇이나 자동으로 트윗을 해주는 트윗봇을 만드는 프로젝트를 하고 싶었는데 어떤걸 만들면 좋을지 몰라 미뤄두고 있었다. 그런데 얼마전에 다음에서 넷플릭스 상영 예정작을 알려주는 페이지를 만든걸 보고 일주일에 한 번씩 넷플릭스 상영 예정작을 트위터 자동봇으로 만들면 편할 것 같아 프로젝트를 진행했다.\nGithub Action?\n깃허브에서 CI/CD를 위해 사용되는 기능이다. Action을 등록할 때 파이썬을 지원해주고 있었다. 때문에 Action에 파이썬으로 작성한 스크립트를 등록하면 별도의 서버를 만들지 않고도 크롤링을 해서 트위터에 게시할 수 있기에 프로젝트에서 사용하기로 결정했다.\n개발 목표\n별도의 서버를 띄우지 않기 위해 github action을 트리거로 사용한다. python과 tweepy 라이브러리를 사용한다. Twitter 에서 개발자 계정을 발급받아 API를 사용한다. Dev Stack stack info Backend language python Backend api twitter api Server server less Scheduler github action 프로젝트 구현 Twitter Bot Flow 트위터 봇 플로우는 [Picture 1] 과 같다.\n[Picture 1] Twitter Bot Flow 리소스 프로젝트에서 활용할 리소스들을 정리했다.\n[Twitter API] [Python tweepy 라이브러리] [다음 넷플릭스 페이지] 트위터 api와 tweepy로 트윗하기 트위터 api 계정을 만들고 대시보드에 들어가면 총 네개의 인증키를 받을 수 있다. 현재 트위터 api에서는 Oauth1, 2와 기본인증법 여러개의 인증을 지원하기 때문에 사용할 인증법을 선택해주면 된다. 트위터 auth에 대한 [더 자세한 사항]을 확인해보자.\n발급되는 키\nApi key Api key secret Bearer Token Access Token Secret tweepy 라이브러리 인증 코드 트위터가 지원하는 인증 방법이 다양하기 때문에 tweepy도 이에 맞춰 다양한 지원을 한다. tweepy에서 제공하는 [인증 기능 문서] 에서 자세히 살필 수 있다.\nauth.py 더보기 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 # Tweepy # Copyright 2009-2020 Joshua Roesslein # See LICENSE for details. import logging import requests import six from requests.auth import AuthBase from requests_oauthlib import OAuth1, OAuth1Session from six.moves.urllib.parse import parse_qs from tweepy.api import API from tweepy.error import TweepError WARNING_MESSAGE = \u0026#34;\u0026#34;\u0026#34;Warning! Due to a Twitter API bug, signin_with_twitter and access_type don\u0026#39;t always play nice together. Details https://dev.twitter.com/discussions/21281\u0026#34;\u0026#34;\u0026#34; log = logging.getLogger(__name__) class AuthHandler(object): def apply_auth(self, url, method, headers, parameters): \u0026#34;\u0026#34;\u0026#34;Apply authentication headers to request\u0026#34;\u0026#34;\u0026#34; raise NotImplementedError def get_username(self): \u0026#34;\u0026#34;\u0026#34;Return the username of the authenticated user\u0026#34;\u0026#34;\u0026#34; raise NotImplementedError class OAuthHandler(AuthHandler): \u0026#34;\u0026#34;\u0026#34;OAuth authentication handler\u0026#34;\u0026#34;\u0026#34; OAUTH_HOST = \u0026#39;api.twitter.com\u0026#39; OAUTH_ROOT = \u0026#39;/oauth/\u0026#39; def __init__(self, consumer_key, consumer_secret, callback=None): if type(consumer_key) == six.text_type: consumer_key = consumer_key.encode(\u0026#39;ascii\u0026#39;) if type(consumer_secret) == six.text_type: consumer_secret = consumer_secret.encode(\u0026#39;ascii\u0026#39;) self.consumer_key = consumer_key self.consumer_secret = consumer_secret self.access_token = None self.access_token_secret = None self.callback = callback self.username = None self.request_token = {} self.oauth = OAuth1Session(consumer_key, client_secret=consumer_secret, callback_uri=self.callback) def _get_oauth_url(self, endpoint): return \u0026#39;https://\u0026#39; + self.OAUTH_HOST + self.OAUTH_ROOT + endpoint def apply_auth(self): return OAuth1(self.consumer_key, client_secret=self.consumer_secret, resource_owner_key=self.access_token, resource_owner_secret=self.access_token_secret, decoding=None) def _get_request_token(self, access_type=None): try: url = self._get_oauth_url(\u0026#39;request_token\u0026#39;) if access_type: url += \u0026#39;?x_auth_access_type=%s\u0026#39; % access_type return self.oauth.fetch_request_token(url) except Exception as e: raise TweepError(e) def set_access_token(self, key, secret): self.access_token = key self.access_token_secret = secret def get_authorization_url(self, signin_with_twitter=False, access_type=None): \u0026#34;\u0026#34;\u0026#34;Get the authorization URL to redirect the user\u0026#34;\u0026#34;\u0026#34; try: if signin_with_twitter: url = self._get_oauth_url(\u0026#39;authenticate\u0026#39;) if access_type: log.warning(WARNING_MESSAGE) else: url = self._get_oauth_url(\u0026#39;authorize\u0026#39;) self.request_token = self._get_request_token(access_type=access_type) return self.oauth.authorization_url(url) except Exception as e: raise TweepError(e) def get_access_token(self, verifier=None): \u0026#34;\u0026#34;\u0026#34; After user has authorized the request token, get access token with user supplied verifier. \u0026#34;\u0026#34;\u0026#34; try: url = self._get_oauth_url(\u0026#39;access_token\u0026#39;) self.oauth = OAuth1Session(self.consumer_key, client_secret=self.consumer_secret, resource_owner_key=self.request_token[\u0026#39;oauth_token\u0026#39;], resource_owner_secret=self.request_token[\u0026#39;oauth_token_secret\u0026#39;], verifier=verifier, callback_uri=self.callback) resp = self.oauth.fetch_access_token(url) self.access_token = resp[\u0026#39;oauth_token\u0026#39;] self.access_token_secret = resp[\u0026#39;oauth_token_secret\u0026#39;] return self.access_token, self.access_token_secret except Exception as e: raise TweepError(e) def get_xauth_access_token(self, username, password): \u0026#34;\u0026#34;\u0026#34; Get an access token from an username and password combination. In order to get this working you need to create an app at http://twitter.com/apps, after that send a mail to api@twitter.com and request activation of xAuth for it. \u0026#34;\u0026#34;\u0026#34; try: url = self._get_oauth_url(\u0026#39;access_token\u0026#39;) oauth = OAuth1(self.consumer_key, client_secret=self.consumer_secret) r = requests.post(url=url, auth=oauth, headers={\u0026#39;x_auth_mode\u0026#39;: \u0026#39;client_auth\u0026#39;, \u0026#39;x_auth_username\u0026#39;: username, \u0026#39;x_auth_password\u0026#39;: password}) credentials = parse_qs(r.content) return credentials.get(\u0026#39;oauth_token\u0026#39;)[0], credentials.get(\u0026#39;oauth_token_secret\u0026#39;)[0] except Exception as e: raise TweepError(e) def get_username(self): if self.username is None: api = API(self) user = api.verify_credentials() if user: self.username = user.screen_name else: raise TweepError(\u0026#39;Unable to get username,\u0026#39; \u0026#39; invalid oauth token!\u0026#39;) return self.username class OAuth2Bearer(AuthBase): def __init__(self, bearer_token): self.bearer_token = bearer_token def __call__(self, request): request.headers[\u0026#39;Authorization\u0026#39;] = \u0026#39;Bearer \u0026#39; + self.bearer_token return request class AppAuthHandler(AuthHandler): \u0026#34;\u0026#34;\u0026#34;Application-only authentication handler\u0026#34;\u0026#34;\u0026#34; OAUTH_HOST = \u0026#39;api.twitter.com\u0026#39; OAUTH_ROOT = \u0026#39;/oauth2/\u0026#39; def __init__(self, consumer_key, consumer_secret): self.consumer_key = consumer_key self.consumer_secret = consumer_secret self._bearer_token = \u0026#39;\u0026#39; resp = requests.post(self._get_oauth_url(\u0026#39;token\u0026#39;), auth=(self.consumer_key, self.consumer_secret), data={\u0026#39;grant_type\u0026#39;: \u0026#39;client_credentials\u0026#39;}) data = resp.json() if data.get(\u0026#39;token_type\u0026#39;) != \u0026#39;bearer\u0026#39;: raise TweepError(\u0026#39;Expected token_type to equal \u0026#34;bearer\u0026#34;, \u0026#39; \u0026#39;but got %s instead\u0026#39; % data.get(\u0026#39;token_type\u0026#39;)) self._bearer_token = data[\u0026#39;access_token\u0026#39;] def _get_oauth_url(self, endpoint): return \u0026#39;https://\u0026#39; + self.OAUTH_HOST + self.OAUTH_ROOT + endpoint def apply_auth(self): return OAuth2Bearer(self._bearer_token) oauth1_session1.py 더보기 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 from __future__ import unicode_literals try: from urlparse import urlparse except ImportError: from urllib.parse import urlparse import logging from oauthlib.common import add_params_to_uri from oauthlib.common import urldecode as _urldecode from oauthlib.oauth1 import SIGNATURE_HMAC, SIGNATURE_RSA, SIGNATURE_TYPE_AUTH_HEADER import requests from . import OAuth1 log = logging.getLogger(__name__) def urldecode(body): \u0026#34;\u0026#34;\u0026#34;Parse query or json to python dictionary\u0026#34;\u0026#34;\u0026#34; try: return _urldecode(body) except Exception: import json return json.loads(body) class TokenRequestDenied(ValueError): def __init__(self, message, response): super(TokenRequestDenied, self).__init__(message) self.response = response @property def status_code(self): \u0026#34;\u0026#34;\u0026#34;For backwards-compatibility purposes\u0026#34;\u0026#34;\u0026#34; return self.response.status_code class TokenMissing(ValueError): def __init__(self, message, response): super(TokenMissing, self).__init__(message) self.response = response class VerifierMissing(ValueError): pass class OAuth1Session(requests.Session): \u0026#34;\u0026#34;\u0026#34;Request signing and convenience methods for the oauth dance. What is the difference between OAuth1Session and OAuth1? OAuth1Session actually uses OAuth1 internally and its purpose is to assist in the OAuth workflow through convenience methods to prepare authorization URLs and parse the various token and redirection responses. It also provide rudimentary validation of responses. An example of the OAuth workflow using a basic CLI app and Twitter. \u0026gt;\u0026gt;\u0026gt; # Credentials obtained during the registration. \u0026gt;\u0026gt;\u0026gt; client_key = \u0026#39;client key\u0026#39; \u0026gt;\u0026gt;\u0026gt; client_secret = \u0026#39;secret\u0026#39; \u0026gt;\u0026gt;\u0026gt; callback_uri = \u0026#39;https://127.0.0.1/callback\u0026#39; \u0026gt;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; # Endpoints found in the OAuth provider API documentation \u0026gt;\u0026gt;\u0026gt; request_token_url = \u0026#39;https://api.twitter.com/oauth/request_token\u0026#39; \u0026gt;\u0026gt;\u0026gt; authorization_url = \u0026#39;https://api.twitter.com/oauth/authorize\u0026#39; \u0026gt;\u0026gt;\u0026gt; access_token_url = \u0026#39;https://api.twitter.com/oauth/access_token\u0026#39; \u0026gt;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; oauth_session = OAuth1Session(client_key,client_secret=client_secret, callback_uri=callback_uri) \u0026gt;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; # First step, fetch the request token. \u0026gt;\u0026gt;\u0026gt; oauth_session.fetch_request_token(request_token_url) { \u0026#39;oauth_token\u0026#39;: \u0026#39;kjerht2309u\u0026#39;, \u0026#39;oauth_token_secret\u0026#39;: \u0026#39;lsdajfh923874\u0026#39;, } \u0026gt;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; # Second step. Follow this link and authorize \u0026gt;\u0026gt;\u0026gt; oauth_session.authorization_url(authorization_url) \u0026#39;https://api.twitter.com/oauth/authorize?oauth_token=sdf0o9823sjdfsdf\u0026amp;oauth_callback=https%3A%2F%2F127.0.0.1%2Fcallback\u0026#39; \u0026gt;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; # Third step. Fetch the access token \u0026gt;\u0026gt;\u0026gt; redirect_response = raw_input(\u0026#39;Paste the full redirect URL here.\u0026#39;) \u0026gt;\u0026gt;\u0026gt; oauth_session.parse_authorization_response(redirect_response) { \u0026#39;oauth_token: \u0026#39;kjerht2309u\u0026#39;, \u0026#39;oauth_token_secret: \u0026#39;lsdajfh923874\u0026#39;, \u0026#39;oauth_verifier: \u0026#39;w34o8967345\u0026#39;, } \u0026gt;\u0026gt;\u0026gt; oauth_session.fetch_access_token(access_token_url) { \u0026#39;oauth_token\u0026#39;: \u0026#39;sdf0o9823sjdfsdf\u0026#39;, \u0026#39;oauth_token_secret\u0026#39;: \u0026#39;2kjshdfp92i34asdasd\u0026#39;, } \u0026gt;\u0026gt;\u0026gt; # Done. You can now make OAuth requests. \u0026gt;\u0026gt;\u0026gt; status_url = \u0026#39;http://api.twitter.com/1/statuses/update.json\u0026#39; \u0026gt;\u0026gt;\u0026gt; new_status = {\u0026#39;status\u0026#39;: \u0026#39;hello world!\u0026#39;} \u0026gt;\u0026gt;\u0026gt; oauth_session.post(status_url, data=new_status) \u0026lt;Response [200]\u0026gt; \u0026#34;\u0026#34;\u0026#34; def __init__( self, client_key, client_secret=None, resource_owner_key=None, resource_owner_secret=None, callback_uri=None, signature_method=SIGNATURE_HMAC, signature_type=SIGNATURE_TYPE_AUTH_HEADER, rsa_key=None, verifier=None, client_class=None, force_include_body=False, **kwargs ): \u0026#34;\u0026#34;\u0026#34;Construct the OAuth 1 session. :param client_key: A client specific identifier. :param client_secret: A client specific secret used to create HMAC and plaintext signatures. :param resource_owner_key: A resource owner key, also referred to as request token or access token depending on when in the workflow it is used. :param resource_owner_secret: A resource owner secret obtained with either a request or access token. Often referred to as token secret. :param callback_uri: The URL the user is redirect back to after authorization. :param signature_method: Signature methods determine how the OAuth signature is created. The three options are oauthlib.oauth1.SIGNATURE_HMAC (default), oauthlib.oauth1.SIGNATURE_RSA and oauthlib.oauth1.SIGNATURE_PLAIN. :param signature_type: Signature type decides where the OAuth parameters are added. Either in the Authorization header (default) or to the URL query parameters or the request body. Defined as oauthlib.oauth1.SIGNATURE_TYPE_AUTH_HEADER, oauthlib.oauth1.SIGNATURE_TYPE_QUERY and oauthlib.oauth1.SIGNATURE_TYPE_BODY respectively. :param rsa_key: The private RSA key as a string. Can only be used with signature_method=oauthlib.oauth1.SIGNATURE_RSA. :param verifier: A verifier string to prove authorization was granted. :param client_class: A subclass of `oauthlib.oauth1.Client` to use with `requests_oauthlib.OAuth1` instead of the default :param force_include_body: Always include the request body in the signature creation. :param **kwargs: Additional keyword arguments passed to `OAuth1` \u0026#34;\u0026#34;\u0026#34; super(OAuth1Session, self).__init__() self._client = OAuth1( client_key, client_secret=client_secret, resource_owner_key=resource_owner_key, resource_owner_secret=resource_owner_secret, callback_uri=callback_uri, signature_method=signature_method, signature_type=signature_type, rsa_key=rsa_key, verifier=verifier, client_class=client_class, force_include_body=force_include_body, **kwargs ) self.auth = self._client @property def token(self): oauth_token = self._client.client.resource_owner_key oauth_token_secret = self._client.client.resource_owner_secret oauth_verifier = self._client.client.verifier token_dict = {} if oauth_token: token_dict[\u0026#34;oauth_token\u0026#34;] = oauth_token if oauth_token_secret: token_dict[\u0026#34;oauth_token_secret\u0026#34;] = oauth_token_secret if oauth_verifier: token_dict[\u0026#34;oauth_verifier\u0026#34;] = oauth_verifier return token_dict @token.setter def token(self, value): self._populate_attributes(value) @property def authorized(self): \u0026#34;\u0026#34;\u0026#34;Boolean that indicates whether this session has an OAuth token or not. If `self.authorized` is True, you can reasonably expect OAuth-protected requests to the resource to succeed. If `self.authorized` is False, you need the user to go through the OAuth authentication dance before OAuth-protected requests to the resource will succeed. \u0026#34;\u0026#34;\u0026#34; if self._client.client.signature_method == SIGNATURE_RSA: # RSA only uses resource_owner_key return bool(self._client.client.resource_owner_key) else: # other methods of authentication use all three pieces return ( bool(self._client.client.client_secret) and bool(self._client.client.resource_owner_key) and bool(self._client.client.resource_owner_secret) ) def authorization_url(self, url, request_token=None, **kwargs): \u0026#34;\u0026#34;\u0026#34;Create an authorization URL by appending request_token and optional kwargs to url. This is the second step in the OAuth 1 workflow. The user should be redirected to this authorization URL, grant access to you, and then be redirected back to you. The redirection back can either be specified during client registration or by supplying a callback URI per request. :param url: The authorization endpoint URL. :param request_token: The previously obtained request token. :param kwargs: Optional parameters to append to the URL. :returns: The authorization URL with new parameters embedded. An example using a registered default callback URI. \u0026gt;\u0026gt;\u0026gt; request_token_url = \u0026#39;https://api.twitter.com/oauth/request_token\u0026#39; \u0026gt;\u0026gt;\u0026gt; authorization_url = \u0026#39;https://api.twitter.com/oauth/authorize\u0026#39; \u0026gt;\u0026gt;\u0026gt; oauth_session = OAuth1Session(\u0026#39;client-key\u0026#39;, client_secret=\u0026#39;secret\u0026#39;) \u0026gt;\u0026gt;\u0026gt; oauth_session.fetch_request_token(request_token_url) { \u0026#39;oauth_token\u0026#39;: \u0026#39;sdf0o9823sjdfsdf\u0026#39;, \u0026#39;oauth_token_secret\u0026#39;: \u0026#39;2kjshdfp92i34asdasd\u0026#39;, } \u0026gt;\u0026gt;\u0026gt; oauth_session.authorization_url(authorization_url) \u0026#39;https://api.twitter.com/oauth/authorize?oauth_token=sdf0o9823sjdfsdf\u0026#39; \u0026gt;\u0026gt;\u0026gt; oauth_session.authorization_url(authorization_url, foo=\u0026#39;bar\u0026#39;) \u0026#39;https://api.twitter.com/oauth/authorize?oauth_token=sdf0o9823sjdfsdf\u0026amp;foo=bar\u0026#39; An example using an explicit callback URI. \u0026gt;\u0026gt;\u0026gt; request_token_url = \u0026#39;https://api.twitter.com/oauth/request_token\u0026#39; \u0026gt;\u0026gt;\u0026gt; authorization_url = \u0026#39;https://api.twitter.com/oauth/authorize\u0026#39; \u0026gt;\u0026gt;\u0026gt; oauth_session = OAuth1Session(\u0026#39;client-key\u0026#39;, client_secret=\u0026#39;secret\u0026#39;, callback_uri=\u0026#39;https://127.0.0.1/callback\u0026#39;) \u0026gt;\u0026gt;\u0026gt; oauth_session.fetch_request_token(request_token_url) { \u0026#39;oauth_token\u0026#39;: \u0026#39;sdf0o9823sjdfsdf\u0026#39;, \u0026#39;oauth_token_secret\u0026#39;: \u0026#39;2kjshdfp92i34asdasd\u0026#39;, } \u0026gt;\u0026gt;\u0026gt; oauth_session.authorization_url(authorization_url) \u0026#39;https://api.twitter.com/oauth/authorize?oauth_token=sdf0o9823sjdfsdf\u0026amp;oauth_callback=https%3A%2F%2F127.0.0.1%2Fcallback\u0026#39; \u0026#34;\u0026#34;\u0026#34; kwargs[\u0026#34;oauth_token\u0026#34;] = request_token or self._client.client.resource_owner_key log.debug(\u0026#34;Adding parameters %s to url %s\u0026#34;, kwargs, url) return add_params_to_uri(url, kwargs.items()) def fetch_request_token(self, url, realm=None, **request_kwargs): r\u0026#34;\u0026#34;\u0026#34;Fetch a request token. This is the first step in the OAuth 1 workflow. A request token is obtained by making a signed post request to url. The token is then parsed from the application/x-www-form-urlencoded response and ready to be used to construct an authorization url. :param url: The request token endpoint URL. :param realm: A list of realms to request access to. :param \\*\\*request_kwargs: Optional arguments passed to \u0026#39;\u0026#39;post\u0026#39;\u0026#39; function in \u0026#39;\u0026#39;requests.Session\u0026#39;\u0026#39; :returns: The response in dict format. Note that a previously set callback_uri will be reset for your convenience, or else signature creation will be incorrect on consecutive requests. \u0026gt;\u0026gt;\u0026gt; request_token_url = \u0026#39;https://api.twitter.com/oauth/request_token\u0026#39; \u0026gt;\u0026gt;\u0026gt; oauth_session = OAuth1Session(\u0026#39;client-key\u0026#39;, client_secret=\u0026#39;secret\u0026#39;) \u0026gt;\u0026gt;\u0026gt; oauth_session.fetch_request_token(request_token_url) { \u0026#39;oauth_token\u0026#39;: \u0026#39;sdf0o9823sjdfsdf\u0026#39;, \u0026#39;oauth_token_secret\u0026#39;: \u0026#39;2kjshdfp92i34asdasd\u0026#39;, } \u0026#34;\u0026#34;\u0026#34; self._client.client.realm = \u0026#34; \u0026#34;.join(realm) if realm else None token = self._fetch_token(url, **request_kwargs) log.debug(\u0026#34;Resetting callback_uri and realm (not needed in next phase).\u0026#34;) self._client.client.callback_uri = None self._client.client.realm = None return token def fetch_access_token(self, url, verifier=None, **request_kwargs): \u0026#34;\u0026#34;\u0026#34;Fetch an access token. This is the final step in the OAuth 1 workflow. An access token is obtained using all previously obtained credentials, including the verifier from the authorization step. Note that a previously set verifier will be reset for your convenience, or else signature creation will be incorrect on consecutive requests. \u0026gt;\u0026gt;\u0026gt; access_token_url = \u0026#39;https://api.twitter.com/oauth/access_token\u0026#39; \u0026gt;\u0026gt;\u0026gt; redirect_response = \u0026#39;https://127.0.0.1/callback?oauth_token=kjerht2309uf\u0026amp;oauth_token_secret=lsdajfh923874\u0026amp;oauth_verifier=w34o8967345\u0026#39; \u0026gt;\u0026gt;\u0026gt; oauth_session = OAuth1Session(\u0026#39;client-key\u0026#39;, client_secret=\u0026#39;secret\u0026#39;) \u0026gt;\u0026gt;\u0026gt; oauth_session.parse_authorization_response(redirect_response) { \u0026#39;oauth_token: \u0026#39;kjerht2309u\u0026#39;, \u0026#39;oauth_token_secret: \u0026#39;lsdajfh923874\u0026#39;, \u0026#39;oauth_verifier: \u0026#39;w34o8967345\u0026#39;, } \u0026gt;\u0026gt;\u0026gt; oauth_session.fetch_access_token(access_token_url) { \u0026#39;oauth_token\u0026#39;: \u0026#39;sdf0o9823sjdfsdf\u0026#39;, \u0026#39;oauth_token_secret\u0026#39;: \u0026#39;2kjshdfp92i34asdasd\u0026#39;, } \u0026#34;\u0026#34;\u0026#34; if verifier: self._client.client.verifier = verifier if not getattr(self._client.client, \u0026#34;verifier\u0026#34;, None): raise VerifierMissing(\u0026#34;No client verifier has been set.\u0026#34;) token = self._fetch_token(url, **request_kwargs) log.debug(\u0026#34;Resetting verifier attribute, should not be used anymore.\u0026#34;) self._client.client.verifier = None return token def parse_authorization_response(self, url): \u0026#34;\u0026#34;\u0026#34;Extract parameters from the post authorization redirect response URL. :param url: The full URL that resulted from the user being redirected back from the OAuth provider to you, the client. :returns: A dict of parameters extracted from the URL. \u0026gt;\u0026gt;\u0026gt; redirect_response = \u0026#39;https://127.0.0.1/callback?oauth_token=kjerht2309uf\u0026amp;oauth_token_secret=lsdajfh923874\u0026amp;oauth_verifier=w34o8967345\u0026#39; \u0026gt;\u0026gt;\u0026gt; oauth_session = OAuth1Session(\u0026#39;client-key\u0026#39;, client_secret=\u0026#39;secret\u0026#39;) \u0026gt;\u0026gt;\u0026gt; oauth_session.parse_authorization_response(redirect_response) { \u0026#39;oauth_token: \u0026#39;kjerht2309u\u0026#39;, \u0026#39;oauth_token_secret: \u0026#39;lsdajfh923874\u0026#39;, \u0026#39;oauth_verifier: \u0026#39;w34o8967345\u0026#39;, } \u0026#34;\u0026#34;\u0026#34; log.debug(\u0026#34;Parsing token from query part of url %s\u0026#34;, url) token = dict(urldecode(urlparse(url).query)) log.debug(\u0026#34;Updating internal client token attribute.\u0026#34;) self._populate_attributes(token) self.token = token return token def _populate_attributes(self, token): if \u0026#34;oauth_token\u0026#34; in token: self._client.client.resource_owner_key = token[\u0026#34;oauth_token\u0026#34;] else: raise TokenMissing( \u0026#34;Response does not contain a token: {resp}\u0026#34;.format(resp=token), token ) if \u0026#34;oauth_token_secret\u0026#34; in token: self._client.client.resource_owner_secret = token[\u0026#34;oauth_token_secret\u0026#34;] if \u0026#34;oauth_verifier\u0026#34; in token: self._client.client.verifier = token[\u0026#34;oauth_verifier\u0026#34;] def _fetch_token(self, url, **request_kwargs): log.debug(\u0026#34;Fetching token from %s using client %s\u0026#34;, url, self._client.client) r = self.post(url, **request_kwargs) if r.status_code \u0026gt;= 400: error = \u0026#34;Token request failed with code %s, response was \u0026#39;%s\u0026#39;.\u0026#34; raise TokenRequestDenied(error % (r.status_code, r.text), r) log.debug(\u0026#39;Decoding token from response \u0026#34;%s\u0026#34;\u0026#39;, r.text) try: token = dict(urldecode(r.text.strip())) except ValueError as e: error = ( \u0026#34;Unable to decode token from token response. \u0026#34; \u0026#34;This is commonly caused by an unsuccessful request where\u0026#34; \u0026#34; a non urlencoded error message is returned. \u0026#34; \u0026#34;The decoding error was %s\u0026#34; \u0026#34;\u0026#34; % e ) raise ValueError(error) log.debug(\u0026#34;Obtained token %s\u0026#34;, token) log.debug(\u0026#34;Updating internal client attributes from token data.\u0026#34;) self._populate_attributes(token) self.token = token return token def rebuild_auth(self, prepared_request, response): \u0026#34;\u0026#34;\u0026#34; When being redirected we should always strip Authorization header, since nonce may not be reused as per OAuth spec. \u0026#34;\u0026#34;\u0026#34; if \u0026#34;Authorization\u0026#34; in prepared_request.headers: # If we get redirected to a new host, we should strip out # any authentication headers. prepared_request.headers.pop(\u0026#34;Authorization\u0026#34;, True) prepared_request.prepare_auth(self.auth) return tweepy를 사용한 구현체 tweepy 라이브러리를 사용해서 실제로 구현을 진행했다.\n트윗하기 구현체\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def OAuth(): try: api_key = os.environ.get(\u0026#39;TWITTER_API_KEY\u0026#39;) api_key_secret = os.environ.get(\u0026#39;TWITTER_API_SECRET\u0026#39;) access_token = os.environ.get(\u0026#39;TWITTER_ACCESS_TOKEN\u0026#39;) access_token_secret = os.environ.get(\u0026#39;TWITTER_ACCESS_TOKEN_SECRET\u0026#39;) auth = tweepy.OAuthHandler(api_key, api_key_secret) auth.set_access_token(access_token, access_token_secret) return auth except Exception as e: return None def post_tweet(container: dict, date): print(\u0026#39;work calling\u0026#39;) oauth = OAuth() api = tweepy.API(oauth) _current_dir = os.path.dirname(os.path.abspath(__file__)) _path = Path(_current_dir) BASE_DIR = _path.parent.absolute() IMG_DIR = f\u0026#39;{BASE_DIR}/img/netflix/{date}\u0026#39; print(f\u0026#39;{IMG_DIR}\u0026#39;) for key in container: tTitle = key tFile = f\u0026#39;{IMG_DIR}/{tTitle}.png\u0026#39; print(f\u0026#39;{tFile}\u0026#39;) reTitle = regex.change_hyphen(tTitle) tweet_format = f\u0026#39;[{reTitle}]\\n 공개 여정일:{container[key]}\u0026#39; api.update_with_media(tFile, status=tweet_format) Github Action Repository Secret 실제 사용 코드를 보면 TWITTER_API_KEY, TWITTER_API_SECRET 등 environ 을 이용해 환경에서 받아온 변수들이 있다. 트위터 api를 사용하려면 여러개의 인증키가 필요한데 코드를 프라이빗 repo로 올려도 되지만 Repository Secret 으로 인증키들을 등록하는 방법을 선택했다.\n[깃허브 공식 가이드]에 secret을 등록하는 방법이 나와있다. 따라해보다 보면 사진처럼 repo에 노출하지 않아도 사용할 수 있는 secret key들이 만들어진다.\n[Picture 2] Repository Secret Repository Secret 사용하기 사용 시나리오\n액션을 등록할 때 secret을 넘겨준다. 실행시에 코드로 환경변수에 접근해서 코드 단에서 환경을 통해 받아온다. 자바에서 main 실행할 때 args들을 통해 환경값을 넘기는 것 파이썬에서 environ를 통해 환경값을 넘기는 것 스택 오버 플로우에 있는 [github action에서 secret을 넘겨 python으로 받아오기]를 레퍼런스로 코드를 만들었다. 중요한 점은 secret으로 등록한 키들을 파이썬을 실행하는 스크립트를 실행한 후에 던져주는 것이었다.\nGithub Action Workflow 깃허브에서는 액션을 등록하기 위해서는 yml로 스크립트를 작성해야 한다. 그 스크립트 안에는 한 작업이 아니라 여러가지 작업이 하나씩 순서대로 처리가 되는데 그래서 액션을 등록하는 파일 이름을 workflow라고 하는 게 아닐까?\n액션을 성공적으로 실행시키기 위해서는 이 workflow 스크립트를 잘 짜는 것이 제일 중요하다. 또 액션이 update (commit) 마다, 스케쥴링에 따라 실행이 되기 때문에 결과를 확인하기 위해서는 짧아도 1~2분 정도는 기다려야 해서 여러번 수정하다보면 많은 시간을 소비하게 된다.\nworkflow 시나리오\n파이썬 실행을 위해 플라스크와 requirements.txt 속의 라이브러리들을 설치 상영 예정작 정보가 있는 사이트 크롤링 크롤링한 이미지들을 저장하기 위한 commit/push 실제로 트윗을 하는 파이썬 코드 실행 트위터 봇을 위한 workflow.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 # This workflow will install Python dependencies, run tests and lint with a single version of Python # For more information see: https://help.github.com/actions/language-and-framework-guides/using-python-with-github-actions name: Netflix_Crawl on: schedule: - cron: \u0026#39;0 0 * * Sat\u0026#39; push: branches: [ master ] pull_request: branches: [ master ] jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - name: Set up Python 3.9 uses: actions/setup-python@v2 with: python-version: 3.9 - name: Install dependencies run: | python -m pip install --upgrade pip pip install flake8 pytest if [ -f requirements.txt ]; then pip install -r requirements.txt; fi - name: Lint with flake8 run: | # stop the build if there are Python syntax errors or undefined names flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics - name: Run netflix crawler with python run: | python3 \u0026#34;./crawl/netflix.py\u0026#34; - name: Commits run: | git config --local user.email \u0026#34;absinthe4902@naver.com\u0026#34; git config --local user.name \u0026#34;AUTO_ADD_GIT_ACTION\u0026#34; git add . git commit -m \u0026#34;AUTO ADD: commit downloaded image\u0026#34; - name: Push uses: ad-m/github-push-action@master with: branch: \u0026#39;master\u0026#39; github_token: $ - name: Tweet for final run: | python3 \u0026#34;./crawl/tweeting.py\u0026#34; env: TWITTER_API_KEY: ${{ secrets.TWITTER_API_KEY }} TWITTER_API_SECRET: ${{ secrets.TWITTER_API_SECRET }} TWITTER_ACCESS_TOKEN: ${{ secrets.TWITTER_ACCESS_TOKEN }} TWITTER_ACCESS_TOKEN_SECRET: ${{ secrets.TWITTER_ACCESS_TOKEN_SECRET }} 매주 토요일 00:00시에 스케쥴러가 돌아가도록 작성했는데 시간대의 기준은 UTC이다.\n삽질 구간\n아까 스택오버플로우에서 말한 것처럼 환경변수는 파이썬 실행 후 넘겨주고 있다. 제일 헤매던 부분은 프로젝트에서 파이썬 코드가 있는 디렉터리를 찾는 부분이었다. 예를 들어 crawl 디렉터리 안에 있는 netflix.py를 실행할 때 동일한 디렉터리 안에 있는 유틸성 파일 regex.py를 찾지 못했다.\n액션은 repo를 기준으로 동작하기 때문에 current dir가 repo였던 twitter_project였을텐데 무슨 이유로 다른 파일들을 찾지 못했는지 모르겠다. 여러번의 수정을 거친 다음에 ./crawl/netflix.py 로 실행을 하니 정상적으로 작동을 했다.\n실제 동작 화면 [Picture 3] 트위터 계정 [Picture 4] 트윗 화면 1 [Picture 5] 트윗 화면 2 다음 프로젝트 목표 똑같이 twitter api를 사용 현재 한국에서 공연 하고 있는 연극과 뮤지컬을 알려주는 자동봇 tweepy를 사용하지 않고 직접 구현하기 최소한 인증을 받는 부분이라도 구현하기 (oauth 공부) 동일하게 server-less로 git action 사용 Update in 2022 위에서 기획했던 트위터 봇을 만들었다.\n[지금 공연중인 뮤지컬 알림봇] [오늘의 뮤지컬 스케줄] 프로젝트 구조 [Picture 6] 프로젝트 구조 Dev Stack stack info Backend language python Backend api twitter api Server Ubuntu 20 Scheduler Linux cron job ","date":"2021-04-16","permalink":"https://leeleelee3264.github.io/post/2021-04-16-twitterbot-with-git-action/","tags":["Project"],"title":"Github action으로 넷플릭스 상영 알림 트위터 봇 만들기"},{"content":"\nShell Script로 무중단 서버 배포하는 방법을 다룬다.\nIndex\n무중단 배포란? 무중단 배포 구현하기 결론 무중단 배포란? 무중단 배포란 업데이트를 위해 배포를 할 때 어플리케이션이 멈추지 않는 것이다. 즉 배포를 할 때 서비스가 중단되는 다운타임이 발생하지 않는다.\n무중단 배포 방식\nBlue Green Deployment Rolling Deployment Canary Deployment Blue Green Deployment Blue와 Green 이라는 동일한 배포 환경을 준비한다. Green 은 live 되고 있는 환경을 의미한다. Blue는 새로운 버전을 가지고 있는 환경을 의미한다. Blue에서 새로운 버전을 테스트하고, 테스트 중에는 로드밸런서기 Green에 리퀘스트를 보낸다. 테스트가 완료되면 Blue로 리퀘스트를 보낸다. 만약 문제가 발생한다면 Green으로 롤백한다. 장점 롤백 하기가 쉽다. 다운타임이 없다. 단점 리소스가 두 배로 든다. 이는 두 배의 비용으로 이어질 수 있다. 두 개의 환경을 up-to-date 상태로 싱크하기 번거롭다. [Picture 1] Blue Green Deployment Rolling Deployment 점진적으로 running 중인 인스턴스를 새로운 버전으로 교체한다. 기존 버전과 새로운 버전이 함께 live 된다. 최소 N+1 개의 인스턴스가 필요하고, 이 한 개의 추가 인스턴스는 새로운 버전을 실행할 노드이다. 새로운 버전을 담은 인스턴스가 무사히 배포되면 다른 인스턴스들을 돌아가면서 새로운 버전으로 배포한다. 모든 인스턴스가 새로운 버전이 될 때 까지 반복한다. 쿠버네티스에서 사용하는 default 배포방식이다. 장점 기존 버전이 live 되어 있어, 최소한의 다운 타임만 존재한다. 최소 하나의 추가 인스턴스만 있으면 가능한 배포 방식이다. 반면 Blue Green은 하나의 infrastructure 가 있어야 한다. 롤백을 해야 한다면 기존 버전으로 트레픽을 돌리는 것이 가능하다. 단점 인스턴스의 개수에 따라 배포 시작과 live 사이에 상당한 지연이 있을 수 있다. 배포가 중간에 실패하면 롤백하는 것에 많은 시간이 소요될 수 있다. [Picture 2] Rolling Deployment Canary Deployment 서버 극히 일부를 새로운 버전으로 배포하여 지정된 유저들에게 서비스한다. 퍼포먼스, 이슈 등을 테스트와 모니터링하기에 좋은 배포 방식이다. 테스트를 할 유저들을 먼저 지정해야 한다. (베타 유저 등) 서비스의 앞단에 있는 로드밸런서, API 게이트웨이, 서비스 프록시등을 재설정하거나 feature 플래그를 두는 방식으로 구현할 수 있다. 문제가 없다면 기존 버전을 모두 새로운 버전으로 릴리즈한다. 장점 일부의 실 사용자들에게 테스트를 할 수 있다. 실패하더라도 일부의 사용자들에게만 영향이 미친다. feature flag를 사용한다면 최소한의 infrastructure 가 필요하다. 롤백이 간단하고 빠르다. 단점 일부의 유저들에게만 새 버전을 라우트 하는 것이 기술적으로 어렵다. feature flag는 cost-effective 한 방식이다. 옵저빌리티와 메트릭스, 분석 환경이 특히나 미리 잘 갖춰있어야 한다. [Picture 3] Canary Deployment 무중단 배포 구현하기 구현 환경 백업 서버를 따로 두지 않아서 맨날 서버를 업데이트할 때 서비스가 죽는다. 사내에서 소규모로 사용되는 서비스이지만 업데이트가 좀 잦은 편인데 계속 배포를 할 때 마다 서비스가 죽어버리면 사용하기가 힘들 것 같아 무중단 배포를 구현해보리고 했다.\n그런데 EC2 리소스가 좀 타이트 해서 평상시에 서버 두대를 띄우기는 힘들어 보였다. EC2 안에 앱서버가 두 대 뜨면 로드밸런스에서 하나가 업데이트 하느라 죽어도 다른 쪽으로 보내줄텐데 이런 형태는 지금 상황에서는 불가능했다. 그래서 평소에는 앱서버를 한 대 만 띄워두고 업데이트를 할 때 만 한 대 를 더 띄워 무중단 배포를 하기로 했다.\n구현할 무중단 배포 시나리오 업데이트 이전 상태 로드 밸런서인 nginx는 계속 두 포트(A: 4000, B: 4001) 모두를 바라본다. nginx가 날려준 리퀘스트를 실제로 처리하는 머신에서는 하나의 서버 A만 운영이 되고 있고, 서버 B는 다운 상태이다. 업데이트 진행 업데이트시에는 A보다 이전 버전인 B를 업데이트 된다. 서버 B를 가동한다. B가 완전히 뜨기 전까지 nginx는 A,B 둘 다 바라보니 리퀘스트 처리가 가능한 A가 받아서 처리한다 B서버가 올라간 걸 확인하면 A 서버를 죽인다. 업데이트 후 그럼 이제 요청이 띄워져있는 서버 인 B로 올라간다. 다음번에 업데이트를 해야 한다면 B보다 옛날 버전인 A가 업데이트 된다. 배포 시나리오 플로우 차트 플로우 차트 더보기 [Picture 4] 무중단 배포 시나리오 flow chart Nginx 작업을 하면서 nginx 는 어떻게 로드밸런싱을 할까 궁금해서 찾아봤다. 내가 하는 것처럼 앱서버를 두 개 두면 nginx는 필수적으로 요청을 분산하게 되어있다. [nginx로-로드밸런싱-하기]\n그리고 이 작업을 하다가 알게 되었는데 항상 nginx에서 설정이 바뀌면 nginx 서비스를 재시작(restart) 했는데 이것보다는 새로고침(reload) 하는게 좋다고 한다. restart는 말 그대로 재시작이라서 nginx를 한번 shutdown 하고 다시 시작한다. 반면 reload는 서버가 돌아가는 상태에서 설정 파일만 다시 불러와서 적용을 해준다. 서버 중단이 없다는 것 때문에 reload가 더 적합하다!\nNginx reload, restart 커맨드\n1 2 3 4 5 6 7 8 // reload, restart 등을 할 때 config 파일이 문법에 맞는지 꼭 먼저 점검을 한다. nginx -t // reload sudo systemctl reload nginx // restart sudo systemctl restart nginx 목표로 하고 있는 client - 로드 밸런서 - 앱서버 까지의 모식도를 그려봤다.\n[Picture 5] Nginx flow 무중단 배포 구현하기 무중단 배포를 위한 디렉터리 구조\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 tc-admin ├── blue_green_deploy.sh ├── blue_green_deploy.txt ├── dnx-touchcare-admin-0.1-SNAPSHOT.jar ├── gc.log ├── log_rotate.sh ├── server.log └── start.sh tc-admin-prod ├── blue_green_deploy.sh ├── dnx-touchcare-admin-0.1-SNAPSHOT.jar ├── gc.log ├── hs_err_pid29917.log ├── hs_err_pid29917.txt ├── log_rotate.sh ├── server.log └── start.sh backup-tc-admin ├── api │ └── dnx-touchcare-admin-0.1-SNAPSHOT.jar └── tpi └── dnx-touchcare-admin-0.1-SNAPSHOT.jar 배포 스크립트\n이 스크립트에서 사용하는 환경은 tpi로, tc-admin을 이용한다. 백업 jar가 올라가는 곳은 backup-tc-admin 으로 tpi/api 로 디렉터리를 분리했다.\nprofile 은 jar 를 실행하는 환경의 정보인데 prod와 백업용 back-prod를 두었고 포트는 각각 4000 과 4001로 할당했다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 #! /bin/sh echo \u0026#34;\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#34;++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\u0026#34; echo \u0026#34;Blue Green Deployment\u0026#34; echo \u0026#34;Author: Seungmin Lee\u0026#34; echo \u0026#34;Date: 2021-02-26\u0026#34; echo \u0026#34;This script is only for blue green deploy, not for back up server.\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#34;++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#34;\u0026#34; # have to change for api and tpi SETTING=tpi PROFILE=prod PROFILE_BACKUP=back-prod PORT=4000 PORT_BACKUP=4001 CURRENT_PATH=/opt/tc-admin BACKUP_PATH=/opt/backup-tc-admin/${SETTING} SUB_URL=/tcadmin/api/test/profile CHECK_URL=https://${SETTING}.example.kr${SUB_URL} echo \u0026#34;\u0026gt;Check which profile is active...\u0026#34; PROFILE_CURRENT=$(curl -s $CHECK_URL) echo \u0026#34;\u0026gt;$PROFILE_CURRENT\u0026#34; if [ $PROFILE_CURRENT = $PROFILE ] then IDLE_PROFILE=$PROFILE_BACKUP IDLE_PORT=$PORT_BACKUP IDLE_DIR=$BACKUP_PATH elif [ $PROFILE_CURRENT = $PROFILE_BACKUP ] then IDLE_PROFILE=$PROFILE IDLE_PORT=$PORT IDLE_DIR=$CURRENT_PATH else echo \u0026#34;\u0026gt;Current profile is not matching with any of them...\u0026#34; echo \u0026#34;\u0026gt;Will use default $PROFILE for profile\u0026#34; IDLE_PROFILE=$PROFILE IDLE_PORT=$PORT IDLE_DIR=$CURRENT_PATH fi # update idle one and run # planed to seperate starting shell, but I have to copy/run first and kill the current one echo \u0026#34;\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#34;++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\u0026#34; echo \u0026#34;Update JAR file to new one and Run\u0026#34; echo \u0026#34;target dir is ${IDLE_DIR}\u0026#34; echo \u0026#34;target property is ${IDLE_PROFILE}\u0026#34; echo \u0026#34;target port is ${IDLE_PORT}\u0026#34; echo \u0026#34;++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#34;\u0026#34; cp -f ~/admin-0.1-SNAPSHOT.jar ${IDLE_DIR}/. echo \u0026#34;\u0026#34; echo \u0026#34;+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\u0026#34; echo \u0026#34;Now Deploy New Server....\u0026#34; echo \u0026#34;+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#34;\u0026#34; DATE=`date +\u0026#39;%Y%m%d\u0026#39;` ETC_JAVA_OPTS=-XX:+UseStringDeduplication nohup java -Xms128m -Xmx128m -XX:NewRatio=1 -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:./gc.log -Dspring.profiles.active=${IDLE_PROFILE} $* -jar ${IDLE_DIR}/dnx-touchcare-admin-0.1-SNAPSHOT.jar \u0026gt;\u0026gt; ./server.log \u0026amp; UP_CHECK=\u0026#34;\u0026#34; while [ -z \u0026#34;$UP_CHECK\u0026#34; ] do UP_CHECK=$(curl -s http://localhost:${IDLE_PORT}/tcadmin/api/test/profile) done echo \u0026#34;\u0026#34; echo \u0026#34;Now killed old server...\u0026#34; echo \u0026#34;\u0026#34; KILL_PROCESS_PID=$(pgrep -f \u0026#34;profiles.active=${PROFILE_CURRENT} -jar\u0026#34;) echo \u0026#34;\u0026gt; kill process ${KILL_PROCESS_PID}\u0026#34; if [ -z $KILL_PROCESS_PID ] then echo \u0026#34;\u0026gt;There are no pid...\u0026#34; else echo \u0026#34;\u0026gt;kill -9 $KILL_PROCESS_PID\u0026#34; kill -9 ${KILL_PROCESS_PID} fi tail -f server.log 스크립트 상세 초반에 환경과 프로파일들, 포트와 jar가 있는 path들을 정해준다. curl 을 이용해서 현재 띄워져있는 서버의 프로파일을 알아낸다. 이때는 도메인을 이용해서 외부에서 서버로 요청을 보내는 것처럼 동작한다. 서버의 profile에 따라서 업데이트를 할 Idle 프로파일과 포트, 패스를 지정한다. 현재 프로파일이 prod면 back-prod로, back-prod면 prod 로 Idle이 설정된다. Idle profile 로 설정된 프로파일이 업데이트가 되면 해당 jar 파일을 띄운다. prod와 back-prod 2개의 서버가 떠있는 상태이다. curl 을 이용해서 Idle jar 가 완전히 구동하고 있다는 응답을 받을 때까지 요청을 보낸다. 이때는 localhost 와 Idle port 를 이용해서 내부에서 서버로 요청을 보낸다. 확인이 되면 이전버전으로 돌아가고 있는 jar의 프로세스를 종료시킨다. 앱서버가 완전히 뜬 것을 확인하는 방법 5을 구현하기 위해 많은 고민을 했다. 어떻게 해야 단순히 서버가 뜨는 것이 아닌 서비스 요청을 받아서 처리하는 수준의 완전히 구동된 서버의 상태를 알 수 있을까?\n앱서버 구동 확인 시도들\n1 2 3 4 5 6 7 8 9 10 11 // 1 try : 임의의 sleep 을 주기 sleep(10000) // 2 try : 해당 프로세스로 확인하기 pgrep -f \u0026#34;pfofiles.active=${IDLE_PROFLE} -jar\u0026#34; // 3 try : 해당 포트로 확인하기 ss -tlp | grep ${IDLE_PORT} // 4 try : 내부로 요청을 보내기 curl http://localhost/{IDLE_PORT} 1 try : 임의의 sleep 을 주기 처음에는 서버가 완전히 뜰 수 있게 임의의 sleep 값을 주었는데 제일 나쁜 방법이었다. 서버가 뜨는 속도가 일정하지 않기 때문에 sleep을 초과하고도 뜨지 않아서 서비스가 멈출 수 있었다.\n2 try : 해당 프로세스로 확인하기 두번째로는 프로세스가 떠있는 것으로 확인을 했는데 프로세스는 jar를 실행하자마자 뜨는 것이기 때문에 서버가 완전히 뜨는 것보다 훨씬 더 빠른 시점이다. 프로세스가 떠있다고 과거의 프로세스를 죽인다면 실상 서버는 완전히 뜬게 아니라서 서비스가 멈춘다.\n3 try : 해당 포트로 확인하기 세번째로는 포트가 열려있는 것으로 확인을 했다. jar를 실행시키자마자 생기는 프로세스보다는 포트가 뒤늦게 열리지만 포트 또한 서버가 완전히 뜨는 여부와는 상관이 없이 일찍 열리기 때문에, 포트만 믿고 과거의 프로세스를 죽이면 서비스가 멈춘다.\n4 try : 내부로 요청을 보내기 어쩌지 고민을 하다가 외부로 요청을 보내면 이전에 떠있는 A서버가 처리를 했는지 B서버가 처리를 했는지 몰라서 무용지물이었다.\n그런데 알고보니 curl은 외부에 있는 nginx를 거쳐서 내부의 서버에 요청을 하는 것 뿐만 아니라 localhost와 port 로 내부에서 내부의 서버에 요청이 가능했다!! 이 방법으로\n(1) 내가 원하는 포트로\n(2) 응답이 올때까지 요청을 보내고\n(3) 응답이 오면 이전의 서버를 죽일 수 있었다.\n결론 이런식으로 쉘스크립트를 이용해서 업데이트 배포를 하면서 서비스는 죽지 않게 무중단으로 운영될 수 있는 환경을 만들었다. 한국에서는 무중단 배포라고 하지만 영어로는 blue green deploy 라고 한다.\n그리고 팀장님이 spring의 context event를 써보면 어떻겠냐고 하셨는데 무중단 배포를 위한 기능은 아닌 것 처럼 보였다. 그래도 spring event가 굉장히 유용한 기능이기에 링크를 남겨 나중에 보기로 했다.\n[Spring Event] Update in 2022 해당 포스팅은 AWS ec2를 하나를 사용하고 그 안에서 앱 서버를 여러 개 띄워서 무중단 배포를 하는 형식이었다.\n하지만 ec2를 하나만 사용하는 경우는 거의 없고, AWS EKS를 사용하여 오케스트레이션을 사용하여 무중단 배포를 하는 형식이 대다수이다. EKS는 AWS에서 제공하는 쿠버네티스로, 평소에 쿠버네티스의 제일 작은 단위인 파드를 여러 개 띄워두고 순차적으로 배포하는 형태이다.\n","date":"2021-03-15","permalink":"https://leeleelee3264.github.io/post/2021-03-15-blue-green-deploy/","tags":["Infra"],"title":"Shell Script로 Blue Green Deployment를 흉내낸 무중단 서버 배포하기"},{"content":"\n책 [Modern Java in Action] 중 Chapter 1,2,3 을 요약한다.\nIndex\nChapter 1 Java 8, 9, 10, 11 무슨 일이 일어나고 있는가? Chapter 2 동작 파라미터화 코드 전달하기 Chapter 3 람다 표현식 Chapter 1 Java 8, 9, 10, 11 무슨 일이 일어나고 있는가? 지금까지의 대부분의 자바 프로그램은 코어 중 하나만을 사용했다. 자바 8이 등장하기 이전에는 나머지 코어를 활용하려면 스레드를 사용하는 것이 좋다고 조언했으나 스레드를 사용하면 관리하기 어렵고 많은 문제가 발생할 수 있다. 자바는 이러한 병렬 실행 환경을 쉽게 관리하고 에러가 덜 발생하는 방향으로 진화하려고 노력했다. 자바 9에서는 리액티브 프로그래밍이라는 병렬 실행 기법을 지원한다.\n자바 8은 간결한 코드, 멀티고어 프로세서의 쉬운 활용이라는 두 가지 요구사항을 기반으로 한다. 자바 8은 데이터베이스 질의 언어에서 표현식을 처리하는 것처럼 병렬 연산을 지원하는 스트림이라는 새로운 API를 제공한다. 즉, 스트림을 이용하면 에러를 자주 일으키며 멀티코어 CPU를 이용하는 것보다 비용이 훨씬 비싼 키워드 synchronized 를 사용하지 않아도 된다.\n자바 8의 핵심 기능\n스트림 API 코드를 전달하는 간결 기법 AKA 동작 파라미터화 인터페이스의 디폴트 메서드 예전이라면 복잡해보이는 익명클래스를 이용해서 동작이 담긴 코드를 넘겼지만, 메서드에 코드를 전달하는 기법을 이용하면 새롭고 간결한 방법으로 동작 파라미터화를 구현할 수 있다. 또한 자바 8은 객체지향과 정반대의 개념에 있는 함수형 프로그래밍에서 위력을 발휘한다. 코드를 전달하고, 조합을 하는 등의 특성은 함수형프로그래밍이다.\n왜 아직도 자바는 변하는가? 언어는 필요성에 따라서 만들어지고 도태된다. 예를 들어 C와 C++은 프로그래밍 안전성이 부족해 바이러스가 침투하기 쉬우나 런타임 풋프린트가 적어서 다영한 임베디드 시스템에서 인기를 끌고 있다.\n자바의 과거 자바는 시작부터 스레드와 락을 이용한 동시성을 지원하고, 유용한 라이브러리도 많이 가지고 있었다. 코드를 JVM 바이트 코드로 컴파일 하는 특징 때문에 (모든 브라우저는 가상머신 코드를 지원) 인터넷 애플릿 프로그램의 주요 언어가 되었다.\n자바의 미래 프로그램 생태계는 빅테이터라는 도전에 직면하면서 멀티코어 컴퓨터나 컴퓨팅 클러스터를 이용해 빅데이터를 효과적으로 처리할 필요성이 커졌다. 즉, 병렬 프로세싱을 이용해야 했고, 이는 자바에 부족한 기술이었다.\n새로운 하드웨어, 새로운 프로그래밍이 등장하는 것처럼 기후가 변하고 식물에 영향을 미치면서 기존 식물을 대신해서 새로운 식물을 길러햐 하는 것처럼 새로운 프로젝트에는 다른 언어를 선택해야 하고, 자바는 또 선택을 받기 위해 노력해야 한다.\n자바 8의 밑바탕이 된 설계 스트림 처리 스트림이란 한 번에 한 개씩 만들어지는 연속적인 데이터 항목들의 모임이다. 이론적인 프로그램은 입력 스트림에서 데이터를 한 개 읽어들이며 마찬가지로 출력 스트림으로 데이터를 한 개씩 기록한다. 우선은 스트림 API가 공장의 조립 라인처럼 어떤 항목을 연속으로 제공하는 어떤 기능이라고 단순하게 생각하자.\n유닉스가 명령을 스트림으로 처리하는 대표적인 예시이다. 스트림으로 처리하기 때문에 cat과 tr 등의 앞의 명령어가 파일을 끝까지 처리하고 있지 않아도 sort나 tail 이 작동할 수 있다.\n유닉스 스트림 처리 예시\n1 cat file1 file2 | tr \u0026#34;[A-Z]\u0026#34; \u0026#34;[a-z]\u0026#34; | sort | tail -3 자바 또한 유닉스가 복잡한 파이프라인을 만드는 것처럼 많은 메서드를 지원하는데 중요한 건 딱 하나다! 우리가 원하는 쿼리를 실행하기 위해 SQL문을 돌리고 밑에서는 C로 어떤 일이 일어나는지 전혀 모르는 것처럼, 우리가 스트림 API를 쓴다면 밑에서는 무슨 일이 일어나는지 전혀 모르면서 스트림 형태의 기능을 쓸 수 있다.\n또한 스트림의 가장 큰 장점은 우리가 조금의 변경사항을 준다면 작업을 여러 CPU 코어에 쉽게 할당해서 병렬성을 얻으면서도 스레드라는 복잡한 작업을 하지 않아도 된다.\n동작 파라미터화로 메서드에 코드 전달하기 이전에도 익명 클래스로 코드를 전달할 수는 있었으나 너무 복잡했다. 코드를 넘긴다는 개념이 잘 안 와닿는다면 이렇게 sorting해주세요~ 하고 컨디션 값 하나만 넘기는게 아니라 아예 sorting을 해주는 코드 자체를 넘겨준다고 생각하면 된다!\n익명 클래스 예시\n1 2 3 4 5 6 // 대표적인 익명 클래스 형식 Collections.sort(inventory, new Comparator\u0026lt;Apple\u0026gt;() { public int compare(Apple al, Appple a2) { return al.getWeight().compareTo(a2.getWeight()); } }); 병렬성과 공유 가변 데이터 스트림 메서드로 전달하는 코드는 다른 코드와 동시에 실행하더라도 안전하게 실행될 수 있어야 한다. 다른 코드와 동시에 실행 하더라도 안전하게 실행할 수 있는 코드를 만들려면 공유된 가변 데이터에 접근하지 않아야 한다. 두 프로세스가 공유된 변수를 동시에 바꾸려하면 어떻게 될지 생각해보라.\n기존처럼 어렵게 synchronized를 이용해서 공유된 가변 데이터를 보호하는 규칙을 만들 수 는 있지만 자바 스트림 API는 자바 스레드 API보다 설정 몇개로 더 쉽게 병렬성을 활용할 수 있다.\nChapter 2 동작 파라미터화 코드 전달하기 동작 파라미터화란 아직은 어떻게 실행할 것인지 결정하지 않은 코드 블록을 의미한다. 이 코드 블록은 나중에 프로그램에서 호출한다. (람다로 넘겨주게 된다)\nChapter2에서는 동작 파라미터화 aka 함수 전달하기가 어떤 과정으로 진화가 되어왔나를 보여주고 있다. 함수 자체를 넘기는 이유는 이게 더 자잘하게 플래그 넘기는 것 보다 확장성이 있기 때문. 어쨌든 중요한 점은 시시각각 변하는 사용자 요구 사항에 비용이 가장 최소화 될 수 있는 것!\n동작 전달 발전 단계\n플래그로 동작 분기 (최악) 함수형 인터페이스 익명 클래스 람다 플래그로 동작 분기 플래그 동작 분기 예시\n1 2 3 4 5 6 7 8 9 10 11 12 List\u0026lt;Apple\u0026gt; filterApples(List\u0026lt;Apple\u0026gt; apples, Color color, int weight, boolean flag) { List\u0026lt;Apple\u0026gt; result = new ArrayList\u0026lt;\u0026gt;(); for(Apple apple : apples) { if((flag \u0026amp;\u0026amp; apple.getColor().equals(color)) || (!flag \u0026amp;\u0026amp; apple.getWeight() \u0026gt; weight)) { result.add(apple); } } return result; } 실전에서는 이런 방법을 쓰면 안된다. true와 flase가 뭘 의미하는지도 모르겠고 앞으로 필터링 요구 사항이 추가되면 확장을 하기도 정말 힘들다. 이렇게 변수 하나로 분기를 하고 동작을 달리 하는 것보다 기본 동작은 똑같고 메인이 되는 (여기서는 필터링 조건 등등) 동작 자체를 다르게 하는 게 효율적이다. 이게 바로 동작 파라메터화 이다.\n함수형 인터페이스 요구 상황에 따라 출력을 해야 하는 정보가 다른 메서드가 있다. 아까와는 달리 값을 이해하기 어려운 플래그로 동작을 분기하지 않고 함수형 인터페이스를 넘겨줘서 동작을 달리해보았다.\n함수형 인터페이스 예시\n이런 형태를 전략 디자인 패턴 Strategy design pattern 이라고 한다. 전략 디자인 패턴은 각 알고리즘을 캡슐화하는 알고리즘 패밀리(구현)들을 정의해둔 다음에 런타임에 알고리즘을 선택하는 기법이다.\n예제에서는 컬렉션 탐색 로직과 적용할 동작을 분리했다! 이렇게 하면 한 메서드가 다른 동작을 수행하도록 재활용을 할 수 있다. → 캡슐화\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 interface PrintPredicate { String print(Apple apple); } class PrintWeightPredicate implements PrintPredicate { @Override public String print(Apple apple) { return Integer.toString(apple.getWeight()); } } class PrintHeavyLightPredicate implements PrintPredicate { int middleWeight = 20; static final String APPLE_HEAVY = \u0026#34;heavy\u0026#34;; static final String APPLE_LIGHT = \u0026#34;light\u0026#34;; @Override public String print(Apple apple) { if(apple.getWeight() \u0026gt; middleWeight) { return APPLE_HEAVY; } return APPLE_LIGHT; } } public class Quiz2Dash1 { public static void prettyPrintApple(List\u0026lt;Apple\u0026gt; inventory, PrintPredicate p) { for(Apple apple : inventory) { String output = p.print(apple); System.out.println(output); } } public static void main(String[] args) { Apple one = new Apple(Color.GREEN, 100); Apple two = new Apple(Color.RED, 20); Apple three = new Apple(Color.RED, 10); List\u0026lt;Apple\u0026gt; apples = Arrays.asList(one, two, three); prettyPrintApple(apples, new PrintHeavyLightPredicate()); prettyPrintApple(apples, new PrintWeightPredicate()); String test = \u0026#34;test\u0026#34;; } } 코드 상세 설명\n일단 오직 하나의 추상 메서드만 있는 인터페이스(함수형 인터페이스)를 선언한다. (line 1) 각자 요구 사항에 맞춰 인터페이스를 구현한 클래스들을 만든다. (line 5, 13) 이 인터페이스를 파라메터로 (동작 파라메터) 받는 메서드를 만든다. (line 31) 실제로 실행 단에서 이 메소드를 호출하며 동시에 동작을 담고 있는 클래스를 던져준다. (line 46, 47) 대표적인 함수형 인터페이스로는 정렬을 하는 Comparator의 sort, 쓰레드를 이용해서 코드 블록을 실행하는 Runnable의 run, 메소드를 호출하는 Callable의 call이 있다.\n익명 클래스를 이용 아무래도 위의 방법은 인터페이스를 만들고 구현하는 등 자질구레한 일들이 굉장히 많아서 귀찮다! 이를 조금이라도 줄이고자 자바는 클래스 선언과 인스턴스화를 동시에 수행하는 익명 클래스를 만들었다.\n익명클래스는 자바의 local class와 비슷한 개념이다. 이름이 없는 클래스로, 이를 이용하면 클래스 선언과 인스턴스화를 동시에 할 수 있고 즉석에서 필요한 구현을 만들어 사용할 수 있다.\n익명 클래스 예시\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 prettyPrintApple(apples, new PrintPredicate() { @Override public String print(Apple apple) { return Integer.toString(apple.getWeight()); } }); prettyPrintApple(apples, new PrintPredicate() { int middleWeight = 20; static final String APPLE_HEAVY = \u0026#34;heavy\u0026#34;; static final String APPLE_LIGHT = \u0026#34;light\u0026#34;; @Override public String print(Apple apple) { if(apple.getWeight() \u0026gt; middleWeight) { return APPLE_HEAVY; } return APPLE_LIGHT; } }); 이렇게 하면 클래스들을 만들고 인스턴스로 만든 다음에 쓰는 과정을 확 줄일 수는 있는데 딱 한 번을 사용할 수 있고 또 쓰려면 익명 클래스로 또 만들어줘야 해서 만약 반복 사용을 한다고 하면 좋은 방법은 아닌 것 같다. 그리고 익명 클래스의 형태가 익숙하지 않은 사람들도 많다.\n람다 표현식 이용 람다 예시\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // 람다로 던져보기 prettyPrintApple(apples, (Apple apple) -\u0026gt; Integer.toString(apple.getWeight()) ); prettyPrintApple(apples, (Apple apple) -\u0026gt; { int middleWeight = 20; String APPLE_HEAVY = \u0026#34;heavy\u0026#34;; String APPLE_LIGHT = \u0026#34;light\u0026#34;; if(apple.getWeight() \u0026gt; middleWeight) { return APPLE_HEAVY; } return APPLE_LIGHT; }); 내가 만든 예제는 람다를 던져도 극단적으로 간단해 보이지는 않는데 람다를 사용하면 명시적으로 클래스를 안 만들어도 되기 때문에 훨씬 더 간단한 형태로 보인다.\n결론 코드 전달 기법을 이용하면 동작을 메서드의 인수로 전달할 수 있다. 자바 8이전에는 이런 작업을 하고 싶다면 상당수의 코드가 추가되었다. 익명 클래스가 있다고 해도 인터페이스를 상속받아 여러 클래스를 구현해야 하는 수고는 여전했는데 람다로 이걸 해결했다.\nChapter 3 람다 표현식 람다를 쓰는 이유는 익명 클래스로 다양한 동작을 구현할 수 있지만 만족할 만큼 코드가 깔끔하지 않아서다.\n아무튼 중요한 사실은 익명클래스도 람다도 코드를 인수로 전달 할 수 있는 귀중한 기능이다. 더 정확하게는 람다 표현식은 메서드로 전달할 수 있는 익명 함수를 단순화한 것이라고 할 수 있다. 람다 표현식은 이름이 없지만, 파라미터 리스트, 바디, 반환 형식, 발생할 수 있는 예외 리스트는 가질 수 있다.\n람다의 특징 익명: 이름이 없어서 익명이다. 메서드를 안 만드니 작명 걱정도 안하고 구현할 코드에 대해 걱정이 줄어든다. 함수: 람다는 메서드처럼 특정 클래스에 종속이 안되어서 그냥 함수라고 부른다. 전달: 람다는 코드를 인수로 전달하는 귀중한 기능이다. 간결성: 익명 클래스보다 훨씬 보기 쉽고 깔끔, 자질구레한 것이 많이 줄었다. 람다의 구조 [Picture 1] 람다의 구조 Argument: 파라메터의 리스트, Array Token: → 모양은 람다의 파라미터 리스트와 바디를 구분하는 용도이고, Statements: -\u0026gt; 뒤가 람다의 바디이다. 람다의 바디를 {}로 감쌓기도 하는데 나중가면 저렇게 싼 형태가 더 알아보기 힘들다고 한다. 람다 표현식에는 return이 함축되어있어서 웬만하면 return을 직접 명시하지 않는다. expression과 statements의 차이 무슨 차이가 있나 했더니 statement가 최종 값을 넣기 때문에 완성된 형태라서 expression을 포함하고 있다.\n예시\n1 2 3 4 5 6 7 8 9 (param) -\u0026gt; expression (param) -\u0026gt; { statements; } // expression 표현식 b + 1 // statements 구문 a = b + 1; // expression statement a++; 어디에, 어떻게 람다를 사용하나? 람다를 사용할 수 있는 부분은 함수형 인터페이스를 파라메터로 받는 자리이다. 함수형 인터페이스란 오직 하나의 추상메서드만 가지고 있는 인터페이스이다. 예시의 코드처럼 @FunctionalInterface 어노테이션이 함께 있다.\n람다 표현식으로 함수형 인터페이스의 추상 메서드 구현을 직접 전달할 수 있으므로 전체 표현식을 함수형 인터페이스의 인스턴스로 취급한다. 따지고보면 함수형 인터페이스를 구현한 클래스의 인스턴스로 취급하는 것이다.\n함수형 인터페이스 예시\n1 2 3 4 5 6 7 8 9 10 11 @FunctionalInterface public interface Predicate\u0026lt;T\u0026gt; { /** * Evaluates this predicate on the given argument. * * @param t the input argument * @return {@code true} if the input argument matches the predicate, * otherwise {@code false} */ boolean test(T t); 람다가 이름도 없이 그냥 띡 던져놓기만 해도 관련 메소드를 착착 만들 수 있었던 이유는.. 함수형 메서드가 가지고 있는 메서드는 하나니까 그 메소드를 만들어서 던지겠지 하는 것도 있고 람다가 던지는 파라메터 리스트, 반환 값을 맞춰본다.\n람다 활용: 실행 어라운드 패턴 자원 처리(데이터 베이스, 파일 등등)에 사용하는 순환 패턴은 자원을 열고, 처리한 다음에 자원을 닫는 순서로 이루어진다. 이걸 실행 어라운드 패턴이라고 한다.\n여기서 어떻게 람다를 활용하나? 어차피 실행부를 감쌓는 자원열고닫기 부분은 동일하다. 실행부에서 뭘 하는지가 중요한데 그 실행부를 동작 파라메터화를 시키고, 실제로 호출을 할 때 내 입맛에 맞게 람다로 만든 표현식을 던지면 일이 간단해진다.\n실행 어라운드 예시\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 @FunctionalInterface interface FileProcess { void work (BufferedReader b); } public class ExFile { static void processFile(FileProcess p) { // try-with-resource 형태라서 따로 자원을 닫을 필요가 없어졌다! try (BufferedReader reader = new BufferedReader(new FileReader(\u0026#34;C:\\\\Temp\\\\file1.txt\u0026#34;))) { p.work(reader); } catch (Exception e){ System.out.println(e.getMessage()); } } public static void main(String[] args) { // 1그냥 파일 읽기 processFile((BufferedReader r) -\u0026gt; { try { System.out.println(r.readLine()); } catch (IOException e) { e.printStackTrace(); } }); // 2 파일 내용 복사하기 processFile((BufferedReader r ) -\u0026gt; { try (BufferedWriter r2 = new BufferedWriter(new FileWriter(\u0026#34;C:\\\\Temp\\\\file2.txt\u0026#34;))) { r2.write(r.readLine()); } catch (IOException e) { e.printStackTrace(); } }); } } 코드 상세 설명\n함수형 인터페이스를 만든다. (line 1) 함수형 인터페이스를 파라메터로 받는 재활용 메서드 만든다. (line 8) 재활용 메서드 안에서 함수형 인터페이스의 단 하나 뿐인 추상메서드 호한다. (line 12) 실제 실행 단에서 재활용 메서드 호출 할 때 원하는 행동 람다로 만들어서 던진다. (line 21, 30) 책에서 말한 것처럼 실행어라운드는 동일하게 사용하고 자원을 실제로 어떻게 사용할 것인지는 람다로 전달해 내 마음대로 실행을 했다. try-with-resource 형식을 잘 기억하자. 이걸 그대로 사용하면 매번 따로 자원을 닫아주는 번거로운 일을 하지 않아도 된다. 그냥 자원을 열 때 try () 안에서 연다는 걸 알면 된다.\n함수형 인터페이스 사용 함수형 인터페이스의 추상 메서드는 람다 표현식의 시그니처 (반환값, 파라메터값)을 묘사한다. 함수형 인터페이스의 추상 메서드 시그니처를 함수 디스크립터라고 한다.\n앞에서도 말한 것처럼 람다는 함수형 인터페이스에서 사용이 가능한데 자바는 이미 다양한 형태의 built-in 함수 인터페이스를 상황별로 만들어놨고 우리는 필요에 따라서 반환값과 파라메터값을 고려해서 쓰기만 하면 된다. (Comparable, Runnable, Callable 등등)\n[Picture 2] built-in 함수형 인터페이스 Predicate 사용 예시\n객체를 사용해서 boolean 값 return, 한마디로 true/false 값 나누고 싶을 때 사용한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 public class ExPredicate { // 맨 앞 \u0026lt;T\u0026gt; 왜 있나 했는데 filter 에서 쓰이는 T를 정의해주는 느낌이다 public static \u0026lt;T\u0026gt; List\u0026lt;T\u0026gt; filter(List\u0026lt;T\u0026gt; list, Predicate\u0026lt;T\u0026gt; p) { List\u0026lt;T\u0026gt; results = new ArrayList\u0026lt;\u0026gt;(); for(T t: list) { if(p.test(t)) { results.add(t); } } return results; } // 람다의 준비물 // 1.functional method // 2.recycle method // 3.call recycle method with lambda public static void main(String[] args) { List\u0026lt;String\u0026gt; sample = Arrays.asList(\u0026#34;longlong\u0026#34;, \u0026#34;short\u0026#34;, \u0026#34;\u0026#34;); List\u0026lt;String\u0026gt; notShort = filter(sample, (String s) -\u0026gt; s.length() \u0026gt; 3); System.out.println(notShort); IntPredicate same = (int i) -\u0026gt; i == 1000; Predicate\u0026lt;Integer\u0026gt; sameBoxing = (Integer i)-\u0026gt; i == (new Integer(1000)); System.out.println(same.test(1000)); System.out.println(sameBoxing.test(1000)); } } Consumer 사용 예시\n객체를 인수로 받아서 어떤 행동을 하고 싶을 때 사용한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 public class ExConsumer { // Consumer 은 반환값이 없어서 뭔가를 실행하고자 할 때 사용한다 public static \u0026lt;T\u0026gt; void doSomething(List\u0026lt;T\u0026gt; list, Consumer\u0026lt;T\u0026gt; c) { for(T t: list) { c.accept(t); } } public static void main(String[] args) { List\u0026lt;Integer\u0026gt; test = Arrays.asList(1,2,3); doSomething(test, (Integer i) -\u0026gt; System.out.println(i/2)); } } Function 사용 예시\n객체를 인수로 받아서 또 다른 타입의 객체를 반환할 때 사용한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 public class ExFunction { // Function 은 입력 값도 있고 반환 값도 있는 형식이다. static \u0026lt;T, R\u0026gt; Map\u0026lt;T, R\u0026gt; map(List\u0026lt;T\u0026gt; list, Function\u0026lt;T, R\u0026gt; f) { Map\u0026lt;T, R\u0026gt; valueMap = new HashMap\u0026lt;\u0026gt;(); for(T t:list) { valueMap.put(t, f.apply(t)); } return valueMap; } public static void main(String[] args) { List\u0026lt;String\u0026gt; test = Arrays.asList(\u0026#34;one\u0026#34;, \u0026#34;two\u0026#34;, \u0026#34;Three\u0026#34;); Map\u0026lt;String, Integer\u0026gt; testLen = map(test, String::length); System.out.println(testLen); } } 기본형 특화 (IntPredicate, IntSupplier etc)\n기본형 값들을 위의 메소드에 사용하려면 참조형만 값으로 받는 제네릭의 특성 때문에 Integer, Double등으로 박싱을 해야 한다. 최신 자바는 오토박싱을 해주기는 하는데 박싱은 객체라서 힙에 차곡차곡 쌓이게 된다. 결국 메모리를 더 쓰게 되고, 기본형을 가져올때도 메모리 탐색을 해야 한다.\n이것이 모이면 결국 자원낭비라서 그냥 기본형으로 함수형인터페이스를 사용한다면 기본형 특화를 쓰는게 좋다.\n형식 검사, 형식 추론, 제약 다이아몬드 연산자 람다 표현식으로 함수형 인터페이스의 인스턴스를 만드는 것은 일종의 추론이라고 할 수 있다. 함수형 인터페이스의 유일한 추상 메서드의 파라메터 값과 반환값 형태와 동일한 람다 형식을 쓰면 아~ 그건가보다~ 하고 납득을 하기 때문에.\n그런데 이런 경우가 하나 더 있다. 바로 다이아몬드 연산자 이다.\n다이아몬드 연산자 예시\n1 2 List\u0026lt;String\u0026gt; listOfStrings = new ArrayList\u0026lt;\u0026gt;(); List\u0026lt;Integer\u0026gt; listOfIntegers = new ArrayList\u0026lt;\u0026gt;(); 이렇게 다이아몬드 연산자를 사용해서 콘텍스트에 따라 제네릭 형식을 추론하고 있다.\n형식추론 이미 간단한 형태의 람다를 더 간단한 형태로 만들수 있다면? 람다 생성은 완전히 자바 컴파일러가 추론을 하는 형태로 이루어져있다. 결과적으로 컴파일러는 람다 표현식의 파라미터 형식에 접근을 할 수 있기 때문에 람다 문법에서는 파라미터 타입을 생략해도 된다.\n파라메터 타입 생략 예시\n1 2 3 4 5 6 7 8 9 10 11 12 public static void main(String[] args) { List\u0026lt;Integer\u0026gt; test = Arrays.asList(1,2,3); doSomething(test, (Integer i) -\u0026gt; System.out.println(i/2)); } // 파라미터 제거 public static void main(String[] args) { List\u0026lt;Integer\u0026gt; test = Arrays.asList(1,2,3); doSomething(test, (i) -\u0026gt; System.out.println(i/2)); } 두 개의 코드는 완전히 동일한 코드이다. 상황에 따라 명시적으로 형식을 포함하는 것이 좋을 때도 있고 형식을 배제하는 것이 가독성을 향상시킬 때고 있다. 어떤 방법을 해야 할지는 역시나 개발자 스스로가 직접 판단을 해야 한다.\n제약: 지역 변수 사용 람다 표현식에서는 다른 메서드들과 마찬가지로 자유 변수(파라메터로 넘겨진 변수 말고 외부에서 선언이 된 변수)를 활용할 수 있다. → 람다 캡쳐링\n그런데 이렇게 람다에서 외부에서 선언된 변수에 접근을 하려면 그 변수는 딱 한 번 만 값을 넣을 수 있는 final 변수여야 한다. 이름에서부터 알 수 있는 것처럼 \u0026lsquo;캡처링\u0026rsquo;이니까 값이 바뀌면 캡쳐의 개념이 아니게 되는 것이다.\n아니면 Effective final 이라고 그냥 그 변수가 선언이 되고 딱 한 번만 할당이 되었으면 final이라고 명시를 안 해도 컴파일러가 알아서 final 처럼 취급을 해준다. 이런 제약이 있는 이유는 인스턴스 변수와 지역 변수가 태생부터 다르기 때문이다.\n인스턴스 변수 VS 지역 변수\n인스턴스 변수: 힙에 저장이 된다. (조금 더 오래 살아남는 값들이 힙에 저장이 되는 편) 지역변수: 스택에 저장이 된다. (기본타입과, 참조타입들의 이름들이 여기에 저장이 되는 편) 특정 메소드에서 사용이 되는 변수가 지역변수이다. 얘들은 메소드가 호출이 되었을 때 스택에 값이 차곡차곡 쌓였다가 메소드 연산이 끝나면 순서대로 다 팝팝팝 해서 터진다.\n람다와 call-by-value 람다에서 접근을 하는 외부 선언 변수는 람다가 그 외부 선언 변수 값 자체에 접근을 하는 게 아니고, 외부 선언 변수 값의 복사본에 접근을 하는 것이다. (call-by-value 형식이라고 할 수 있음). 그래서 그 복사본의 값이 바뀌지 않아야 해서 값을 한 번 만 할당하게 하는 것이다.\n애초에 이런 형식으로 된 이유는 스레드 세이프하게 만들기 위해서이다. 지역변수 값은 스택에 존재하기 때문에 자신을 정의한 스레드와 생존을 같이 해야 한다. 람다가 스레드 A 에서 실행된다면 변수를 할당한 다른 스레드 B가 사라져버려 변수 할당이 해제 되었는데도 람다에서는 계속 그 변수에 접근을 하려고 할 수 있다.\n따라서, 람다는 자신이 정의된 메서드의 지역 변수의 값은 바꿀 수 없다. 람다가 정의된 메서드의 지역 변수값은 final 변수기 때문에 람다는 변수가 아닌 값에 국한되어 어떤 동작들을 수행한다.\n참고하면 좋을 자료들 [자바 메모리 관리 - 스택 \u0026amp; 힙] [JVM의 메모리 구조 및 할당과정] [Stack Memory and Heap Space in Java | Baeldung] 메서드 참조 메서드 참조를 이용하면 기존의 메서드 정의를 재활용해서 람다처럼 전달을 할 수 있다. (동작 파라메터). 때때로 람다 표현식을 쓰는 것보다 메서드 참조를 사용하는 게 더 가독성이 좋고 자연스러울 수 있다. 왜냐하면 메서드 참조는 어떤클래스.어떤메소드 형태로 넘기기 때문에 확실히 명시된다는 느낌이 있다.\n메서드 참조 예시\n1 2 3 4 5 6 7 8 9 10 11 12 static List\u0026lt;Apple\u0026gt; filterApples(List\u0026lt;Apple\u0026gt; inventory, Predicate\u0026lt;Apple\u0026gt; p) { List\u0026lt;Apple\u0026gt; result = new ArrayList\u0026lt;\u0026gt;(); for(Apple apple: inventory) { if(p.test(apple)) { result.add(apple); } } return result; } List\u0026lt;Apple\u0026gt; result1 = filterApples(apples, Apple::isHeavyApple); List\u0026lt;Apple\u0026gt; result2 = filterApples(apples, Apple::isGreenApple); 어떤 메소드를 실행해야 하는지 대놓고 이름을 가져와서 보여주고 있다. 훨씬 더 명시된 모습. 그런데 여기서 실제로 메서드를 호출하는 건 아니고 람다에서 이렇게이렇게 동작을 하세요! 하고 코드를 짜서 넘겨주는 것처럼 실제 실행은 다른 곳에서 하기 때문에 메서드 참조를 할 때 메서드 뒤에 ()을 쓰지 않아도 된다.\n메서드 참조 유형\n정적 메서드 참조 Integer::parseInt (parseInt는 static 메서드) 인스턴스 메서드 참조 String::length (length는 일반 메서드) 기존 객체의 인스턴스 메서드 참조 기존 객체의 인스턴스 메서드 참조 예시\n1 2 String test = new String(\u0026#34;hello\u0026#34;); someMethod(test::length); 3번의 형태를 쓰는 경우는 람다 외부에서 (final로) 생성이 된 변수들의 메서드를 람다 안에서 사용을 할 때 쓰는 형태라고 한다. 이런식은 클래스 안에 private 하게 전용 핼퍼 메서드를 정의한 상황에서 유용하게 쓸 수 있다고 한다.\n컴파일러는 람다 표현식의 형식을 검사하던 방식과 비슷한 과정으로 메서드 참조가 주어진 함수형 인터페이스와 호환하는지 확인한다. 메서드 참조는 콘텍스트의 형식과 일치해야 한다.\n생성자 참조 예시\n1 2 Supplier\u0026lt;Apple\u0026gt; c1 = Apple:new; Apple a1 = c1.get(); Map으로 카테고리별로 생성자 참조를 해놔서 값 맵핑해서 필요한 인수들이 다 다른 상태에서 객체 만드는 건 좋아보인다. 객체를 생성할 때에는 자바 built-in 함수인터페이스인 Function이나 BiFunction를 사용하면 좋을 것 같다.\n람다 표현식을 조합할 수 있는 유용한 메서드 여러가지 유틸성 함수형 메서드들을 조합해서 사용을 할 수 있다. 한 마다로 간단한 여러개의 람다 표현식을 조합해서 복잡한 람다 표현식을 만들 수 있다.\n함수형 인터페이스가 이렇게 만능으로 구현이 될 수 있는 이유는 default메서드로 만들어져있기 때문이다. 미래를 생각해서 이것저것 다 넣어놓고 구현 안 해도 되는 default를 추가한 것 같다. 이는 결국 내가 함수형 메서드를 만들었을 때도 미래지향적으로 핼퍼 메서드를 이거저거 만들어 놓으면 연결연결연결 해서 쓸 수 있다는 얘기다.\n심지어 이렇게 람다 표현식을 복잡하게 조합을 해도 코드 자체가 문제를 잘 설명한다고 한다.\nComperator 연결 예시\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 // 역정렬 inventory.sort(comparing(Apple:getWeight).reversed()); /** * Returns a comparator that imposes the reverse ordering of this * comparator. * * @return a comparator that imposes the reverse ordering of this * comparator. * @since 1.8 */ default Comparator\u0026lt;T\u0026gt; reversed() { return Collections.reverseOrder(this); } // 값이 같다면 다른 조건 하나 더 넣어서 비교 inventory.sort(comparing(Apple:getWeight).reversed() .thenComparing(Apple:getCountry)); default Comparator\u0026lt;T\u0026gt; thenComparing(Comparator\u0026lt;? super T\u0026gt; other) { Objects.requireNonNull(other); return (Comparator\u0026lt;T\u0026gt; \u0026amp; Serializable) (c1, c2) -\u0026gt; { int res = compare(c1, c2); return (res != 0) ? res : other.compare(c1, c2); }; } Predicate 연결 예시\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // and 조건 Predicate\u0026lt;Apple\u0026gt; redAntHeavyApple = redApple.and(apple -\u0026gt; apple.getWeight() \u0026gt;150); // or 조건 Predicate\u0026lt;Apple\u0026gt; redAndHeavyAppleOrGreen = readApple.and(apple -\u0026gt; apple.getWeight() \u0026gt; 150) .or(apple -\u0026gt; GREEN.equals(a.getColor())); default Predicate\u0026lt;T\u0026gt; or(Predicate\u0026lt;? super T\u0026gt; other) { Objects.requireNonNull(other); return (t) -\u0026gt; test(t) || other.test(t); } default Predicate\u0026lt;T\u0026gt; and(Predicate\u0026lt;? super T\u0026gt; other) { Objects.requireNonNull(other); return (t) -\u0026gt; test(t) \u0026amp;\u0026amp; other.test(t); } Function 연결 예시\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 // andThen 주어진 함수를 먼저 적용한 결과를 다른 함수의 입력으로 전달 Function\u0026lt;Integer, Integer\u0026gt; f = x -\u0026gt; x + 1; Function\u0026lt;Integer, Integer\u0026gt; g = x -\u0026gt; x * 2; Function\u0026lt;Integer, Integer\u0026gt; h = f.andThen(g); int result = h.apply(1); //4가 나온다. // compose 인수로 주어진 함수를 먼저 실행한 다름에 결과를 외부 함수로 전달 Function\u0026lt;Integer, Integer\u0026gt; f = x -\u0026gt; x + 1; Function\u0026lt;Integer, Integer\u0026gt; g = x -\u0026gt; x * 2; Function\u0026lt;Integer, Integer\u0026gt; h = f.compose(g); int result = h.apply(1); //3가 나온다. // 결국 andThen과 compose 의 차이는 앞을 먼저 계산하냐 뒤를 먼저 계산하냐의 차이 default \u0026lt;V\u0026gt; Function\u0026lt;V, R\u0026gt; compose(Function\u0026lt;? super V, ? extends T\u0026gt; before) { Objects.requireNonNull(before); return (V v) -\u0026gt; apply(before.apply(v)); } default \u0026lt;V\u0026gt; Function\u0026lt;T, V\u0026gt; andThen(Function\u0026lt;? super R, ? extends V\u0026gt; after) { Objects.requireNonNull(after); return (T t) -\u0026gt; after.apply(apply(t)); } 이렇게 람다로 호출을 바로 못하고 함수형 메서드로 여러개를 만들어서 체인을 걸어야 하는 이유는 저 메서드들이 함수형 메서드에 들어있는 애들이다 보니까 확실하게 함수형 메서드로 객체 선언이 되어야 한다. 저렇게 만들어놓으면 람다로 던지지 않고 그냥 redAndHeavyApple 이렇게 자체를 던져서 사용을 하면 되겠다.\n실제 호출 예시\n1 2 3 4 5 Predicate\u0026lt;Apple\u0026gt; redApple = apple -\u0026gt; apple.getColor().equals(Color.RED); Predicate\u0026lt;Apple\u0026gt; redAntHeavyApple = redApple.and(apple -\u0026gt; apple.getWeight() \u0026gt;150); List\u0026lt;Apple\u0026gt; finalResult = filterApples(apples, redAntHeavyApple); System.out.println(finalResult); ","date":"2021-02-14","permalink":"https://leeleelee3264.github.io/post/2021-02-14-java-in-action-part1/","tags":["Book"],"title":"[Modern Java in Action] Chapter 1,2,3"}]